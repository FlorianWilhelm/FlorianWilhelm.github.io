<!DOCTYPE html>
<html lang="en" prefix="og: http://ogp.me/ns# fb: https://www.facebook.com/2008/fbml">
<head>
    <title>Causal Inference and Propensity Score Methods - Florian Wilhelm's blog</title>
    <!-- Using the latest rendering mode for IE -->
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">



<link rel="canonical" href="https://florianwilhelm.info/2017/04/causal_inference_propensity_score/">

        <meta name="author" content="Florian Wilhelm" />
        <meta name="keywords" content="scikit-learn,machine-learning,python,causal inference" />
        <meta name="description" content="In the field of machine learning and particularly in supervised learning, correlation is crucial to predict the target variable with the help of the feature variables. Rarely do we think about causation and the actual effect of a single feature variable or covariate on the target or response. Some even …" />

        <meta property="og:site_name" content="Florian Wilhelm's blog" />
        <meta property="og:type" content="article"/>
        <meta property="og:title" content="Causal Inference and Propensity Score Methods"/>
        <meta property="og:url" content="https://florianwilhelm.info/2017/04/causal_inference_propensity_score/"/>
        <meta property="og:description" content="In the field of machine learning and particularly in supervised learning, correlation is crucial to predict the target variable with the help of the feature variables. Rarely do we think about causation and the actual effect of a single feature variable or covariate on the target or response. Some even …"/>
        <meta property="article:published_time" content="2017-04-15" />
            <meta property="article:section" content="post, talk" />
            <meta property="article:tag" content="scikit-learn" />
            <meta property="article:tag" content="machine-learning" />
            <meta property="article:tag" content="python" />
            <meta property="article:tag" content="causal inference" />
            <meta property="article:author" content="Florian Wilhelm" />

    <meta name="twitter:dnt" content="on">
    <meta name="twitter:card" content="summary">
        <meta name="twitter:site" content="@FlorianWilhelm">
        <meta name="twitter:creator" content="@FlorianWilhelm">
    <meta name="twitter:domain" content="https://florianwilhelm.info">


    <!-- Bootstrap -->
        <link rel="stylesheet" href="https://florianwilhelm.info/theme/css/bootstrap.min.css" type="text/css"/>
    <link href="https://florianwilhelm.info/theme/css/font-awesome.min.css" rel="stylesheet">

    <link href="https://florianwilhelm.info/theme/css/pygments/native.css" rel="stylesheet">
        <link href="https://florianwilhelm.info/theme/css/typogrify.css" rel="stylesheet">
    <link rel="stylesheet" href="https://florianwilhelm.info/theme/css/style.css" type="text/css"/>


        <link href="https://florianwilhelm.info/feeds/post-talk.atom.xml" type="application/atom+xml" rel="alternate"
              title="Florian Wilhelm's blog post, talk ATOM Feed"/>
    <!-- Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-LQCSE9V2BL"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());

        gtag('config', 'G-LQCSE9V2BL');
    </script>
    <!-- End Google Analytics Code -->
    <!-- Google Tag Manager -->
    <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
    new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
    'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','');</script>
    <!-- End Google Tag Manager -->

</head>
<body>

<div class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
        <div class="navbar-header">
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-ex1-collapse">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a href="https://florianwilhelm.info/" class="navbar-brand">
Florian Wilhelm's blog            </a>
        </div>
        <div class="collapse navbar-collapse navbar-ex1-collapse">
            <ul class="nav navbar-nav">
                    <li><a href="/about/">About me</a></li>
            </ul>
            <ul class="nav navbar-nav navbar-right">
              <li><a href="https://florianwilhelm.info/archives.html"><i class="fa fa-th-list"></i><span class="icon-label">Archives</span></a></li>
            </ul>
        </div>
        <!-- /.navbar-collapse -->
    </div>
</div> <!-- /.navbar -->

<!-- Banner -->
<!-- End Banner -->

<!-- Content Container -->
<div class="container">
    <div class="row">
        <div class="col-sm-9">
    <section id="content">
        <article>
            <header class="page-header">
                <h1>
                    <a href="https://florianwilhelm.info/2017/04/causal_inference_propensity_score/"
                       rel="bookmark"
                       title="Permalink to Causal Inference and Propensity Score Methods">
                        Causal Inference and Propensity Score&nbsp;Methods
                    </a>
                </h1>
            </header>
            <div class="entry-content">
                <div class="panel">
                    <div class="panel-body">
<footer class="post-info">
    <span class="label label-default">Date</span>
    <span class="published">
        <i class="fa fa-calendar"></i><time datetime="2017-04-15T18:00:00+02:00"> Apr. 15, 2017</time>
    </span>





<span class="label label-default">Tags</span>
	<a href="https://florianwilhelm.info/tag/scikit-learn/">scikit-learn</a>
        /
	<a href="https://florianwilhelm.info/tag/machine-learning/">machine-learning</a>
        /
	<a href="https://florianwilhelm.info/tag/python/">python</a>
        /
	<a href="https://florianwilhelm.info/tag/causal-inference/">causal inference</a>
    
</footer><!-- /.post-info -->                    </div>
                </div>
                <p>In the field of machine learning and particularly in supervised learning, correlation is crucial to predict the target variable with the help of the feature variables. Rarely do we think about causation and the actual effect of a single feature variable or covariate on the target or response. Some even go so far as to say that &#8220;correlation trumps causation&#8221; like in the book &#8220;Big Data: A Revolution That Will Transform How We Live, Work, and Think&#8221; by Viktor Mayer-Schönberger and Kenneth Cukier. Following their reasoning, with Big Data there is no need to think about causation anymore, since <a href="https://en.wikipedia.org/wiki/Nonparametric_statistics">nonparametric models</a> will do just fine using correlation alone. For many practical use cases, this point of view may seem acceptable — but surely not for&nbsp;all.</p>
<p>Consider for instance you are managing an advertisement campaign with a budget allowing you to send discount vouchers to 10,000 of your customers. Obviously, you want to maximise the outcome of the campaign, meaning you want to focus on customers that buy because they received a voucher. If <span class="math">\(y_{1i}\)</span> and <span class="math">\(y_{0i}\)</span> are the amounts of money spent by a customer <span class="math">\(i\)</span> described by <span class="math">\(x_i\)</span> that either received or did not receive a voucher, we want to find a subset <span class="math">\(I'\subset{}I\)</span> where <span class="math">\(I\)</span> is the set of all former customers with <span class="math">\(|I'|=10,000\)</span> so that <span class="math">\(\sum_{i\in I'}y_{1i} - y_{0i}\)</span> is&nbsp;maximised. </p>
<p>Another example would be to estimate the effect of additional booking options in an online marketplace. A common use case of an online vehicle marketplace is to provide estimations to sellers on the effect that additional booking options (e.g. highlighting, top of page etc.) may have on the selling time of a given vehicle. Likewise, many dealers will be interested in knowing how changing the price of a vehicle will affect the probability of selling the vehicle within a certain period of time or the expected selling&nbsp;time.</p>
<p>These two examples outline the need for methods to estimate the actual causal effect of a controllable covariate onto the response. Before we dip our toes into the deep sea of causal inference, let&#8217;s consider a few general aspects. The first and maybe not so obvious point when coming from a supervised learning background is that causal inference is thinking about <em>what did not happen</em>. That means that we don&#8217;t know for instance how much a customer that received a voucher would have ordered if they had not received one. This fundamental problem basically renders it an unsupervised learning problem. More interesting aspects of causal inference are summarized in a blog post by Macartan Humphreys on <a href="https://egap.org/resource/10-things-to-know-about-causal-inference/">10 Things to Know About Causal Inference</a>. The question remains how can we estimate the causal effect of a controllable&nbsp;covariate?</p>
<h2>Strongly&nbsp;ignorable</h2>
<p>The answer to this question lies in another important (and actually the original) use case of causal inference, which is the analysis of therapy effects. In a best-case scenario, the effect of a therapy can be determined in a randomized trial by comparing the response of a treatment group to a control group. In a randomized trial, the allocation of participants to the test or control group is random and thus independent of any covariates <span class="math">\(X\)</span>. Following the original paper of Rosenbaum <span class="amp">&amp;</span> Rubin <sup id="fnref:rose"><a class="footnote-ref" href="#fn:rose">2</a></sup>, in a randomized trial the treatment assignment <span class="math">\(Z\)</span> and the (unobservable) potential outcomes <span class="math">\({Y_1, Y_0}\)</span> are conditionally independent given the covariates <span class="math">\(X\)</span>,&nbsp;i.e. </p>
<div class="math">$${Y_1, Y_0} ⫫ Z \mid X.$$</div>
<p> Furthermore, we assume that each participant in the experiment has a chance to receive each treatment, i.e. <span class="math">\(0 &lt; p(Z=1|x) &lt; 1\)</span>. The treatment assignment is said to be <em>strongly ignorable</em> if those two conditions hold for our observed covariates <span class="math">\(x\)</span>. </p>
<h2>Causal effect in a randomized&nbsp;trial</h2>
<p>In a randomized trial, the strong ignorability of <span class="math">\(Z\)</span> allows us to estimate the effect of the treatment by comparing the response of the treatment group with that of the control group. The following approach may be used to estimate the individual effect of an additional booking option on a vehicle&#8217;s selling time with machine learning methods like a random&nbsp;forest:</p>
<ol>
<li>Train the model with the covariates <span class="math">\(X\)</span> and <span class="math">\(Z\)</span> as feature and response <span class="math">\(Y\)</span> as&nbsp;target,</li>
<li>predict for a given <span class="math">\(x\)</span> the response <span class="math">\(\hat{y}_1\)</span> with <span class="math">\(Z=1\)</span> and <span class="math">\(\hat{y}_0\)</span> with <span class="math">\(Z=0\)</span>,</li>
<li>calculate the effect with <span class="math">\(\hat{y}_1 - \hat{y}_0\)</span> or <span class="math">\(\frac{\hat{y}_1}{\hat{y}_0}\)</span>.</li>
</ol>
<p>In a real-world big-data problem we often have no control over the experimental setup — we are just left with the data. This happens for instance in observational studies. Imagine for instance the treatment with an experimental drug having strong side-effects that might cure a life-threatening disease. A controlled, randomized experiment where the control groups gets a placebo might be impracticable or even&nbsp;unethical. </p>
<p>In such a nonrandomized experiment, there is no proper mathematical way to check if the treatment is strongly ignorable<sup id="fnref:pearl1"><a class="footnote-ref" href="#fn:pearl1">3</a></sup>. According to Pearl<sup id="fnref:pearl2"><a class="footnote-ref" href="#fn:pearl2">4</a></sup> by now assuming strong ignorability we are basically assuming that our covariate set <span class="math">\(X\)</span> is <em>admissible</em>, i.e. <span class="math">\(p(y|\mathrm{do}(z))=\sum_{x}p(y|x,z)p(x)\)</span>. Here, Pearl&#8217;s <span class="math">\(\mathrm{do}\)</span>-notation <span class="math">\(p(y|\mathrm{do}(z))\)</span> denotes the “causal effect” of <span class="math">\(Z\)</span> on <span class="math">\(Y\)</span>, i.e. the distribution of <span class="math">\(Y\)</span> after setting variable <span class="math">\(Z\)</span> to a constant <span class="math">\(Z = z\)</span> by external intervention. In practice, the assumption of admissibility of <span class="math">\(X\)</span> is often used to estimate a causal effect. This led to incorrect results in some studies as well as controversies<sup id="fnref2:pearl1"><a class="footnote-ref" href="#fn:pearl1">3</a></sup>, so one should always be aware that the entire causal analysis depends on the validity of this&nbsp;assumption. </p>
<p>So far, we have not only assumed admissibility of <span class="math">\(X\)</span> but also a randomized trial for our approach. Therefore, we should check beforehand that <span class="math">\(X ⫫ Z\)</span>, (which is a necessary but not sufficient condition), before applying the aforementioned approach. This can be verified by using <span class="math">\(X\)</span> to predict <span class="math">\(Z\)</span>. If this is not possible, and thus <span class="math">\(p(z|x) = p(z)\)</span>, the former approach is viable. But what if <span class="math">\(Z\)</span> is not independent of <span class="math">\(X\)</span> — as is often the case with real-world data. For instance, dealers of expensive vehicle brands might be more willing to spend money and therefore tend to use more booking options. The data from the last marketing campaign presumingly includes a bias induced by the current strategy of the marketing department on how to pick customers that get a voucher. In these cases, we have to isolate the effect of <span class="math">\(Z\)</span> from our covariates <span class="math">\(X\)</span>.</p>
<h2>Propensity&nbsp;score</h2>
<p>By predicting <span class="math">\(Z\)</span> based on <span class="math">\(X\)</span>, we have estimated the <em>propensity score</em>, i.e. <span class="math">\(p(Z=1|x)\)</span>. This of course assumes that we have used a classification method that returns probabilities for the classes <span class="math">\(Z=1\)</span> and <span class="math">\(Z=0\)</span>. Let <span class="math">\(e_i=p(Z=1|x_i)\)</span> be the propensity score of the <span class="math">\(i\)</span>-th observation, i.e. the propensity of the <span class="math">\(i\)</span>-th participant getting the treatment (<span class="math">\(Z=1\)</span>). </p>
<p>We can use the propensity score to define weights <span class="math">\(w_i\)</span> to create a synthetic sample in which the distribution of measured baseline covariates is independent of treatment assignment<sup id="fnref:austin"><a class="footnote-ref" href="#fn:austin">5</a></sup>,&nbsp;i.e. </p>
<div class="math">$$w_i=\frac{z_i}{e_i}+\frac{1-z_i}{1-e_i},$$</div>
<p> where <span class="math">\(z_i\)</span> indicates if the <span class="math">\(i\)</span>-th subject was&nbsp;treated. </p>
<p>The covariates from our data sample <span class="math">\(x_i\)</span> are then weighted by <span class="math">\(w_i\)</span> to eliminate the correlation between <span class="math">\(X\)</span> and <span class="math">\(Z\)</span>, which is a technique known as <em>inverse probability of treatment weighting</em> (<span class="caps">IPTW</span>). This allows us to estimate the causal effect via the following&nbsp;approach:</p>
<ol>
<li>Train a model with covariates <span class="math">\(X\)</span> to predict <span class="math">\(Z\)</span>,</li>
<li>calculate the propensity scores <span class="math">\(e_i\)</span> by applying the trained model to all <span class="math">\(x_i\)</span>,</li>
<li>train a second model with covariates <span class="math">\(X\)</span> and <span class="math">\(Z\)</span> as features and response <span class="math">\(Y\)</span> as target by using <span class="math">\(w_i\)</span> as sample weight for the <span class="math">\(i\)</span>-th&nbsp;observation,</li>
<li>use this model to predict the causal effect like in the randomized trial&nbsp;approach.</li>
</ol>
<p><span class="caps">IPTW</span> is based on a simple intuition. For a randomized trial with <span class="math">\(p(Z=1)=k\)</span> the propensity score would be equal for all patients, i.e. <span class="math">\(e_i=\frac{1}{k}\)</span> and thus <span class="math">\(w_i=k\)</span>. In a nonrandomized trial, we would assign low weights to samples where the assignment of treatment matches our expectation and high weights otherwise. By doing so, we draw the attention of the machine learning algorithm to the observations where the effect of treatment is most prevalent, i.e. least confounded with the&nbsp;covariates.</p>
<h2>Python&nbsp;implementation</h2>
<p>We can set up a synthetic experiment to demonstrate and evaluate this method with the help of Python and Scikit-Learn. A synthetic experiment is appropriate to address the fundamental problem of causal inference described above. With real data, we  don&#8217;t know what would have happened if we had not treated someone, sent a voucher or not booked that additional option and vice versa. Therefore, we derive a model that describes the relationship of <span class="math">\(X\)</span> to <span class="math">\(Y\)</span> as well as the effect of <span class="math">\(Z\)</span> on <span class="math">\(Y\)</span>. We use this model to generate observational data where for each sample <span class="math">\(x_i\)</span> we either have <span class="math">\(Z=1\)</span> or <span class="math">\(Z=0\)</span> and thus incomplete information in our data. Our task is now to estimate the effect of <span class="math">\(Z\)</span> with the help of the generated&nbsp;data.</p>
<p>The following portion of this article is available for <a href="https://florianwilhelm.info/notebooks/causal_inference_propensity_score.ipynb">download</a> as a Jupyter&nbsp;notebook.</p>
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">math</span> <span class="kn">import</span> <span class="n">exp</span><span class="p">,</span> <span class="n">log</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">scipy</span> <span class="k">as</span> <span class="nn">sp</span>
<span class="kn">from</span> <span class="nn">scipy</span> <span class="kn">import</span> <span class="n">stats</span>
<span class="kn">from</span> <span class="nn">scipy.special</span> <span class="kn">import</span> <span class="n">expit</span>

<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span><span class="p">,</span> <span class="n">RandomForestRegressor</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="kn">from</span> <span class="nn">sklearn.naive_bayes</span> <span class="kn">import</span> <span class="n">GaussianNB</span>
<span class="kn">from</span> <span class="nn">sklearn.calibration</span> <span class="kn">import</span> <span class="n">CalibratedClassifierCV</span>

<span class="kn">import</span> <span class="nn">statsmodels.api</span> <span class="k">as</span> <span class="nn">sm</span>
<span class="kn">import</span> <span class="nn">statsmodels.formula.api</span> <span class="k">as</span> <span class="nn">smf</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span> 

<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="n">sns</span><span class="o">.</span><span class="n">set_style</span><span class="p">(</span><span class="s2">&quot;whitegrid&quot;</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="s2">&quot;notebook&quot;</span><span class="p">,</span> <span class="n">font_scale</span><span class="o">=</span><span class="mf">1.5</span><span class="p">,</span> <span class="n">rc</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;lines.linewidth&quot;</span><span class="p">:</span> <span class="mf">2.5</span><span class="p">})</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;figure.figsize&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">8</span>

<span class="n">np</span><span class="o">.</span><span class="n">seterr</span><span class="p">(</span><span class="n">divide</span><span class="o">=</span><span class="s1">&#39;ignore&#39;</span><span class="p">,</span> <span class="n">invalid</span><span class="o">=</span><span class="s1">&#39;ignore&#39;</span><span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
</pre></div>


<h3>Model</h3>
<p>Assume we have patients characterized by sex and age suffering from some disease with different severities. The recovery time of a patient depends only on the sex, age, severity and if the patient is on medication. Let the expected recovery time in days <span class="math">\(t_{recovery}\)</span> be defined&nbsp;as
</p>
<div class="math">$$\mathbf{E}(t_{recovery}) = \exp(2+0.5\cdot{}I_{male}+0.03\cdot{}age+2\cdot{}severity-1\cdot{}I_{medication}),$$</div>
<p>
where <span class="math">\(I\)</span> is an indicator function.
Furthermore, we will assume a Poisson distribution in order to generate some synthetic data of our patient&#8217;s recovery time. Due to our definition, treating the disease with medication reduces the recovery time to <span class="math">\(\exp(-1)\approx 0.37\)</span> of the recovery time having no treatment. Although the recovery time is specific to each patients, i.e. his/her features, the effect of a reduction to 37% of the recovery time without medication is the same for all&nbsp;patients. </p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">exp_recovery_time</span><span class="p">(</span><span class="n">sex</span><span class="p">,</span> <span class="n">age</span><span class="p">,</span> <span class="n">severity</span><span class="p">,</span> <span class="n">medication</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">exp</span><span class="p">(</span><span class="mi">2</span><span class="o">+</span><span class="mf">0.5</span><span class="o">*</span><span class="n">sex</span><span class="o">+</span><span class="mf">0.03</span><span class="o">*</span><span class="n">age</span><span class="o">+</span><span class="mi">2</span><span class="o">*</span><span class="n">severity</span><span class="o">-</span><span class="mi">1</span><span class="o">*</span><span class="n">medication</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">rvs_recovery_time</span><span class="p">(</span><span class="n">sex</span><span class="p">,</span> <span class="n">age</span><span class="p">,</span> <span class="n">severity</span><span class="p">,</span> <span class="n">medication</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">stats</span><span class="o">.</span><span class="n">poisson</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">exp_recovery_time</span><span class="p">(</span><span class="n">sex</span><span class="p">,</span> <span class="n">age</span><span class="p">,</span> <span class="n">severity</span><span class="p">,</span> <span class="n">medication</span><span class="p">))</span>
</pre></div>


<p>For the features of the patients we will use a Beta distribution to show how badly the disease struck the patients, a Gamma distribution for the age of our patients and a Bernoulli distribution for the gender of the&nbsp;patients.</p>
<div class="highlight"><pre><span></span><span class="n">N</span> <span class="o">=</span> <span class="mi">10000</span>  <span class="c1"># number of observations, i.e. patients</span>
<span class="n">sexes</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">N</span><span class="p">)</span>  <span class="c1"># sex == 1 if male otherwise female</span>
<span class="n">ages_dist</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">gamma</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="n">ages</span> <span class="o">=</span> <span class="n">ages_dist</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">N</span><span class="p">)</span>
<span class="n">sev_dist</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">beta</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">)</span>
<span class="n">severties</span> <span class="o">=</span> <span class="n">sev_dist</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">N</span><span class="p">)</span>
</pre></div>


<p>It&#8217;s always a good idea to take a look at the nontrivial&nbsp;distributions:</p>
<div class="highlight"><pre><span></span><span class="n">f</span><span class="p">,</span> <span class="p">(</span><span class="n">ax1</span><span class="p">,</span> <span class="n">ax2</span><span class="p">)</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;age&#39;</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;years&#39;</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;density&#39;</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">ages</span><span class="p">))</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;severity&#39;</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;0 = lowest severity, 1 = highest severity&#39;</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;density&#39;</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">distplot</span><span class="p">(</span><span class="n">ages</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax1</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">distplot</span><span class="p">(</span><span class="n">severties</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">();</span>
</pre></div>


<p align="center">
<img class="noZoom" src="/images/cips_age_severity_density.png">
</p>

<h3>Randomized&nbsp;trial</h3>
<p>In a controlled randomized trial we randomly select patients and assign them with a chance of 50% to either treatment or control. Therefore, the assignment of treatement is completely random and&nbsp;independent. </p>
<div class="highlight"><pre><span></span><span class="n">meds</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">N</span><span class="p">)</span>
</pre></div>


<p>We assemble everything in a dataframe also including a constant&nbsp;column.</p>
<div class="highlight"><pre><span></span><span class="n">const</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>
<span class="n">df_rnd</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="nb">dict</span><span class="p">(</span><span class="n">sex</span><span class="o">=</span><span class="n">sexes</span><span class="p">,</span> <span class="n">age</span><span class="o">=</span><span class="n">ages</span><span class="p">,</span> <span class="n">severity</span><span class="o">=</span><span class="n">severties</span><span class="p">,</span> <span class="n">medication</span><span class="o">=</span><span class="n">meds</span><span class="p">,</span> <span class="n">const</span><span class="o">=</span><span class="n">const</span><span class="p">))</span>
<span class="n">features</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;sex&#39;</span><span class="p">,</span> <span class="s1">&#39;age&#39;</span><span class="p">,</span> <span class="s1">&#39;severity&#39;</span><span class="p">,</span> <span class="s1">&#39;medication&#39;</span><span class="p">,</span> <span class="s1">&#39;const&#39;</span><span class="p">]</span>
<span class="n">df_rnd</span> <span class="o">=</span> <span class="n">df_rnd</span><span class="p">[</span><span class="n">features</span><span class="p">]</span> <span class="c1"># to enforce column order</span>
<span class="n">df_rnd</span><span class="p">[</span><span class="s1">&#39;recovery&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df_rnd</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">rvs_recovery_time</span><span class="p">(</span><span class="o">*</span><span class="n">x</span><span class="p">)</span> <span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">df_rnd</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>


<table>
<thead>
<tr>
<th align="right"></th>
<th align="right">sex</th>
<th align="right">age</th>
<th align="right">severity</th>
<th align="right">medication</th>
<th align="right">const</th>
<th align="right">recovery</th>
</tr>
</thead>
<tbody>
<tr>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">24.5187</td>
<td align="right">0.85895</td>
<td align="right">1</td>
<td align="right">1</td>
<td align="right">34</td>
</tr>
<tr>
<td align="right">1</td>
<td align="right">1</td>
<td align="right">11.0802</td>
<td align="right">0.905123</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">97</td>
</tr>
<tr>
<td align="right">2</td>
<td align="right">0</td>
<td align="right">37.0149</td>
<td align="right">0.601475</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">77</td>
</tr>
<tr>
<td align="right">3</td>
<td align="right">0</td>
<td align="right">35.6577</td>
<td align="right">0.74984</td>
<td align="right">1</td>
<td align="right">1</td>
<td align="right">39</td>
</tr>
<tr>
<td align="right">4</td>
<td align="right">0</td>
<td align="right">36.7352</td>
<td align="right">0.38546</td>
<td align="right">1</td>
<td align="right">1</td>
<td align="right">18</td>
</tr>
</tbody>
</table>
<div class="highlight"><pre><span></span><span class="n">df_rnd</span><span class="o">.</span><span class="n">describe</span><span class="p">()</span>
</pre></div>


<table>
<thead>
<tr>
<th align="left"></th>
<th align="right">sex</th>
<th align="right">age</th>
<th align="right">severity</th>
<th align="right">medication</th>
<th align="right">const</th>
<th align="right">recovery</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">count</td>
<td align="right">10000</td>
<td align="right">10000</td>
<td align="right">10000</td>
<td align="right">10000</td>
<td align="right">10000</td>
<td align="right">10000</td>
</tr>
<tr>
<td align="left">mean</td>
<td align="right">0.498700</td>
<td align="right">32.160968</td>
<td align="right">0.666299</td>
<td align="right">0.497400</td>
<td align="right">1</td>
<td align="right">76.085700</td>
</tr>
<tr>
<td align="left">std</td>
<td align="right">0.500023</td>
<td align="right">11.243333</td>
<td align="right">0.20101</td>
<td align="right">0.500018</td>
<td align="right">0</td>
<td align="right">63.304659</td>
</tr>
<tr>
<td align="left">min</td>
<td align="right">0.000000</td>
<td align="right">4.508904</td>
<td align="right">0.0298182</td>
<td align="right">0.000000</td>
<td align="right">1</td>
<td align="right">0.000000</td>
</tr>
<tr>
<td align="left">25%</td>
<td align="right">0.000000</td>
<td align="right">24.044093</td>
<td align="right">0.525905</td>
<td align="right">0.000000</td>
<td align="right">1</td>
<td align="right">33.000000</td>
</tr>
<tr>
<td align="left">50%</td>
<td align="right">0.000000</td>
<td align="right">30.760101</td>
<td align="right">0.693532</td>
<td align="right">0.000000</td>
<td align="right">1</td>
<td align="right">57.000000</td>
</tr>
<tr>
<td align="left">75%</td>
<td align="right">1.000000</td>
<td align="right">38.922208</td>
<td align="right">0.829290</td>
<td align="right">1.000000</td>
<td align="right">1</td>
<td align="right">99.000000</td>
</tr>
<tr>
<td align="left">max</td>
<td align="right">1.000000</td>
<td align="right">98.330906</td>
<td align="right">0.999327</td>
<td align="right">1.000000</td>
<td align="right">1</td>
<td align="right">805.000000</td>
</tr>
</tbody>
</table>
<p>By construction, there is no correlation between medication and any other&nbsp;covariate.</p>
<div class="highlight"><pre><span></span><span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">df_rnd</span><span class="o">.</span><span class="n">corr</span><span class="p">(),</span> <span class="n">vmin</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mi">1</span><span class="p">);</span>
</pre></div>


<p align="center">
<img class="noZoom" src="/images/cips_heatmap_nocorr.png">
</p>

<p>To get started we use a Poisson regression to estimate the coefficients of our formula for <span class="math">\(\mathbf{E}(t_{recovery})\)</span> from the generated data. Of course we expect to see approximately the same coefficients since Poisson regression assumes the exact same model that generated our&nbsp;data.</p>
<div class="highlight"><pre><span></span><span class="n">glm</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">GLM</span><span class="p">(</span><span class="n">df_rnd</span><span class="p">[</span><span class="s1">&#39;recovery&#39;</span><span class="p">],</span> <span class="n">df_rnd</span><span class="p">[</span><span class="n">features</span><span class="p">],</span> <span class="n">family</span><span class="o">=</span><span class="n">sm</span><span class="o">.</span><span class="n">families</span><span class="o">.</span><span class="n">Poisson</span><span class="p">())</span>
<span class="n">res</span> <span class="o">=</span> <span class="n">glm</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="n">res</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>


<table>
<caption>Generalized Linear Model Regression Results</caption>
<tbody><tr>
  <th>Dep. Variable:</th>      <td>recovery</td>     <th>  No. Observations:  </th>  <td> 10000</td> 
</tr>
<tr>
  <th>Model:</th>                 <td><span class="caps"><span class="caps">GLM</span></span></td>       <th>  Df Residuals:      </th>  <td>  9995</td> 
</tr>
<tr>
  <th>Model Family:</th>        <td>Poisson</td>     <th>  Df Model:          </th>  <td>     4</td> 
</tr>
<tr>
  <th>Link Function:</th>         <td>log</td>       <th>  Scale:             </th>    <td>1.0</td>  
</tr>
<tr>
  <th>Method:</th>               <td><span class="caps"><span class="caps">IRLS</span></span></td>       <th>  Log-Likelihood:    </th> <td> -34429.</td>
</tr>
<tr>
  <th>Date:</th>           <td>Sat, 15 Apr 2017</td> <th>  Deviance:          </th> <td>  10080.</td>
</tr>
<tr>
  <th>Time:</th>               <td>20:13:45</td>     <th>  Pearson chi2:      </th> <td>1.00e+04</td>
</tr>
<tr>
  <th>No. Iterations:</th>         <td>5</td>        <th>                     </th>     <td> </td>   
</tr>
</tbody></table>

<table>
<tbody><tr>
       <td></td>         <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P&gt;|z|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>sex</th>        <td>    0.4994</td> <td>    0.002</td> <td>  211.934</td> <td> 0.000</td> <td>    0.495</td> <td>    0.504</td>
</tr>
<tr>
  <th>age</th>        <td>    0.0301</td> <td> 8.95e-05</td> <td>  335.807</td> <td> 0.000</td> <td>    0.030</td> <td>    0.030</td>
</tr>
<tr>
  <th>severity</th>   <td>    2.0000</td> <td>    0.006</td> <td>  309.610</td> <td> 0.000</td> <td>    1.987</td> <td>    2.013</td>
</tr>
<tr>
  <th>medication</th> <td>   -1.0024</td> <td>    0.003</td> <td> -387.721</td> <td> 0.000</td> <td>   -1.007</td> <td>   -0.997</td>
</tr>
<tr>
  <th>const</th>      <td>    1.9990</td> <td>    0.006</td> <td>  326.234</td> <td> 0.000</td> <td>    1.987</td> <td>    2.011</td>
</tr>
</tbody></table>

<p>Now we use a randome forest which is a pretty standard machine learning method to estimate the individual effects of the treatment on the patients. We fit the model and predict for each patient the recovery time assuming medication, i.e. medication column is 1, as well as assuming no medication, i.e. medication column is 0. Subsequently we divide the prediction assuming medication by the prediction assuming no medication to get an estimation of the treatment&#8217;s&nbsp;effect.</p>
<div class="highlight"><pre><span></span><span class="n">reg</span> <span class="o">=</span> <span class="n">RandomForestRegressor</span><span class="p">()</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">df_rnd</span><span class="p">[</span><span class="n">features</span><span class="p">]</span><span class="o">.</span><span class="n">as_matrix</span><span class="p">()</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">df_rnd</span><span class="p">[</span><span class="s1">&#39;recovery&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
<span class="n">reg</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>


<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_text output_subarea output_execute_result">
<pre>RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,
           max_features='auto', max_leaf_nodes=None, min_samples_leaf=1,
           min_samples_split=2, min_weight_fraction_leaf=0.0,
           n_estimators=10, n_jobs=1, oob_score=False, random_state=None,
           verbose=0, warm_start=False)</pre>
</div>
</div>
</div>
</div>

<div class="highlight"><pre><span></span><span class="n">X_neg</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="c1"># set the medication column to 0</span>
<span class="n">X_neg</span><span class="p">[:,</span> <span class="n">df_rnd</span><span class="o">.</span><span class="n">columns</span><span class="o">.</span><span class="n">get_loc</span><span class="p">(</span><span class="s1">&#39;medication&#39;</span><span class="p">)]</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">X_pos</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="c1"># set the medication column to 1</span>
<span class="n">X_pos</span><span class="p">[:,</span> <span class="n">df_rnd</span><span class="o">.</span><span class="n">columns</span><span class="o">.</span><span class="n">get_loc</span><span class="p">(</span><span class="s1">&#39;medication&#39;</span><span class="p">)]</span> <span class="o">=</span> <span class="mi">1</span>

<span class="n">preds_rnd</span> <span class="o">=</span> <span class="n">reg</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_pos</span><span class="p">)</span> <span class="o">/</span> <span class="n">reg</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_neg</span><span class="p">)</span>
</pre></div>


<p>Let&#8217;s take a look at the distribution of individual effects. Even though we are assuming no model by using a random forest, our estimations of the treatment effect look&nbsp;decent.</p>
<div class="highlight"><pre><span></span><span class="n">ax</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">distplot</span><span class="p">(</span><span class="n">preds_rnd</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;treatment effect&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;density&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">preds_rnd</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;truth&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">();</span>
</pre></div>


<p align="center">
<img class="noZoom" src="/images/cips_treatment_effect_random.png">
</p>

<h3>Nonrandomized&nbsp;trial</h3>
<p>To make things a bit more interesting, we put now patients on a treatment depending on their sex and severity of the illness. Since men often suffer more than women from the same illness, e.g. <a href="https://en.wikipedia.org/wiki/Man_flu">man flu</a>, they tend to complain more and thus are more likely to convince the doctor of prescibing a medication. Thereafter we generate the recovery time again and follow the same procedure as before in the randomized&nbsp;trial.</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">get_medication</span><span class="p">(</span><span class="n">sex</span><span class="p">,</span> <span class="n">age</span><span class="p">,</span> <span class="n">severity</span><span class="p">,</span> <span class="n">medication</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">):</span>
    <span class="k">return</span> <span class="nb">int</span><span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="mi">3</span><span class="o">*</span><span class="n">sex</span> <span class="o">+</span> <span class="mi">2</span><span class="o">/</span><span class="mi">3</span><span class="o">*</span><span class="n">severity</span> <span class="o">+</span> <span class="mf">0.15</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mf">0.8</span><span class="p">)</span>

<span class="n">df_obs</span> <span class="o">=</span> <span class="n">df_rnd</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s1">&#39;recovery&#39;</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">df_obs</span><span class="p">[</span><span class="s1">&#39;medication&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df_obs</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">get_medication</span><span class="p">(</span><span class="o">*</span><span class="n">x</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">df_obs</span><span class="p">[</span><span class="s1">&#39;recovery&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df_obs</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">rvs_recovery_time</span><span class="p">(</span><span class="o">*</span><span class="n">x</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">df_obs</span><span class="o">.</span><span class="n">describe</span><span class="p">()</span>
</pre></div>


<table>
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>sex</th>
      <th>age</th>
      <th>severity</th>
      <th>medication</th>
      <th>const</th>
      <th>recovery</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>10000.000000</td>
      <td>10000.000000</td>
      <td>10000.000000</td>
      <td>10000.000000</td>
      <td>10000.0</td>
      <td>10000.000000</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>0.498700</td>
      <td>32.160968</td>
      <td>0.666299</td>
      <td>0.251900</td>
      <td>1.0</td>
      <td>85.029100</td>
    </tr>
    <tr>
      <th>std</th>
      <td>0.500023</td>
      <td>11.243333</td>
      <td>0.201010</td>
      <td>0.434126</td>
      <td>0.0</td>
      <td>51.400825</td>
    </tr>
    <tr>
      <th>min</th>
      <td>0.000000</td>
      <td>4.508904</td>
      <td>0.029818</td>
      <td>0.000000</td>
      <td>1.0</td>
      <td>8.000000</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>0.000000</td>
      <td>24.044093</td>
      <td>0.525905</td>
      <td>0.000000</td>
      <td>1.0</td>
      <td>50.000000</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>0.000000</td>
      <td>30.760101</td>
      <td>0.693532</td>
      <td>0.000000</td>
      <td>1.0</td>
      <td>73.000000</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>1.000000</td>
      <td>38.922208</td>
      <td>0.829290</td>
      <td>1.000000</td>
      <td>1.0</td>
      <td>106.000000</td>
    </tr>
    <tr>
      <th>max</th>
      <td>1.000000</td>
      <td>98.330906</td>
      <td>0.999327</td>
      <td>1.000000</td>
      <td>1.0</td>
      <td>624.000000</td>
    </tr>
  </tbody>
</table>

<div class="highlight"><pre><span></span><span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">df_obs</span><span class="o">.</span><span class="n">corr</span><span class="p">(),</span> <span class="n">vmin</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mi">1</span><span class="p">);</span>
</pre></div>


<p align="center">
<img class="noZoom" src="/images/cips_heatmap_corr.png">
</p>

<div class="highlight"><pre><span></span><span class="n">glm</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">GLM</span><span class="p">(</span><span class="n">df_obs</span><span class="p">[</span><span class="s1">&#39;recovery&#39;</span><span class="p">],</span> <span class="n">df_obs</span><span class="p">[</span><span class="n">features</span><span class="p">],</span> <span class="n">family</span><span class="o">=</span><span class="n">sm</span><span class="o">.</span><span class="n">families</span><span class="o">.</span><span class="n">Poisson</span><span class="p">())</span>
<span class="n">res</span> <span class="o">=</span> <span class="n">glm</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="n">res</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>


<table>
<caption>Generalized Linear Model Regression Results</caption>
<tbody><tr>
  <th>Dep. Variable:</th>      <td>recovery</td>     <th>  No. Observations:  </th>  <td> 10000</td> 
</tr>
<tr>
  <th>Model:</th>                 <td><span class="caps"><span class="caps">GLM</span></span></td>       <th>  Df Residuals:      </th>  <td>  9995</td> 
</tr>
<tr>
  <th>Model Family:</th>        <td>Poisson</td>     <th>  Df Model:          </th>  <td>     4</td> 
</tr>
<tr>
  <th>Link Function:</th>         <td>log</td>       <th>  Scale:             </th>    <td>1.0</td>  
</tr>
<tr>
  <th>Method:</th>               <td><span class="caps"><span class="caps">IRLS</span></span></td>       <th>  Log-Likelihood:    </th> <td> -35645.</td>
</tr>
<tr>
  <th>Date:</th>           <td>Sat, 15 Apr 2017</td> <th>  Deviance:          </th> <td>  10018.</td>
</tr>
<tr>
  <th>Time:</th>               <td>20:13:47</td>     <th>  Pearson chi2:      </th> <td>9.98e+03</td>
</tr>
<tr>
  <th>No. Iterations:</th>         <td>5</td>        <th>                     </th>     <td> </td>   
</tr>
</tbody></table>

<table>
<tbody><tr>
       <td></td>         <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P&gt;|z|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>sex</th>        <td>    0.5043</td> <td>    0.002</td> <td>  203.256</td> <td> 0.000</td> <td>    0.499</td> <td>    0.509</td>
</tr>
<tr>
  <th>age</th>        <td>    0.0299</td> <td> 8.58e-05</td> <td>  349.024</td> <td> 0.000</td> <td>    0.030</td> <td>    0.030</td>
</tr>
<tr>
  <th>severity</th>   <td>    1.9996</td> <td>    0.006</td> <td>  313.055</td> <td> 0.000</td> <td>    1.987</td> <td>    2.012</td>
</tr>
<tr>
  <th>medication</th> <td>   -1.0063</td> <td>    0.003</td> <td> -302.201</td> <td> 0.000</td> <td>   -1.013</td> <td>   -1.000</td>
</tr>
<tr>
  <th>const</th>      <td>    2.0013</td> <td>    0.006</td> <td>  340.305</td> <td> 0.000</td> <td>    1.990</td> <td>    2.013</td>
</tr>
</tbody></table>

<p>The first stunning result is that the Poisson regression is still able to correctly estimate the coefficients of our model. This is due to the <a href="http://www.preventionresearch.org/wp-content/uploads/2011/07/SPR-Propensity-pc-workshop-slides.pdf">model dependence</a> and in realistic cases actually a bad thing. In a nutshell, model dependence means that the inference of the causal effect depends on the chosen model. In our case, the assumptions about the relation of the covariates in the Poisson regression extrapolates our data and thus makes our results heavily depend on the Poisson model. Since we also used a Poisson model to generate the data we are lucky but this is in reality rarely the case. Let&#8217;s check how the random forest&nbsp;performs.</p>
<div class="highlight"><pre><span></span><span class="n">reg</span> <span class="o">=</span> <span class="n">RandomForestRegressor</span><span class="p">()</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">df_obs</span><span class="p">[</span><span class="n">features</span><span class="p">]</span><span class="o">.</span><span class="n">as_matrix</span><span class="p">()</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">df_obs</span><span class="p">[</span><span class="s1">&#39;recovery&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
<span class="n">reg</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>


<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_text output_subarea output_execute_result">
<pre>RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,
           max_features='auto', max_leaf_nodes=None, min_samples_leaf=1,
           min_samples_split=2, min_weight_fraction_leaf=0.0,
           n_estimators=10, n_jobs=1, oob_score=False, random_state=None,
           verbose=0, warm_start=False)</pre>
</div>
</div>
</div>
</div>

<div class="highlight"><pre><span></span><span class="n">X_neg</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">X_neg</span><span class="p">[:,</span> <span class="n">df_obs</span><span class="o">.</span><span class="n">columns</span><span class="o">.</span><span class="n">get_loc</span><span class="p">(</span><span class="s1">&#39;medication&#39;</span><span class="p">)]</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">X_pos</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">X_pos</span><span class="p">[:,</span> <span class="n">df_obs</span><span class="o">.</span><span class="n">columns</span><span class="o">.</span><span class="n">get_loc</span><span class="p">(</span><span class="s1">&#39;medication&#39;</span><span class="p">)]</span> <span class="o">=</span> <span class="mi">1</span>

<span class="n">preds_no_rnd</span> <span class="o">=</span> <span class="n">reg</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_pos</span><span class="p">)</span> <span class="o">/</span> <span class="n">reg</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_neg</span><span class="p">)</span>

<span class="n">ax</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">distplot</span><span class="p">(</span><span class="n">preds_no_rnd</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;treatment effect&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;density&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">preds_no_rnd</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;truth&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">();</span>
</pre></div>


<p align="center">
<img class="noZoom" src="/images/cips_treatment_effect_no_random.png">
</p>

<p>The distribution is now quite skewed and we can see that for a lot of our patients their individual treatment effect is heavily underestimated. This due to the fact that the random forest overrates the impact of a patient&#8217;s sex which is highly correlated with the&nbsp;medication.</p>
<h3>Inverse probability of treatment&nbsp;weighting</h3>
<p>To deminish the impact of other covariates onto the effect of medication we will no calculate the propensity score and use inverse probability of treatment weighting (<span class="caps">IPTW</span>). In order to do that we use a classification to predict the probability of a patient to be treated. This can be accomplished by Scikit-Learn&#8217;s <code>predict_proba</code> method that is available for most classificators. Don&#8217;t be fooled by the name though, in most cases (logistic regression is an exception) the probabilites are not calibrated and cannot be relied on. To fix this, we use the <a href="http://scikit-learn.org/stable/modules/calibration.html">CalibratedClassifierCV</a> in order to get proper probabilities (and it doesn&#8217;t hurt applying it for logistic regression too). After that we calculate the inverse probability of treatment weights and pass those as sample weights to the estimator during the&nbsp;fit.</p>
<div class="highlight"><pre><span></span><span class="c1"># classifier to estimate the propensity score</span>
<span class="bp">cls</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="c1">#cls = GaussianNB()  # another possible propensity score estimator</span>

<span class="c1"># calibration of the classifier</span>
<span class="bp">cls</span> <span class="o">=</span> <span class="n">CalibratedClassifierCV</span><span class="p">(</span><span class="bp">cls</span><span class="p">)</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">df_obs</span><span class="p">[</span><span class="n">features</span><span class="p">]</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="s1">&#39;medication&#39;</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">as_matrix</span><span class="p">()</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">df_obs</span><span class="p">[</span><span class="s1">&#39;medication&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
<span class="bp">cls</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>


<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_text output_subarea output_execute_result">
<pre>CalibratedClassifierCV(base_estimator=LogisticRegression(C=1.0, class_weight=None, 
          dual=False, fit_intercept=True, intercept_scaling=1, max_iter=100, 
          multi_class='ovr', n_jobs=1, penalty='l2', random_state=42, 
          solver='liblinear', tol=0.0001, verbose=0, warm_start=False), 
            cv=3, method='sigmoid')</pre>
</div>
</div>
</div>
</div>

<div class="highlight"><pre><span></span><span class="n">propensity</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>
<span class="n">propensity</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>


<table>
<thead>
<tr>
<th align="left"></th>
<th align="right">0</th>
<th align="right">1</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">0</td>
<td align="right">0.947430</td>
<td align="right">0.052570</td>
</tr>
<tr>
<td align="left">1</td>
<td align="right">0.170632</td>
<td align="right">0.829368</td>
</tr>
<tr>
<td align="left">2</td>
<td align="right">0.992034</td>
<td align="right">0.007966</td>
</tr>
<tr>
<td align="left">3</td>
<td align="right">0.975970</td>
<td align="right">0.024030</td>
</tr>
<tr>
<td align="left">4</td>
<td align="right">0.998434</td>
<td align="right">0.001566</td>
</tr>
</tbody>
</table>
<p>We can see that the propensity scores of our patients in the randomized trial vary a lot as&nbsp;expected.</p>
<div class="highlight"><pre><span></span><span class="n">ax</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">distplot</span><span class="p">(</span><span class="n">propensity</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Propensity scores of nonrandomized trial&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;propensity scores&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;density&#39;</span><span class="p">);</span>
</pre></div>


<p align="center">
<img class="noZoom" src="/images/cips_propensity_scores_no_random.png">
</p>

<p>Only for comparison we can also plot the propensity scores of the randomized trail and check if the propensity score is <span class="math">\(\frac{1}{2}\)</span> as&nbsp;expected.</p>
<div class="highlight"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">df_rnd</span><span class="p">[</span><span class="n">features</span><span class="p">]</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="s1">&#39;medication&#39;</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">as_matrix</span><span class="p">()</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">df_rnd</span><span class="p">[</span><span class="s1">&#39;medication&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
<span class="bp">cls</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">distplot</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X</span><span class="p">)[:,</span><span class="mi">1</span><span class="p">]);</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Propensity scores of randomized trial&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;propensity scores&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;density&#39;</span><span class="p">);</span>
</pre></div>


<p align="center">
<img class="noZoom" src="/images/cips_propensity_scores_random.png">
</p>

<p>We now calculate at this point the inverse probability of treatment weights (IPTWs) with the help of the propensity scores of the nonrandomized&nbsp;trial.</p>
<div class="highlight"><pre><span></span><span class="c1"># DataFrame&#39;s lookup method extracts the column index </span>
<span class="c1"># provided by df2[&#39;medication&#39;] for each row</span>
<span class="n">df_obs</span><span class="p">[</span><span class="s1">&#39;iptw&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.</span> <span class="o">/</span> <span class="n">propensity</span><span class="o">.</span><span class="n">lookup</span><span class="p">(</span>
   <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">propensity</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="n">df_obs</span><span class="p">[</span><span class="s1">&#39;medication&#39;</span><span class="p">])</span>

<span class="n">df_obs</span><span class="o">.</span><span class="n">describe</span><span class="p">()</span>
</pre></div>


<table>
<thead>
<tr>
<th align="left"></th>
<th align="right">sex</th>
<th align="right">age</th>
<th align="right">severity</th>
<th align="right">medication</th>
<th align="right">const</th>
<th align="right">recovery</th>
<th align="right">iptw</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">count</td>
<td align="right">10000</td>
<td align="right">10000</td>
<td align="right">10000</td>
<td align="right">10000</td>
<td align="right">10000</td>
<td align="right">10000</td>
<td align="right">10000</td>
</tr>
<tr>
<td align="left">mean</td>
<td align="right">0.498700</td>
<td align="right">32.160968</td>
<td align="right">0.666299</td>
<td align="right">0.251900</td>
<td align="right">1.0</td>
<td align="right">85.029100</td>
<td align="right">1.860499</td>
</tr>
<tr>
<td align="left">std</td>
<td align="right">0.500023</td>
<td align="right">11.243333</td>
<td align="right">0.201010</td>
<td align="right">0.434126</td>
<td align="right">0.0</td>
<td align="right">51.400825</td>
<td align="right">4.455724</td>
</tr>
<tr>
<td align="left">min</td>
<td align="right">0.000000</td>
<td align="right">4.508904</td>
<td align="right">0.029818</td>
<td align="right">0.000000</td>
<td align="right">1.0</td>
<td align="right">8.000000</td>
<td align="right">1.000105</td>
</tr>
<tr>
<td align="left">25%</td>
<td align="right">0.000000</td>
<td align="right">24.044093</td>
<td align="right">0.525905</td>
<td align="right">0.000000</td>
<td align="right">1.0</td>
<td align="right">50.000000</td>
<td align="right">1.016131</td>
</tr>
<tr>
<td align="left">50%</td>
<td align="right">0.000000</td>
<td align="right">30.760101</td>
<td align="right">0.693532</td>
<td align="right">0.000000</td>
<td align="right">1.0</td>
<td align="right">73.000000</td>
<td align="right">1.093217</td>
</tr>
<tr>
<td align="left">75%</td>
<td align="right">1.000000</td>
<td align="right">38.922208</td>
<td align="right">0.829290</td>
<td align="right">1.000000</td>
<td align="right">1.0</td>
<td align="right">106.000000</td>
<td align="right">1.449351</td>
</tr>
<tr>
<td align="left">max</td>
<td align="right">1.000000</td>
<td align="right">98.330906</td>
<td align="right">0.999327</td>
<td align="right">1.000000</td>
<td align="right">1.000000</td>
<td align="right">624.000000</td>
<td align="right">184.561863</td>
</tr>
</tbody>
</table>
<p>The poisson regression benefits from using the IPTWs as weights since the Z-scores of the coefficients&nbsp;increase.</p>
<div class="highlight"><pre><span></span><span class="n">glm</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">GLM</span><span class="p">(</span><span class="n">df_obs</span><span class="p">[</span><span class="s1">&#39;recovery&#39;</span><span class="p">],</span> <span class="n">df_obs</span><span class="p">[</span><span class="n">features</span><span class="p">],</span> 
             <span class="n">family</span><span class="o">=</span><span class="n">sm</span><span class="o">.</span><span class="n">families</span><span class="o">.</span><span class="n">Poisson</span><span class="p">(),</span>
             <span class="n">freq_weights</span><span class="o">=</span><span class="n">df_obs</span><span class="p">[</span><span class="s1">&#39;iptw&#39;</span><span class="p">])</span>
<span class="n">res</span> <span class="o">=</span> <span class="n">glm</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="n">res</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>


<table>
<caption>Generalized Linear Model Regression Results</caption>
<tbody><tr>
  <th>Dep. Variable:</th>      <td>recovery</td>     <th>  No. Observations:  </th>  <td> 10000</td> 
</tr>
<tr>
  <th>Model:</th>                 <td><span class="caps"><span class="caps">GLM</span></span></td>       <th>  Df Residuals:      </th>  <td> 18599</td> 
</tr>
<tr>
  <th>Model Family:</th>        <td>Poisson</td>     <th>  Df Model:          </th>  <td>     4</td> 
</tr>
<tr>
  <th>Link Function:</th>         <td>log</td>       <th>  Scale:             </th>    <td>1.0</td>  
</tr>
<tr>
  <th>Method:</th>               <td><span class="caps"><span class="caps">IRLS</span></span></td>       <th>  Log-Likelihood:    </th> <td> -64795.</td>
</tr>
<tr>
  <th>Date:</th>           <td>Sat, 15 Apr 2017</td> <th>  Deviance:          </th> <td>  18482.</td>
</tr>
<tr>
  <th>Time:</th>               <td>20:13:49</td>     <th>  Pearson chi2:      </th> <td>1.83e+04</td>
</tr>
<tr>
  <th>No. Iterations:</th>         <td>5</td>        <th>                     </th>     <td> </td>   
</tr>
</tbody></table>

<table>
<tbody><tr>
       <td></td>         <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P&gt;|z|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>sex</th>        <td>    0.5018</td> <td>    0.002</td> <td>  294.822</td> <td> 0.000</td> <td>    0.498</td> <td>    0.505</td>
</tr>
<tr>
  <th>age</th>        <td>    0.0298</td> <td> 6.56e-05</td> <td>  454.701</td> <td> 0.000</td> <td>    0.030</td> <td>    0.030</td>
</tr>
<tr>
  <th>severity</th>   <td>    2.0016</td> <td>    0.005</td> <td>  429.319</td> <td> 0.000</td> <td>    1.992</td> <td>    2.011</td>
</tr>
<tr>
  <th>medication</th> <td>   -1.0017</td> <td>    0.002</td> <td> -534.776</td> <td> 0.000</td> <td>   -1.005</td> <td>   -0.998</td>
</tr>
<tr>
  <th>const</th>      <td>    2.0055</td> <td>    0.005</td> <td>  441.204</td> <td> 0.000</td> <td>    1.997</td> <td>    2.014</td>
</tr>
</tbody></table>

<p>Let&#8217;s check how our random forest does with the help of&nbsp;IPTWs.</p>
<div class="highlight"><pre><span></span><span class="n">reg</span> <span class="o">=</span> <span class="n">RandomForestRegressor</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">df_obs</span><span class="p">[</span><span class="n">features</span><span class="p">]</span><span class="o">.</span><span class="n">as_matrix</span><span class="p">()</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">df_obs</span><span class="p">[</span><span class="s1">&#39;recovery&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
<span class="n">reg</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="n">df_obs</span><span class="p">[</span><span class="s1">&#39;iptw&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
</pre></div>


<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_text output_subarea output_execute_result">
<pre>RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,
           max_features='auto', max_leaf_nodes=None, min_samples_leaf=1,
           min_samples_split=2, min_weight_fraction_leaf=0.0,
           n_estimators=10, n_jobs=1, oob_score=False, random_state=42,
           verbose=0, warm_start=False)</pre>
</div>
</div>
</div>
</div>

<div class="highlight"><pre><span></span><span class="n">X_neg</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">X_neg</span><span class="p">[:,</span> <span class="n">df_obs</span><span class="o">.</span><span class="n">columns</span><span class="o">.</span><span class="n">get_loc</span><span class="p">(</span><span class="s1">&#39;medication&#39;</span><span class="p">)]</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">X_pos</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">X_pos</span><span class="p">[:,</span> <span class="n">df_obs</span><span class="o">.</span><span class="n">columns</span><span class="o">.</span><span class="n">get_loc</span><span class="p">(</span><span class="s1">&#39;medication&#39;</span><span class="p">)]</span> <span class="o">=</span> <span class="mi">1</span>

<span class="n">preds_propensity</span> <span class="o">=</span> <span class="n">reg</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_pos</span><span class="p">)</span> <span class="o">/</span> <span class="n">reg</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_neg</span><span class="p">)</span>

<span class="n">ax</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">distplot</span><span class="p">(</span><span class="n">preds_propensity</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;treatment effect&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;density&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">preds_propensity</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;truth&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">();</span>
</pre></div>


<p align="center">
<img class="noZoom" src="/images/cips_treatment_effect_no_random_iptw.png">
</p>

<p>After taking a brief look at the distribution we see that using <span class="caps">IPTW</span> drastically improved the estimation of the treatment&#8217;s causal effect. On second glance though, it can also be seen that for a few samples we have over-estimation beyond 1. Looking at those patients it can be seen that all of them are female. This indicates that the causal effect for these cases could not be captured maybe due to the bagging approach in random forests. For most of the patients the estimation of the causal effect is improved&nbsp;though.</p>
<p>A direct comparison is given below, showing the estimations of treatment effects for the <em>randomized</em> trail, the <em>non-randomized</em> trail and the <em>non-randomized with <span class="caps">IPTW</span></em>&nbsp;application.</p>
<div class="highlight"><pre><span></span><span class="n">sns</span><span class="o">.</span><span class="n">distplot</span><span class="p">(</span><span class="n">preds_rnd</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;randomized&#39;</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">distplot</span><span class="p">(</span><span class="n">preds_no_rnd</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;non-randomized&#39;</span><span class="p">)</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">distplot</span><span class="p">(</span><span class="n">preds_propensity</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;non-randomized with IPTW&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;treatment effect&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;density&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">();</span>
</pre></div>


<p align="center">
<img class="noZoom" src="/images/cips_treatment_effect_no_random_cmp.png">
</p>

<p>The actual trick of <span class="caps">IPTW</span> is that sample weights are chosen in such a way that the correlation of other covariates and the medication is decreased. With the help of a weighted correlation this can also be illustrated. Remarkably enough, neither Numpy, Scipy, Pandas nor StatsModels seem to directly provide a weigthed correlation function, only weighted covariance, which we use to define a weighted&nbsp;correlation.</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">weighted_corr</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">w</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">w</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">w</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="n">cov</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cov</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">rowvar</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">aweights</span><span class="o">=</span><span class="n">w</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">sigma</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">cov</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">cov</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">outer</span><span class="p">(</span><span class="n">sigma</span><span class="p">,</span> <span class="n">sigma</span><span class="p">)</span>
</pre></div>


<p>Here is the original correlation of the nonrandomized trial again by setting all weights to&nbsp;1.</p>
<div class="highlight"><pre><span></span><span class="n">sel_cols</span> <span class="o">=</span> <span class="p">[</span><span class="n">col</span> <span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="n">df_obs</span><span class="o">.</span><span class="n">columns</span> <span class="k">if</span> <span class="n">col</span> <span class="o">!=</span> <span class="s1">&#39;iptw&#39;</span><span class="p">]</span>
<span class="n">orig_corr</span> <span class="o">=</span> <span class="n">weighted_corr</span><span class="p">(</span><span class="n">df_obs</span><span class="p">[</span><span class="n">sel_cols</span><span class="p">]</span><span class="o">.</span><span class="n">as_matrix</span><span class="p">(),</span> <span class="n">w</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">df_obs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
<span class="n">orig_corr</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">orig_corr</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">sel_cols</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">sel_cols</span><span class="p">)</span>
<span class="n">orig_corr</span>
</pre></div>


<table>
<thead>
<tr>
<th align="left"></th>
<th align="right">sex</th>
<th align="right">age</th>
<th align="right">severity</th>
<th align="right">medication</th>
<th align="right">const</th>
<th align="right">recovery</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">sex</td>
<td align="right">1.000000</td>
<td align="right">-0.013332</td>
<td align="right">-0.012855</td>
<td align="right">0.509914</td>
<td align="right">NaN</td>
<td align="right">0.046250</td>
</tr>
<tr>
<td align="left">age</td>
<td align="right">-0.013332</td>
<td align="right">1.000000</td>
<td align="right">0.005086</td>
<td align="right">-0.002818</td>
<td align="right">NaN</td>
<td align="right">0.622472</td>
</tr>
<tr>
<td align="left">severity</td>
<td align="right">-0.012855</td>
<td align="right">0.005086</td>
<td align="right">1.000000</td>
<td align="right">0.348317</td>
<td align="right">NaN</td>
<td align="right">0.378225</td>
</tr>
<tr>
<td align="left">medication</td>
<td align="right">0.509914</td>
<td align="right">-0.002818</td>
<td align="right">0.348317</td>
<td align="right">1.000000</td>
<td align="right">NaN</td>
<td align="right">-0.276164</td>
</tr>
<tr>
<td align="left">const</td>
<td align="right">NaN</td>
<td align="right">NaN</td>
<td align="right">NaN</td>
<td align="right">NaN</td>
<td align="right">NaN</td>
<td align="right">NaN</td>
</tr>
<tr>
<td align="left">recovery</td>
<td align="right">0.046250</td>
<td align="right">0.622472</td>
<td align="right">0.378225</td>
<td align="right">-0.276164</td>
<td align="right">NaN</td>
<td align="right">1.000000</td>
</tr>
</tbody>
</table>
<div class="highlight"><pre><span></span><span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">orig_corr</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mi">1</span><span class="p">);</span>
</pre></div>


<p align="center">
<img class="noZoom" src="/images/cips_heatmap_corr.png">
</p>

<p>Using the IPTWs the correlation reduces quite a&nbsp;bit.</p>
<div class="highlight"><pre><span></span><span class="n">iptw_corr</span> <span class="o">=</span> <span class="n">weighted_corr</span><span class="p">(</span><span class="n">df_obs</span><span class="p">[</span><span class="n">sel_cols</span><span class="p">]</span><span class="o">.</span><span class="n">as_matrix</span><span class="p">(),</span> <span class="n">w</span><span class="o">=</span><span class="n">df_obs</span><span class="p">[</span><span class="s1">&#39;iptw&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
<span class="n">iptw_corr</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">iptw_corr</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">sel_cols</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">sel_cols</span><span class="p">)</span>
<span class="n">iptw_corr</span>
</pre></div>


<table>
<thead>
<tr>
<th align="left"></th>
<th align="right">sex</th>
<th align="right">age</th>
<th align="right">severity</th>
<th align="right">medication</th>
<th align="right">const</th>
<th align="right">recovery</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">sex</td>
<td align="right">1.000000</td>
<td align="right">-0.001601</td>
<td align="right">-0.154496</td>
<td align="right">0.087948</td>
<td align="right">NaN</td>
<td align="right">0.222515</td>
</tr>
<tr>
<td align="left">age</td>
<td align="right">-0.001601</td>
<td align="right">1.000000</td>
<td align="right">-0.028975</td>
<td align="right">0.014248</td>
<td align="right">NaN</td>
<td align="right">0.461467</td>
</tr>
<tr>
<td align="left">severity</td>
<td align="right">-0.154496</td>
<td align="right">-0.028975</td>
<td align="right">1.000000</td>
<td align="right">0.103541</td>
<td align="right">NaN</td>
<td align="right">0.369249</td>
</tr>
<tr>
<td align="left">medication</td>
<td align="right">0.087948</td>
<td align="right">0.014248</td>
<td align="right">0.103541</td>
<td align="right">1.000000</td>
<td align="right">NaN</td>
<td align="right">-0.531669</td>
</tr>
<tr>
<td align="left">const</td>
<td align="right">NaN</td>
<td align="right">NaN</td>
<td align="right">NaN</td>
<td align="right">NaN</td>
<td align="right">NaN</td>
<td align="right">NaN</td>
</tr>
<tr>
<td align="left">recovery</td>
<td align="right">0.222515</td>
<td align="right">0.461467</td>
<td align="right">0.369249</td>
<td align="right">-0.531669</td>
<td align="right">NaN</td>
<td align="right">1.000000</td>
</tr>
</tbody>
</table>
<div class="highlight"><pre><span></span><span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">iptw_corr</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mi">1</span><span class="p">);</span>
</pre></div>


<p align="center">
<img class="noZoom" src="/images/cips_heatmap_corr_iptw.png">
</p>

<h2>Final&nbsp;notes</h2>
<p>“With great power comes great responsibility” so they say, and <span class="caps">IPTW</span> is surely a weapon of math destruction that needs to be handled carefully. Keep in mind that a controlled randomized experiment remains the gold standard by which to  estimate a causal effect and should always be preferred. But when reality hits us hard and we have just the data, i.e. no influence on the experiment generating it, we saw that <span class="caps">IPTW</span> improves causal inference to some&nbsp;extent. </p>
<p>Furthermore, the demonstrated technique relies on several things. First, <span class="math">\(X\)</span> needs to be <em>admissible</em> — which cannot be practically checked. But we <em>can</em> investigate how the treatment was assigned in our data. In our campaign example, that would mean asking the marketing department about their former strategy when sending vouchers to customers. Then we could verify that the data includes all factors on which their strategy relied. Second, we need an accurate method of estimating the propensity scores for this approach to work. For the sake of simplicity, our demonstration did not check the prediction quality of our machine learning models on a test set, which would be advisable in a real application. The application of machine learning models should always encompass training, validation, test splits and a proper cost&nbsp;functional.</p>
<p>That said, propensity score techniques like <span class="caps">IPTW</span> can be very useful. Results can be improved further by first using only the covariates to estimate the recovery time, followed by a residual training with the treatment and the sample weighting to further guide the machine learning algorithm by isolating the causal effect of the treatment — but this is beyond the scope of this post. An overview of other propensity score methods like propensity score matching, stratification on the propensity score and covariate adjustment using the propensity score are well explained in the <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3144483/">propensity score methods introduction</a> by Peter Austin<sup id="fnref2:austin"><a class="footnote-ref" href="#fn:austin">5</a></sup>.</p>
<h2>PyData meetup&nbsp;talk</h2>
<p>A talk about this blog post was presented at PyData meetup in Berlin, April&nbsp;19th:</p>
<p><span class="videobox">
                    <iframe width="800" height="500"
                        src='https://www.youtube.com/embed/tUq4esYY6CY'
                        frameborder='0' webkitAllowFullScreen
                        mozallowfullscreen allowFullScreen>
                    </iframe>
                </span></p>
<h2>References</h2>
<div class="footnote">
<hr>
<ol>
<li id="fn:stuart">
<p>E. Stuart; <a href="http://www.preventionresearch.org/wp-content/uploads/2011/07/SPR-Propensity-pc-workshop-slides.pdf">The why, when, and how of propensity score methods for estimating causal effects</a>; Johns Hopkins Bloomberg School of Public Health, 2011&#160;<a class="footnote-backref" href="#fnref:stuart" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:rose">
<p>Paul R. Rosenbaum, Donald B. Rubin; &#8220;The Central Role of the Propensity Score in Observational Studies for Causal Effects&#8221;; Biometrika, Vol. 70, No. 1., Apr., 1983, <a href="https://academic.oup.com/biomet/article/70/1/41/240879">pp. 41-55</a>&#160;<a class="footnote-backref" href="#fnref:rose" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
<li id="fn:pearl1">
<p>Judea Pearl; &#8220;<span class="caps">CAUSALITY</span> - Models, Reasoning and Inference&#8221;; 2nd Edition, 2009, <a href="http://bayes.cs.ucla.edu/BOOK-09/ch11-3-5-final.pdf">pp. 348-352</a>&#160;<a class="footnote-backref" href="#fnref:pearl1" title="Jump back to footnote 3 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:pearl1" title="Jump back to footnote 3 in the text">&#8617;</a></p>
</li>
<li id="fn:pearl2">
<p>Judea Pearl; &#8220;<span class="caps">CAUSALITY</span> - Models, Reasoning and Inference&#8221;; 2nd Edition, 2009, <a href="http://bayes.cs.ucla.edu/BOOK-09/ch11-3-2-final.pdf">pp. 341-344</a>&#160;<a class="footnote-backref" href="#fnref:pearl2" title="Jump back to footnote 4 in the text">&#8617;</a></p>
</li>
<li id="fn:austin">
<p>Peter C. Austin; &#8220;An Introduction to Propensity Score Methods for Reducing the Effects of Confounding in Observational Studies&#8221;; Multivariate Behav Res. 2011 May; 46(3): <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3144483/">pp. 399–424</a>&#160;<a class="footnote-backref" href="#fnref:austin" title="Jump back to footnote 5 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:austin" title="Jump back to footnote 5 in the text">&#8617;</a></p>
</li>
</ol>
</div>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-MML-AM_CHTML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js','color.js'], equationNumbers: { autoNumber: 'auto' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
            </div>
            <!-- /.entry-content -->
<section class="well" id="related-posts">
    <h4>Related Posts:</h4>
    <ul>
        <li><a href="https://florianwilhelm.info/2014/07/extending_scikit-learn_with_your_own_regressor/">Extending Scikit-Learn with your own&nbsp;regressor</a></li>
        <li><a href="https://florianwilhelm.info/2016/03/explaining_the_idea_behind_ard/">Explaining the Idea behind <span class="caps">ARD</span> and Bayesian&nbsp;Interpolation</a></li>
        <li><a href="https://florianwilhelm.info/2013/10/handling_big_data_with_python/">Handling Big Data with&nbsp;Python</a></li>
        <li><a href="https://florianwilhelm.info/2019/10/uncertainty_quantification_in_ai/">Are you sure about that?! Uncertainty Quantification in <span class="caps">AI</span></a></li>
        <li><a href="https://florianwilhelm.info/2018/10/performance_evalution_of_gans_in_a_semi-supervised_ocr_use_case/">Performance evaluation of GANs in a semi-supervised <span class="caps">OCR</span> use&nbsp;case</a></li>
    </ul>
</section>
    <hr/>
    <section class="comments" id="comments">
        <h2>Comments</h2>

        <div id="disqus_thread"></div>
        <script type="text/javascript">
            /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
            var disqus_shortname = 'florianwilhelmblog'; // required: replace example with your forum shortname

            var disqus_config = function () {
                this.language = "en";

                        this.page.identifier = '2017-04-15-causal_inference_propensity_score';
                        this.page.url = 'https://florianwilhelm.info/2017/04/causal_inference_propensity_score/';
            };

            /* * * DON'T EDIT BELOW THIS LINE * * */
            (function () {
                var dsq = document.createElement('script');
                dsq.type = 'text/javascript';
                dsq.async = true;
                dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
                (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
            })();
        </script>
        <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by
            Disqus.</a></noscript>
        <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>

    </section>
        </article>
    </section>

        </div>
        <div class="col-sm-3" id="sidebar">
            <aside>
<!-- Sidebar -->
<section class="well well-sm">
  <ul class="list-group list-group-flush">

<!-- Sidebar/Social -->
<li class="list-group-item">
  <h4><i class="fa fa-home fa-lg"></i><span class="icon-label">Social</span></h4>
  <ul class="list-group" id="social">
    <li class="list-group-item"><a href="https://linkedin.com/in/florian-wilhelm-621ba834"><i class="fa fa-linkedin-square fa-lg"></i> LinkedIn</a></li>
    <li class="list-group-item"><a href="https://github.com/FlorianWilhelm"><i class="fa fa-github-square fa-lg"></i> GitHub</a></li>
  </ul>
</li>
<!-- End Sidebar/Social -->

<!-- Sidebar/Tag Cloud -->
<li class="list-group-item">
  <a href="https://florianwilhelm.info/"><h4><i class="fa fa-tags fa-lg"></i><span class="icon-label">Tags</span></h4></a>
  <ul class="list-group list-inline tagcloud" id="tags">
    <li class="list-group-item tag-3">
      <a href="https://florianwilhelm.info/tag/ai/">ai</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="https://florianwilhelm.info/tag/airbyte/">airbyte</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="https://florianwilhelm.info/tag/asynchronous/">asynchronous</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="https://florianwilhelm.info/tag/asyncio/">asyncio</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="https://florianwilhelm.info/tag/bayesian/">bayesian</a>
    </li>
    <li class="list-group-item tag-2">
      <a href="https://florianwilhelm.info/tag/big-data/">big data</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="https://florianwilhelm.info/tag/bokeh/">bokeh</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="https://florianwilhelm.info/tag/causal-inference/">causal inference</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="https://florianwilhelm.info/tag/conference/">conference</a>
    </li>
    <li class="list-group-item tag-3">
      <a href="https://florianwilhelm.info/tag/configuration/">configuration</a>
    </li>
    <li class="list-group-item tag-1">
      <a href="https://florianwilhelm.info/tag/data-science/">data science</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="https://florianwilhelm.info/tag/dbt/">dbt</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="https://florianwilhelm.info/tag/deep-learning/">deep learning</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="https://florianwilhelm.info/tag/event-driven/">event-driven</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="https://florianwilhelm.info/tag/gans/">GANs</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="https://florianwilhelm.info/tag/google-hangouts/">google hangouts</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="https://florianwilhelm.info/tag/gps/">gps</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="https://florianwilhelm.info/tag/hadoop/">hadoop</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="https://florianwilhelm.info/tag/hive/">hive</a>
    </li>
    <li class="list-group-item tag-3">
      <a href="https://florianwilhelm.info/tag/integer-programming/">integer programming</a>
    </li>
    <li class="list-group-item tag-2">
      <a href="https://florianwilhelm.info/tag/jupyter/">jupyter</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="https://florianwilhelm.info/tag/kalman-filter/">kalman filter</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="https://florianwilhelm.info/tag/lightdash/">lightdash</a>
    </li>
    <li class="list-group-item tag-2">
      <a href="https://florianwilhelm.info/tag/machine-learning/">machine-learning</a>
    </li>
    <li class="list-group-item tag-2">
      <a href="https://florianwilhelm.info/tag/mathematics/">mathematics</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="https://florianwilhelm.info/tag/ml/">ml</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="https://florianwilhelm.info/tag/nlp/">nlp</a>
    </li>
    <li class="list-group-item tag-3">
      <a href="https://florianwilhelm.info/tag/predictive-analytics/">predictive analytics</a>
    </li>
    <li class="list-group-item tag-2">
      <a href="https://florianwilhelm.info/tag/production/">production</a>
    </li>
    <li class="list-group-item tag-2">
      <a href="https://florianwilhelm.info/tag/programming/">programming</a>
    </li>
    <li class="list-group-item tag-0">
      <a href="https://florianwilhelm.info/tag/python/">python</a>
    </li>
    <li class="list-group-item tag-3">
      <a href="https://florianwilhelm.info/tag/recommender-systems/">recommender systems</a>
    </li>
    <li class="list-group-item tag-2">
      <a href="https://florianwilhelm.info/tag/scikit-learn/">scikit-learn</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="https://florianwilhelm.info/tag/scipy/">scipy</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="https://florianwilhelm.info/tag/semi-supervised/">semi-supervised</a>
    </li>
    <li class="list-group-item tag-3">
      <a href="https://florianwilhelm.info/tag/snowflake/">snowflake</a>
    </li>
    <li class="list-group-item tag-2">
      <a href="https://florianwilhelm.info/tag/spark/">spark</a>
    </li>
    <li class="list-group-item tag-3">
      <a href="https://florianwilhelm.info/tag/template/">template</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="https://florianwilhelm.info/tag/uncertainty-quantification/">uncertainty quantification</a>
    </li>
  </ul>
</li>
<!-- End Sidebar/Tag Cloud -->
  </ul>
</section>
<!-- End Sidebar -->            </aside>
        </div>
    </div>
</div>
<!-- End Content Container -->

<footer>
   <div class="container">
      <hr>
      <div class="row">
         <div class="col-xs-10">&copy; 2025 Florian Wilhelm
            &middot; Powered by <a href="https://github.com/getpelican/pelican-themes/tree/master/pelican-bootstrap3" target="_blank">pelican-bootstrap3</a>,
            <a href="http://docs.getpelican.com/" target="_blank">Pelican</a>,
            <a href="http://getbootstrap.com" target="_blank">Bootstrap</a>         </div>
         <div class="col-xs-2"><p class="pull-right"><i class="fa fa-arrow-up"></i> <a href="#">Back to top</a></p></div>
      </div>
   </div>
</footer>
<script src="https://florianwilhelm.info/theme/js/jquery.min.js"></script>

<!-- Include all compiled plugins (below), or include individual files as needed -->
<script src="https://florianwilhelm.info/theme/js/bootstrap.min.js"></script>

<!-- Enable responsive features in IE8 with Respond.js (https://github.com/scottjehl/Respond) -->
<script src="https://florianwilhelm.info/theme/js/respond.min.js"></script>


    <!-- Disqus -->
    <script type="text/javascript">
        /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
        var disqus_shortname = 'florianwilhelmblog'; // required: replace example with your forum shortname

        /* * * DON'T EDIT BELOW THIS LINE * * */
        (function () {
            var s = document.createElement('script');
            s.async = true;
            s.type = 'text/javascript';
            s.src = '//' + disqus_shortname + '.disqus.com/count.js';
            (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
        }());
    </script>
    <!-- End Disqus Code -->


<script>
   $(document).ready(function () {
      $("table").attr("class","table table-condensed table-bordered");
   });
</script>
</body>
</html>