<!DOCTYPE html>
<html lang="en" prefix="og: http://ogp.me/ns# fb: https://www.facebook.com/2008/fbml">
<head>
    <title>Efficient UD(A)Fs with PySpark - Florian Wilhelm</title>
    <!-- Using the latest rendering mode for IE -->
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">




<style type="text/css">

/*some stuff for output/input prompts*/
div.cell{border:1px solid transparent;display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch}div.cell.selected{border-radius:4px;border:thin #ababab solid}
div.cell.edit_mode{border-radius:4px;border:thin #008000 solid}
div.cell{width:100%;padding:5px 5px 5px 0;margin:0;outline:none}
div.prompt{min-width:11ex;padding:.4em;margin:0;font-family:monospace;text-align:right;line-height:1.21429em}
@media (max-width:480px){div.prompt{text-align:left}}div.inner_cell{display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch;-webkit-box-flex:1;-moz-box-flex:1;box-flex:1;flex:1}
div.input_area{border:1px solid #cfcfcf;border-radius:4px;background:#f7f7f7;line-height:1.21429em}
div.prompt:empty{padding-top:0;padding-bottom:0}
div.input{page-break-inside:avoid;display:-webkit-box;-webkit-box-orient:horizontal;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:horizontal;-moz-box-align:stretch;display:box;box-orient:horizontal;box-align:stretch;}
div.inner_cell{width:90%;}
div.input_area{border:1px solid #cfcfcf;border-radius:4px;background:#f7f7f7;}
div.input_prompt{color:navy;border-top:1px solid transparent;}
div.output_wrapper{margin-top:5px;position:relative;display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;width:100%;}
div.output_scroll{height:24em;width:100%;overflow:auto;border-radius:4px;-webkit-box-shadow:inset 0 2px 8px rgba(0, 0, 0, 0.8);-moz-box-shadow:inset 0 2px 8px rgba(0, 0, 0, 0.8);box-shadow:inset 0 2px 8px rgba(0, 0, 0, 0.8);}
div.output_collapsed{margin:0px;padding:0px;display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;width:100%;}
div.out_prompt_overlay{height:100%;padding:0px 0.4em;position:absolute;border-radius:4px;}
div.out_prompt_overlay:hover{-webkit-box-shadow:inset 0 0 1px #000000;-moz-box-shadow:inset 0 0 1px #000000;box-shadow:inset 0 0 1px #000000;background:rgba(240, 240, 240, 0.5);}
div.output_prompt{color:darkred;}

a.anchor-link:link{text-decoration:none;padding:0px 20px;visibility:hidden;}
h1:hover .anchor-link,h2:hover .anchor-link,h3:hover .anchor-link,h4:hover .anchor-link,h5:hover .anchor-link,h6:hover .anchor-link{visibility:visible;}
/* end stuff for output/input prompts*/


.highlight-ipynb .hll { background-color: #ffffcc }
.highlight-ipynb  { background: #f8f8f8; }
.highlight-ipynb .c { color: #408080; font-style: italic } /* Comment */
.highlight-ipynb .err { border: 1px solid #FF0000 } /* Error */
.highlight-ipynb .k { color: #008000; font-weight: bold } /* Keyword */
.highlight-ipynb .o { color: #666666 } /* Operator */
.highlight-ipynb .cm { color: #408080; font-style: italic } /* Comment.Multiline */
.highlight-ipynb .cp { color: #BC7A00 } /* Comment.Preproc */
.highlight-ipynb .c1 { color: #408080; font-style: italic } /* Comment.Single */
.highlight-ipynb .cs { color: #408080; font-style: italic } /* Comment.Special */
.highlight-ipynb .gd { color: #A00000 } /* Generic.Deleted */
.highlight-ipynb .ge { font-style: italic } /* Generic.Emph */
.highlight-ipynb .gr { color: #FF0000 } /* Generic.Error */
.highlight-ipynb .gh { color: #000080; font-weight: bold } /* Generic.Heading */
.highlight-ipynb .gi { color: #00A000 } /* Generic.Inserted */
.highlight-ipynb .go { color: #888888 } /* Generic.Output */
.highlight-ipynb .gp { color: #000080; font-weight: bold } /* Generic.Prompt */
.highlight-ipynb .gs { font-weight: bold } /* Generic.Strong */
.highlight-ipynb .gu { color: #800080; font-weight: bold } /* Generic.Subheading */
.highlight-ipynb .gt { color: #0044DD } /* Generic.Traceback */
.highlight-ipynb .kc { color: #008000; font-weight: bold } /* Keyword.Constant */
.highlight-ipynb .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */
.highlight-ipynb .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */
.highlight-ipynb .kp { color: #008000 } /* Keyword.Pseudo */
.highlight-ipynb .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */
.highlight-ipynb .kt { color: #B00040 } /* Keyword.Type */
.highlight-ipynb .m { color: #666666 } /* Literal.Number */
.highlight-ipynb .s { color: #BA2121 } /* Literal.String */
.highlight-ipynb .na { color: #7D9029 } /* Name.Attribute */
.highlight-ipynb .nb { color: #008000 } /* Name.Builtin */
.highlight-ipynb .nc { color: #0000FF; font-weight: bold } /* Name.Class */
.highlight-ipynb .no { color: #880000 } /* Name.Constant */
.highlight-ipynb .nd { color: #AA22FF } /* Name.Decorator */
.highlight-ipynb .ni { color: #999999; font-weight: bold } /* Name.Entity */
.highlight-ipynb .ne { color: #D2413A; font-weight: bold } /* Name.Exception */
.highlight-ipynb .nf { color: #0000FF } /* Name.Function */
.highlight-ipynb .nl { color: #A0A000 } /* Name.Label */
.highlight-ipynb .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */
.highlight-ipynb .nt { color: #008000; font-weight: bold } /* Name.Tag */
.highlight-ipynb .nv { color: #19177C } /* Name.Variable */
.highlight-ipynb .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */
.highlight-ipynb .w { color: #bbbbbb } /* Text.Whitespace */
.highlight-ipynb .mf { color: #666666 } /* Literal.Number.Float */
.highlight-ipynb .mh { color: #666666 } /* Literal.Number.Hex */
.highlight-ipynb .mi { color: #666666 } /* Literal.Number.Integer */
.highlight-ipynb .mo { color: #666666 } /* Literal.Number.Oct */
.highlight-ipynb .sb { color: #BA2121 } /* Literal.String.Backtick */
.highlight-ipynb .sc { color: #BA2121 } /* Literal.String.Char */
.highlight-ipynb .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */
.highlight-ipynb .s2 { color: #BA2121 } /* Literal.String.Double */
.highlight-ipynb .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */
.highlight-ipynb .sh { color: #BA2121 } /* Literal.String.Heredoc */
.highlight-ipynb .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */
.highlight-ipynb .sx { color: #008000 } /* Literal.String.Other */
.highlight-ipynb .sr { color: #BB6688 } /* Literal.String.Regex */
.highlight-ipynb .s1 { color: #BA2121 } /* Literal.String.Single */
.highlight-ipynb .ss { color: #19177C } /* Literal.String.Symbol */
.highlight-ipynb .bp { color: #008000 } /* Name.Builtin.Pseudo */
.highlight-ipynb .vc { color: #19177C } /* Name.Variable.Class */
.highlight-ipynb .vg { color: #19177C } /* Name.Variable.Global */
.highlight-ipynb .vi { color: #19177C } /* Name.Variable.Instance */
.highlight-ipynb .il { color: #666666 } /* Literal.Number.Integer.Long */
</style>

<style type="text/css">
/* Overrides of notebook CSS for static HTML export */
div.entry-content {
  overflow: visible;
  padding: 8px;
}
.input_area {
  padding: 0.2em;
}

a.heading-anchor {
 white-space: normal;
}

.rendered_html
code {
 font-size: .8em;
}

pre.ipynb {
  color: black;
  background: #f7f7f7;
  border: none;
  box-shadow: none;
  margin-bottom: 0;
  padding: 0;
  margin: 0px;
  font-size: 13px;
}

/* remove the prompt div from text cells */
div.text_cell .prompt {
    display: none;
}

/* remove horizontal padding from text cells, */
/* so it aligns with outer body text */
div.text_cell_render {
    padding: 0.5em 0em;
}

img.anim_icon{padding:0; border:0; vertical-align:middle; -webkit-box-shadow:none; -box-shadow:none}
</style>

<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML" type="text/javascript"></script>
<script type="text/javascript">
init_mathjax = function() {
    if (window.MathJax) {
        // MathJax loaded
        MathJax.Hub.Config({
            tex2jax: {
                inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                displayMath: [ ['$$','$$'], ["\\[","\\]"] ]
            },
            displayAlign: 'center', // Change this to 'center' to center equations.
            "HTML-CSS": {
                styles: {'.MathJax_Display': {"margin": 0}}
            }
        });
        MathJax.Hub.Queue(["Typeset",MathJax.Hub]);
    }
}
init_mathjax();
</script>

<link rel="canonical" href="https://florianwilhelm.info/2017/10/efficient_udfs_with_pyspark/">

        <meta name="author" content="Florian Wilhelm" />
        <meta name="keywords" content="spark,python,big data" />
        <meta name="description" content="Nowadays, Spark surely is one of the most prevalent technologies in the fields of data science and big data. Luckily, even though it is developed in Scala and runs in the Java Virtual Machine (JVM), it comes with Python bindings also known as PySpark, whose API was heavily influenced by …" />

        <meta property="og:site_name" content="Florian Wilhelm" />
        <meta property="og:type" content="article"/>
        <meta property="og:title" content="Efficient UD(A)Fs with PySpark"/>
        <meta property="og:url" content="https://florianwilhelm.info/2017/10/efficient_udfs_with_pyspark/"/>
        <meta property="og:description" content="Nowadays, Spark surely is one of the most prevalent technologies in the fields of data science and big data. Luckily, even though it is developed in Scala and runs in the Java Virtual Machine (JVM), it comes with Python bindings also known as PySpark, whose API was heavily influenced by …"/>
        <meta property="article:published_time" content="2017-10-11" />
            <meta property="article:section" content="post" />
            <meta property="article:tag" content="spark" />
            <meta property="article:tag" content="python" />
            <meta property="article:tag" content="big data" />
            <meta property="article:author" content="Florian Wilhelm" />

    <meta name="twitter:card" content="summary">
        <meta name="twitter:site" content="@FlorianWilhelm">
        <meta name="twitter:creator" content="@FlorianWilhelm">
    <meta name="twitter:domain" content="https://florianwilhelm.info">

    <!-- Bootstrap -->
        <link rel="stylesheet" href="https://florianwilhelm.info/theme/css/bootstrap.min.css" type="text/css"/>
    <link href="https://florianwilhelm.info/theme/css/font-awesome.min.css" rel="stylesheet">

    <link href="https://florianwilhelm.info/theme/css/pygments/native.css" rel="stylesheet">
        <link href="https://florianwilhelm.info/theme/css/typogrify.css" rel="stylesheet">
    <link rel="stylesheet" href="https://florianwilhelm.info/theme/css/style.css" type="text/css"/>




        <link href="https://florianwilhelm.info/feeds/post.atom.xml" type="application/atom+xml" rel="alternate"
              title="Florian Wilhelm post ATOM Feed"/>

</head>
<body>

<div class="navbar navbar-inverse navbar-fixed-top" role="navigation">
	<div class="container">
        <div class="navbar-header">
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-ex1-collapse">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a href="https://florianwilhelm.info/" class="navbar-brand">
Florian Wilhelm            </a>
        </div>
        <div class="collapse navbar-collapse navbar-ex1-collapse">
            <ul class="nav navbar-nav">
                    <li><a href="/">Home</a></li>
                    <li><a href="/about/">About me</a></li>
            </ul>
            <ul class="nav navbar-nav navbar-right">
              <li><a href="https://florianwilhelm.info/archives.html"><i class="fa fa-th-list"></i><span class="icon-label">Archives</span></a></li>
            </ul>
        </div>
        <!-- /.navbar-collapse -->
    </div>
</div> <!-- /.navbar -->
<!-- Banner -->
<!-- End Banner -->
<div class="container">
    <div class="row">
        <div class="col-sm-9">
    <section id="content">
        <article>
            <header class="page-header">
                <h1>
                    <a href="https://florianwilhelm.info/2017/10/efficient_udfs_with_pyspark/"
                       rel="bookmark"
                       title="Permalink to Efficient UD(A)Fs with PySpark">
                        Efficient <span class="caps">UD</span>(A)Fs with&nbsp;PySpark
                    </a>
                </h1>
            </header>
            <div class="entry-content">
                <div class="panel">
                    <div class="panel-body">
<footer class="post-info">
    <span class="label label-default">Date</span>
    <span class="published">
        <i class="fa fa-calendar"></i><time datetime="2017-10-11T12:30:00+02:00"> Oct. 11, 2017</time>
    </span>





<span class="label label-default">Tags</span>
	<a href="https://florianwilhelm.info/tag/spark/">spark</a>
        /
	<a href="https://florianwilhelm.info/tag/python/">python</a>
        /
	<a href="https://florianwilhelm.info/tag/big-data/">big data</a>
    
</footer><!-- /.post-info -->                    </div>
                </div>
                <p>Nowadays, Spark surely is one of the most prevalent technologies in the fields of data science and big data. Luckily, even though it is developed in Scala and runs in the Java Virtual Machine (<span class="caps">JVM</span>), it comes with Python bindings also known as <a href="https://spark.apache.org/docs/latest/api/python/index.html">PySpark</a>, whose <span class="caps">API</span> was heavily influenced by <a href="http://pandas.pydata.org/">Pandas</a>.
With respect to functionality, modern PySpark has about the same capabilities as Pandas when it comes to typical <span class="caps">ETL</span> and data wrangling, e.g. groupby, aggregations and so on.
As a general rule of thumb, one should consider an alternative to Pandas whenever the data set has more than 10,000,000 rows which, depending on the number of columns and data types, translates to about 5-10 <span class="caps">GB</span> of memory usage. At that point PySpark might be an option for you that does the job, but of course there are others like for instance <a href="http://dask.pydata.org/en/latest/index.html">Dask</a> which won&#8217;t be addressed in this&nbsp;post. </p>
<p>If you are new to Spark, one important thing to note is that Spark has two remarkable features besides its programmatic data wrangling capabilities. One is that Spark comes with <span class="caps">SQL</span> as an alternative way of defining queries and the other is <a href="https://spark.apache.org/docs/latest/ml-guide.html">Spark MLlib</a> for machine learning. Both topics are beyond the scope of this post but should be taken into account if you are considering PySpark as an alternative to Pandas and scikit-learn for larger data&nbsp;sets.</p>
<p>But enough praise for PySpark, there are still some ugly sides as well as rough edges to it and we want to address some of them here, of course, in a constructive way.
First of all, due to its relatively young age, PySpark lacks some features that Pandas provides, for example in areas such as reshaping/pivoting or time series.
Also, it is not as straightforward to use advanced mathematical functions from SciPy within PySpark.
That&#8217;s why sooner or later, you might walk into a scenario where you want to apply some Pandas or SciPy operations to your data frame in PySpark.
Unfortunately, there is no built-in mechanism for using Pandas transformations in PySpark.
In fact, this requires a lot of boilerplate code with many error-prone details to consider.
Therefore we make a wish to the coding fairy, cross two fingers that someone else already solved this and start googling&#8230; and here we are&nbsp;;-)</p>
<p>The remainder of this blog post walks you through the process of writing efficient Pandas UDAFs in PySpark. In fact, we end up abstracting all the necessary boilerplate code into a single Python decorator, which allows us to conveniently specify our PySpark Pandas function.
To give more insights into performance considerations, this post also contains a little journey into the internals of&nbsp;PySpark.</p>
<h2>UDAFs with&nbsp;RDDs</h2>
<p>To start with a recap, an aggregation function is a function that operates on a set of rows and produces a result, for example a <code>sum()</code> or <code>count()</code> function.
A <em>User-Defined Aggregation Function</em> (<span class="caps">UDAF</span>) is typically used for more complex aggregations that are not natively shipped with your analysis tool in question.
In our case, this means we provide some Python code that takes a set of rows and produces an aggregate result.
At the time of writing - with PySpark 2.2 as latest version - there is no &#8220;official&#8221; way of defining an arbitrary <span class="caps">UDAF</span> function.
Also, the tracking Jira issue <a href="https://issues.apache.org/jira/browse/SPARK-10915"><span class="caps">SPARK</span>-10915</a> does not indicate that this changes in near future.
Depending on your use-case, this might even be a reason to completely discard PySpark as a viable solution.
However, as you might have guessed from the title of this article, there are workarounds to the rescue.
<!-- langsamer weg den wir probiert hatten: groupby() + collect_list() + udf die liste an events in pandas DF lädt ... -->
This is where the <a href="https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD"><span class="caps">RDD</span></a> <span class="caps">API</span> comes in.
As a reminder, a <em>Resilient Distributed Dataset</em> (<span class="caps">RDD</span>) is the low-level data structure of Spark and a Spark <a href="https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame">DataFrame</a> is built on top of it. As we are mostly dealing with DataFrames in PySpark, we can get access to the underlying <span class="caps">RDD</span> with the help of the <code>rdd</code> attribute and convert it back with <code>toDF()</code>.
This <span class="caps">RDD</span> <span class="caps">API</span> allows us to specify arbitrary Python functions that get executed on the data.
To give an example, let&#8217;s say we have a DataFrame <code>df</code> of one billion rows with a boolean <code>is_sold</code> column and we want to filter for rows with sold products. One could accomplish this with the&nbsp;code</p>
<div class="highlight"><pre><span></span><span class="n">df</span><span class="o">.</span><span class="n">rdd</span><span class="o">.</span><span class="n">filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">is_sold</span> <span class="o">==</span> <span class="bp">True</span><span class="p">)</span><span class="o">.</span><span class="n">toDF</span><span class="p">()</span>
</pre></div>


<p>Although not explicitly declared as such, this lambda function is essentially a user-defined function (<span class="caps">UDF</span>).
For this exact use case, we could also use the more high-level DataFrame <code>filter()</code> method, producing the same&nbsp;result:</p>
<div class="highlight"><pre><span></span><span class="n">df</span><span class="o">.</span><span class="n">filter</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">is_sold</span> <span class="o">==</span> <span class="bp">True</span><span class="p">)</span>
</pre></div>


<p>Before we now go into the details on how to implement UDAFs using the <span class="caps">RDD</span> <span class="caps">API</span>, there is something important to keep in mind which might sound counterintuitive to the title of this post: in PySpark you should <em>avoid</em> all kind of Python UDFs - like <span class="caps">RDD</span> functions or data frame UDFs - as much as possible!
Whenever there is a built-in DataFrame method available, this will be much faster than its <span class="caps">RDD</span> counterpart. 
To get a better understanding of the substantial performance difference, we will now take a little detour and investigate what happens behind the scenes in those two filter&nbsp;examples.</p>
<!---
PySpark Internals
===========================

Communication between Python and Spark happens on different levels:
 1. *PySpark Remote Controlling Spark*: Local communication between the PySpark driver and the Java SparkContext
 2. *Python UDFs*: Data transfer between the data frames in JVM and the Python workers executing the UDF
 3. Data transfer between the distributed data frames in JVM memory and the Python driver PySpark actions and data frame creation from python (e.g.:("PySpark toDF(), c 

Local communication acts like a JVM remote control from Python. 
-->

<h2>PySpark&nbsp;internals</h2>
<p>PySpark is actually a wrapper around the Spark core written in Scala. 
When you start your <a href="http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.SparkSession">SparkSession</a> in Python, in the background PySpark uses <a href="https://www.py4j.org/">Py4J</a> to launch a <span class="caps">JVM</span> and create a Java SparkContext. 
All PySpark operations, for example our <code>df.filter()</code> method call, behind the scenes get translated into corresponding calls on the respective Spark DataFrame object within the <span class="caps">JVM</span> SparkContext. This is in general extremely fast and the overhead can be neglected as long as you don&#8217;t call the function millions of times.
So in our <code>df.filter()</code> example, the DataFrame operation and the filter condition will be send to the Java SparkContext, where it gets compiled into an overall optimized query plan.
Once the query is executed, the filter condition is evaluated on the distributed DataFrame within Java, without any callback to Python!
In case our workflow loads the DataFrame from Hive and saves the resulting DataFrame as Hive table, throughout the entire query execution all data operations are performed in a distributed fashion within Java Spark workers, which allows Spark to be very fast for queries on large data sets. 
Okay, so why is the <span class="caps">RDD</span> <code>filter()</code> method then so much slower?
The reason is that the lambda function cannot be directly applied to the DataFrame residing in <span class="caps">JVM</span> memory. 
<!--- 
To get a better understanding of the huge performance difference, we need to look more closely at the previously mentioned second point of data transfer between the JVM and Python.
-->
What actually happens internally is that Spark spins up Python workers next to the Spark executors on the cluster nodes.
At execution time, the Spark workers send our lambda function to those Python workers.
Next, the Spark workers start serializing their <span class="caps">RDD</span> partitions and pipe them to the Python workers via sockets, where our lambda function gets evaluated on each row.
For the resulting rows, the whole serialization/deserialization procedure happens again in the opposite direction so that
the actual <code>filter()</code> can be applied to the result&nbsp;set.</p>
<p>The entire data flow when using arbitrary Python functions in PySpark is also shown in the following image, which has been taken from the old <a href="https://cwiki.apache.org/confluence/display/SPARK/PySpark+Internals">PySpark Internals</a>&nbsp;wiki:</p>
<p><img src="/images/pyspark_udf_dataflow.png"/><br></p>
<p>Even if all of this sounded awkwardly technical to you, you get the point that executing Python functions in a distributed Java system is very expensive in terms of execution time due to excessive copying of data back and&nbsp;forth.</p>
<p>To give a short summary to this low-level excursion: as long as we avoid all kind of Python UDFs, a PySpark program will be approximately as fast as Spark program based on Scala.
If we cannot avoid UDFs, we should at least try to make them as efficient as possible, which is what we show in the remaining post. Before we move on though, one side note should be kept in mind. The general problem of accessing data frames from different programming languages in the realm of data analytics is currently addressed by the creator of Pandas <a href="http://wesmckinney.com/">Wes McKinney</a>. He is also the initiator of the <a href="http://arrow.apache.org/">Apache Arrow</a> project which tries to standardize the way columnar data is stored in memory so that everyone using Arrow won&#8217;t need to do the cumbersome object translation by serialization and deserialization anymore. Hopefully with version 2.3, as shown in the issues <a href="https://issues.apache.org/jira/browse/SPARK-13534"><span class="caps">SPARK</span>-13534</a> and <a href="https://issues.apache.org/jira/browse/SPARK-21190"><span class="caps">SPARK</span>-21190</a>, Spark will make use of Arrow, which should drastically speed up our Python UDFs. Still, even in that case we should always prefer built-in Spark functions whenever&nbsp;possible.</p>
<h1>PySpark UDAFs with&nbsp;Pandas</h1>
<p>As mentioned before our detour into the internals of PySpark, for defining an arbitrary <span class="caps">UDAF</span> function we need an operation that allows us to operate on multiple rows and produce one or multiple resulting rows.
This functionality is provided by the <a href="https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD"><span class="caps">RDD</span></a> method <code>mapPartitions</code>, where we can apply an arbitrary Python function <code>my_func</code> to a DataFrame <code>df</code> partition&nbsp;with:</p>
<div class="highlight"><pre><span></span><span class="n">df</span><span class="o">.</span><span class="n">rdd</span><span class="o">.</span><span class="n">mapPartitions</span><span class="p">(</span><span class="n">my_func</span><span class="p">)</span><span class="o">.</span><span class="n">toDF</span><span class="p">()</span>
</pre></div>


<p>If you want to further read up on RDDs and partitions, you can checkout the chapter <a href="https://jaceklaskowski.gitbooks.io/mastering-apache-spark/spark-rdd-partitions.html">Partitions and Partitioning</a> of the excellent <em>Mastering Apache Spark 2</em> book by Jacek Laskowski.
In most cases we would want to control the number of partitions, like 100, or even group by a column, let&#8217;s say <code>country</code>, in which case we would&nbsp;write:</p>
<div class="highlight"><pre><span></span><span class="n">df</span><span class="o">.</span><span class="n">repartition</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span><span class="o">.</span><span class="n">rdd</span><span class="o">.</span><span class="n">mapPartitions</span><span class="p">(</span><span class="n">my_func</span><span class="p">)</span><span class="o">.</span><span class="n">toDF</span><span class="p">()</span>
</pre></div>


<p>or</p>
<div class="highlight"><pre><span></span><span class="n">df</span><span class="o">.</span><span class="n">repartition</span><span class="p">(</span><span class="s1">&#39;country&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">rdd</span><span class="o">.</span><span class="n">mapPartitions</span><span class="p">(</span><span class="n">my_func</span><span class="p">)</span><span class="o">.</span><span class="n">toDF</span><span class="p">()</span>
</pre></div>


<p>Having solved one problem, as it is quite often in life, we have introduced another problem. As we are working now with the low-level <span class="caps">RDD</span> interface, our function <code>my_func</code> will be passed an iterator of PySpark <a href="https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.Row">Row</a> objects and needs to return them as well. A <code>Row</code> object itself is only a container for the column values in one row, as you might have guessed. When we return such a <code>Row</code>, the data types of these values therein must be interpretable by Spark in order to translate them back to Scala. This is a lot of low-level stuff to deal with since in most cases we would love to implement our <span class="caps">UDF</span>/<span class="caps">UDAF</span> with the help of Pandas, keeping in mind that one partition should hold less than 10 million&nbsp;rows.</p>
<p>So first we need to define a nice function that will convert a <code>Row</code> iterator into a Pandas&nbsp;DataFrame:</p>
<div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span>


<span class="n">_logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="vm">__name__</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">rows_to_pandas</span><span class="p">(</span><span class="n">rows</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Converts a Spark Row iterator of a partition to a Pandas DataFrame assuming YARN</span>

<span class="sd">    Args:</span>
<span class="sd">        rows: iterator over PySpark Row objects</span>

<span class="sd">    Returns:</span>
<span class="sd">        Pandas DataFrame</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">first_row</span><span class="p">,</span> <span class="n">rows</span> <span class="o">=</span> <span class="n">peek</span><span class="p">(</span><span class="n">rows</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">first_row</span><span class="p">:</span>
        <span class="n">_logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s2">&quot;Spark DataFrame is empty! Returning empty Pandas DataFrame!&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>

    <span class="n">first_row_info</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;{} ({}): {}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">rtype</span><span class="p">(</span><span class="n">first_row</span><span class="p">[</span><span class="n">k</span><span class="p">]),</span> <span class="n">first_row</span><span class="p">[</span><span class="n">k</span><span class="p">])</span>
                      <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">first_row</span><span class="o">.</span><span class="n">__fields__</span><span class="p">]</span>
    <span class="n">_logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="s2">&quot;First partition row: {}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">first_row_info</span><span class="p">))</span>
    <span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="o">.</span><span class="n">from_records</span><span class="p">(</span><span class="n">rows</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">first_row</span><span class="o">.</span><span class="n">__fields__</span><span class="p">)</span>
    <span class="n">_logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="s2">&quot;Converted partition to DataFrame of shape {} with types:</span><span class="se">\n</span><span class="s2">{}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">dtypes</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">df</span>
</pre></div>


<p>This function actually does only one thing which is calling <code>df = pd.DataFrame.from_records(rows, columns=first_row.__fields__)</code> in order to generate a DataFrame. The rest of the code makes sure that the iterator is not empty and for debugging reasons we also peek into the first row and print the value as well as the datatype of each column. This has proven in practice to be extremely helpful in case something goes wrong and one needs to debug what&#8217;s going on in the <span class="caps">UDF</span>/<span class="caps">UDAF</span>. The functions <code>peek</code> and <code>rtype</code> are defined as&nbsp;follows:</p>
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">itertools</span> <span class="kn">import</span> <span class="n">chain</span>


<span class="k">def</span> <span class="nf">peek</span><span class="p">(</span><span class="n">iterable</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Peek into the first element and return the whole iterator again</span>

<span class="sd">    Args:</span>
<span class="sd">        iterable: iterable object like list or iterator</span>

<span class="sd">    Returns:</span>
<span class="sd">        tuple of first element and original iterable</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">iterable</span> <span class="o">=</span> <span class="nb">iter</span><span class="p">(</span><span class="n">iterable</span><span class="p">)</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">first_elem</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">iterable</span><span class="p">)</span>
    <span class="k">except</span> <span class="ne">StopIteration</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">None</span><span class="p">,</span> <span class="n">iterable</span>
    <span class="n">iterable</span> <span class="o">=</span> <span class="n">chain</span><span class="p">([</span><span class="n">first_elem</span><span class="p">],</span> <span class="n">iterable</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">first_elem</span><span class="p">,</span> <span class="n">iterable</span>


<span class="k">def</span> <span class="nf">rtype</span><span class="p">(</span><span class="n">var</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Heuristic representation for nested types/containers</span>

<span class="sd">    Args:</span>
<span class="sd">        var: some (nested) variable</span>

<span class="sd">    Returns:</span>
<span class="sd">        str: string representation of nested datatype (NA=Not Available)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">etype</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">type</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span>

    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">var</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
        <span class="n">elem_type</span> <span class="o">=</span> <span class="n">etype</span><span class="p">(</span><span class="n">var</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="k">if</span> <span class="n">var</span> <span class="k">else</span> <span class="s2">&quot;NA&quot;</span>
        <span class="k">return</span> <span class="s2">&quot;List[{}]&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">elem_type</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">var</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
        <span class="n">keys</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">var</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
        <span class="k">if</span> <span class="n">keys</span><span class="p">:</span>
            <span class="n">key</span> <span class="o">=</span> <span class="n">keys</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">key_type</span><span class="p">,</span> <span class="n">val_type</span> <span class="o">=</span> <span class="n">etype</span><span class="p">(</span><span class="n">key</span><span class="p">),</span> <span class="n">etype</span><span class="p">(</span><span class="n">var</span><span class="p">[</span><span class="n">key</span><span class="p">])</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">key_type</span><span class="p">,</span> <span class="n">val_type</span> <span class="o">=</span> <span class="s2">&quot;NA&quot;</span><span class="p">,</span> <span class="s2">&quot;NA&quot;</span>
        <span class="k">return</span> <span class="s2">&quot;Dict[{}, {}]&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">key_type</span><span class="p">,</span> <span class="n">val_type</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">var</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
        <span class="n">elem_types</span> <span class="o">=</span> <span class="s1">&#39;, &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">etype</span><span class="p">(</span><span class="n">elem</span><span class="p">)</span> <span class="k">for</span> <span class="n">elem</span> <span class="ow">in</span> <span class="n">var</span><span class="p">)</span>
        <span class="k">return</span> <span class="s2">&quot;Tuple[{}]&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">elem_types</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">etype</span><span class="p">(</span><span class="n">var</span><span class="p">)</span>
</pre></div>


<p>The next part is to actually convert the result of our <span class="caps">UDF</span>/<span class="caps">UDAF</span> back to an iterator of Row objects. Since our result will most likely be a Pandas DataFrame or Series, we define the&nbsp;following:</p>
<div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">pyspark.sql.types</span> <span class="kn">import</span> <span class="n">Row</span>


<span class="k">def</span> <span class="nf">convert_dtypes</span><span class="p">(</span><span class="n">rows</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Converts some Pandas data types to pure Python data types</span>

<span class="sd">    Args:</span>
<span class="sd">        rows (array): numpy recarray holding all rows</span>

<span class="sd">    Returns:</span>
<span class="sd">        Iterator over lists of row values</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">dtype_map</span> <span class="o">=</span> <span class="p">{</span><span class="n">pd</span><span class="o">.</span><span class="n">Timestamp</span><span class="p">:</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">to_pydatetime</span><span class="p">(),</span>
                 <span class="n">np</span><span class="o">.</span><span class="n">datetime64</span><span class="p">:</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">Timestamp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">to_pydatetime</span><span class="p">(),</span>
                 <span class="n">np</span><span class="o">.</span><span class="n">bool_</span><span class="p">:</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nb">bool</span><span class="p">(</span><span class="n">x</span><span class="p">),</span>
                 <span class="n">np</span><span class="o">.</span><span class="n">int8</span><span class="p">:</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nb">int</span><span class="p">(</span><span class="n">x</span><span class="p">),</span>
                 <span class="n">np</span><span class="o">.</span><span class="n">int16</span><span class="p">:</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nb">int</span><span class="p">(</span><span class="n">x</span><span class="p">),</span>
                 <span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">:</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nb">int</span><span class="p">(</span><span class="n">x</span><span class="p">),</span>
                 <span class="n">np</span><span class="o">.</span><span class="n">int64</span><span class="p">:</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nb">int</span><span class="p">(</span><span class="n">x</span><span class="p">),</span>
                 <span class="n">np</span><span class="o">.</span><span class="n">float16</span><span class="p">:</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nb">float</span><span class="p">(</span><span class="n">x</span><span class="p">),</span>
                 <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">:</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nb">float</span><span class="p">(</span><span class="n">x</span><span class="p">),</span>
                 <span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">:</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nb">float</span><span class="p">(</span><span class="n">x</span><span class="p">),</span>
                 <span class="n">np</span><span class="o">.</span><span class="n">float128</span><span class="p">:</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nb">float</span><span class="p">(</span><span class="n">x</span><span class="p">)}</span>
    <span class="k">for</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">rows</span><span class="p">:</span>
        <span class="k">yield</span> <span class="p">[</span><span class="n">dtype_map</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">elem</span><span class="p">),</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">)(</span><span class="n">elem</span><span class="p">)</span> <span class="k">for</span> <span class="n">elem</span> <span class="ow">in</span> <span class="n">row</span><span class="p">]</span>


<span class="k">def</span> <span class="nf">pandas_to_rows</span><span class="p">(</span><span class="n">df</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Converts Pandas DataFrame to iterator of Row objects</span>

<span class="sd">    Args:</span>
<span class="sd">        df: Pandas DataFrame</span>

<span class="sd">    Returns:</span>
<span class="sd">        Iterator over PySpark Row objects</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">df</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">_logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="s2">&quot;Returning nothing&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="nb">iter</span><span class="p">([])</span>
    <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">df</span><span class="p">)</span> <span class="ow">is</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">:</span>
        <span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">to_frame</span><span class="p">()</span><span class="o">.</span><span class="n">T</span>
    <span class="k">if</span> <span class="n">df</span><span class="o">.</span><span class="n">empty</span><span class="p">:</span>
        <span class="n">_logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s2">&quot;Pandas DataFrame is empty! Returning nothing!&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="nb">iter</span><span class="p">([])</span>
    <span class="n">_logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="s2">&quot;Convert DataFrame of shape {} to partition with types:</span><span class="se">\n</span><span class="s2">{}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">dtypes</span><span class="p">))</span>
    <span class="n">records</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">to_records</span><span class="p">(</span><span class="n">index</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
    <span class="n">records</span> <span class="o">=</span> <span class="n">convert_dtypes</span><span class="p">(</span><span class="n">records</span><span class="p">)</span>
    <span class="n">first_row</span><span class="p">,</span> <span class="n">records</span> <span class="o">=</span> <span class="n">peek</span><span class="p">(</span><span class="n">records</span><span class="p">)</span>
    <span class="n">first_row_info</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;{} ({}): {}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">rtype</span><span class="p">(</span><span class="n">v</span><span class="p">),</span> <span class="n">v</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">columns</span><span class="p">,</span> <span class="n">first_row</span><span class="p">)]</span>
    <span class="n">_logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="s2">&quot;First record row: {}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">first_row_info</span><span class="p">))</span>
    <span class="n">row</span> <span class="o">=</span> <span class="n">Row</span><span class="p">(</span><span class="o">*</span><span class="n">df</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">row</span><span class="p">(</span><span class="o">*</span><span class="n">elems</span><span class="p">)</span> <span class="k">for</span> <span class="n">elems</span> <span class="ow">in</span> <span class="n">records</span><span class="p">)</span>
</pre></div>


<p>This looks a bit more complicated but essentially we convert a Pandas Series to a DataFrame if necessary and handle the edge cases of an empty DataFrame or <code>None</code> as return value. We then convert the DataFrame to records, convert some NumPy data types to the Python equivalent and create an iterator over Row objects from the converted&nbsp;records. </p>
<p>With these functions at hand we can define a <a href="https://wiki.python.org/moin/PythonDecorators#What_is_a_Decorator">Python decorator</a> that will allow us to automatically call the functions <code>rows_to_pandas</code> and <code>pandas_to_rows</code> at the right&nbsp;time:</p>
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">functools</span> <span class="kn">import</span> <span class="n">wraps</span>


<span class="k">class</span> <span class="nc">pandas_udaf</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Decorator for PySpark UDAFs using Pandas</span>

<span class="sd">    Args:</span>
<span class="sd">        loglevel (int): minimum loglevel for emitting messages</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">loglevel</span><span class="o">=</span><span class="n">logging</span><span class="o">.</span><span class="n">INFO</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">loglevel</span> <span class="o">=</span> <span class="n">loglevel</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">func</span><span class="p">):</span>
        <span class="nd">@wraps</span><span class="p">(</span><span class="n">func</span><span class="p">)</span>
        <span class="k">def</span> <span class="nf">wrapper</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">):</span>
            <span class="c1"># use *args to allow decorating methods (incl. self arg)</span>
            <span class="n">args</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>
            <span class="n">setup_logger</span><span class="p">(</span><span class="n">loglevel</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">loglevel</span><span class="p">)</span>
            <span class="n">args</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">rows_to_pandas</span><span class="p">(</span><span class="n">args</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
            <span class="n">df</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">pandas_to_rows</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">wrapper</span>
</pre></div>


<p>The code is pretty much self-explanatory if you have ever written a Python decorator; otherwise, you should read about it since it takes some time to wrap your head around it. Basically, we set up a default logger, create a Pandas DataFrame from the Row iterator, pass it to our <span class="caps">UDF</span>/<span class="caps">UDAF</span> and convert its return value back to a Row iterator. The only additional thing that might still raise questions is the usage of <code>args[-1]</code>. This is due to the fact that <code>func</code> might also be a method of an object. In this case, the first argument would be <code>self</code> but the last argument is in either cases the actual argument that <code>mapPartitions</code> will pass to us. The code of <code>setup_logger</code> depends on your Spark installation. In case you are using Spark on Apache <a href="https://hortonworks.com/apache/yarn/"><span class="caps">YARN</span></a>, it might look like&nbsp;this:</p>
<div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">sys</span>


<span class="k">def</span> <span class="nf">setup_logger</span><span class="p">(</span><span class="n">loglevel</span><span class="o">=</span><span class="n">logging</span><span class="o">.</span><span class="n">INFO</span><span class="p">,</span> <span class="n">logfile</span><span class="o">=</span><span class="s2">&quot;pyspark.log&quot;</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Setup basic logging for logging on the executor</span>

<span class="sd">    Args:</span>
<span class="sd">        loglevel (int): minimum loglevel for emitting messages</span>
<span class="sd">        logfile (str): name of the logfile</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">logformat</span> <span class="o">=</span> <span class="s2">&quot;</span><span class="si">%(asctime)s</span><span class="s2"> </span><span class="si">%(levelname)s</span><span class="s2"> </span><span class="si">%(module)s</span><span class="s2">.</span><span class="si">%(funcName)s</span><span class="s2">: </span><span class="si">%(message)s</span><span class="s2">&quot;</span>
    <span class="n">datefmt</span> <span class="o">=</span> <span class="s2">&quot;%y/%m/</span><span class="si">%d</span><span class="s2"> %H:%M:%S&quot;</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">logfile</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;LOG_DIRS&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;,&#39;</span><span class="p">)[</span><span class="mi">0</span><span class="p">],</span> <span class="n">logfile</span><span class="p">)</span>
    <span class="k">except</span> <span class="p">(</span><span class="ne">KeyError</span><span class="p">,</span> <span class="ne">IndexError</span><span class="p">):</span>
        <span class="n">logging</span><span class="o">.</span><span class="n">basicConfig</span><span class="p">(</span><span class="n">level</span><span class="o">=</span><span class="n">loglevel</span><span class="p">,</span>
                            <span class="n">stream</span><span class="o">=</span><span class="n">sys</span><span class="o">.</span><span class="n">stdout</span><span class="p">,</span> 
                            <span class="n">format</span><span class="o">=</span><span class="n">logformat</span><span class="p">,</span>
                            <span class="n">datefmt</span><span class="o">=</span><span class="n">datefmt</span><span class="p">)</span>
        <span class="n">logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="vm">__name__</span><span class="p">)</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="s2">&quot;LOG_DIRS is not in environment variables or empty, using STDOUT instead.&quot;</span><span class="p">)</span>

    <span class="n">logging</span><span class="o">.</span><span class="n">basicConfig</span><span class="p">(</span><span class="n">level</span><span class="o">=</span><span class="n">loglevel</span><span class="p">,</span>
                        <span class="n">filename</span><span class="o">=</span><span class="n">logfile</span><span class="p">,</span>
                        <span class="n">format</span><span class="o">=</span><span class="n">logformat</span><span class="p">,</span>
                        <span class="n">datefmt</span><span class="o">=</span><span class="n">datefmt</span><span class="p">)</span>
</pre></div>


<p>Now having all parts in place let&#8217;s assume the code above resides in the python module <a href="https://florianwilhelm.info/src/pyspark_udaf.py">pyspark_udaf.py</a>. A future post will cover the topic of deploying dependencies in a systematic way for production requirements. For now we just presume that <a href="https://florianwilhelm.info/src/pyspark_udaf.py">pyspark_udaf.py</a> as well as all its dependencies like Pandas, NumPy, etc. are accessible by the Spark driver as well as the executors. This allows us to then easily define an example <span class="caps">UDAF</span> <code>my_func</code> that collects some basic statistics for each country&nbsp;as:</p>
<div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pyspark_udaf</span>
<span class="kn">import</span> <span class="nn">logging</span>


<span class="nd">@pyspark_udaf.pandas_udaf</span><span class="p">(</span><span class="n">loglevel</span><span class="o">=</span><span class="n">logging</span><span class="o">.</span><span class="n">DEBUG</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">my_func</span><span class="p">(</span><span class="n">df</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">df</span><span class="o">.</span><span class="n">empty</span><span class="p">:</span>
        <span class="k">return</span>
    <span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s1">&#39;country&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s1">&#39;country&#39;</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">describe</span><span class="p">())</span>
    <span class="k">return</span> <span class="n">df</span><span class="o">.</span><span class="n">reset_index</span><span class="p">()</span>
</pre></div>


<p>It is of course not really useful in practice to return some statistics with the help of a <span class="caps">UDAF</span> that could also be retrieved with basic PySpark functionality but this is just an example. We now generate a dummy data DataFrame and apply the function to each partition as above&nbsp;with:</p>
<div class="highlight"><pre><span></span><span class="c1"># make pyspark_udaf.py available to the executors</span>
<span class="n">spark</span><span class="o">.</span><span class="n">sparkContext</span><span class="o">.</span><span class="n">addFile</span><span class="p">(</span><span class="s1">&#39;./pyspark_udaf.py&#39;</span><span class="p">)</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span>
    <span class="n">data</span> <span class="o">=</span> <span class="p">[(</span><span class="s1">&#39;DEU&#39;</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;DEU&#39;</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mf">8.0</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;FRA&#39;</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mf">6.0</span><span class="p">),</span>
            <span class="p">(</span><span class="s1">&#39;FRA&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">8.0</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;DEU&#39;</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mf">8.0</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;FRA&#39;</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">)],</span>
    <span class="n">schema</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;country&#39;</span><span class="p">,</span> <span class="s1">&#39;feature1&#39;</span><span class="p">,</span> <span class="s1">&#39;feature2&#39;</span><span class="p">])</span>

<span class="n">stats_df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">repartition</span><span class="p">(</span><span class="s1">&#39;country&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">rdd</span><span class="o">.</span><span class="n">mapPartitions</span><span class="p">(</span><span class="n">my_func</span><span class="p">)</span><span class="o">.</span><span class="n">toDF</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="n">stats_df</span><span class="o">.</span><span class="n">toPandas</span><span class="p">())</span>
</pre></div>


<p>The code above can be easily tested with the help of a Jupyter notebook with PySpark where the <a href="http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.SparkSession">SparkSession</a> <code>spark</code> is&nbsp;predefined.</p>
<h2>Summary</h2>
<p>Overall, this proposed method allows the definition of an <span class="caps">UDF</span> as well as an <span class="caps">UDAF</span> since it is up to the function <code>my_func</code> if it returns (1) a DataFrame having as many rows as the input DataFrame (think <a href="https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.transform.html">Pandas transform</a>), (2) a DataFrame of only a single row or (3) optionally a Series (think <a href="https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.aggregate.html">Pandas aggregate</a>) or a DataFrame with an arbitrary number of rows (think <a href="https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.apply.html">Pandas apply</a>) with even varying columns.
Therefore, this approach should be applicable to a variety of use cases where the built-in PySpark functionality is not&nbsp;sufficient.</p>
<p>To wrap it up, this blog post gives you a template on how to write PySpark <span class="caps">UD</span>(A)Fs while abstracting all the boilerplate in a dedicated module.
We also went down the rabbit hole to explore the technical difficulties the Spark developers face in providing Python bindings to a distributed <span class="caps">JVM</span>-based system.
In this respect we are really looking forward to closer integration of <a href="http://arrow.apache.org/">Apache Arrow</a> and Spark in the upcoming Spark 2.3 and future&nbsp;versions.</p>
<p>This article was coauthored by my inovex colleague <em>Bernhard Schäfer</em> and was also published on the <a href="https://www.inovex.de/blog/">inovex blog</a>.</p>
            </div>
            <!-- /.entry-content -->
<section class="well" id="related-posts">
    <h4>Related Posts:</h4>
    <ul>
        <li><a href="https://florianwilhelm.info/2018/03/isolated_environments_with_pyspark/">Managing isolated Environments with&nbsp;PySpark</a></li>
        <li><a href="https://florianwilhelm.info/2016/10/python_udf_in_hive/">Hive UDFs and UDAFs with&nbsp;Python</a></li>
        <li><a href="https://florianwilhelm.info/2013/10/handling_big_data_with_python/">Handling Big Data with&nbsp;Python</a></li>
        <li><a href="https://florianwilhelm.info/2018/07/bridging_the_gap_from_ds_to_prod/">Bridging the Gap: from Data Science to&nbsp;Production</a></li>
        <li><a href="https://florianwilhelm.info/2018/07/how_mobilede_brings_ds_to_prod_for_a_personalized_web_experience/">How mobile.de brings Data Science to Production for a Personalized Web&nbsp;Experience</a></li>
    </ul>
</section>
    <hr/>
    <section class="comments" id="comments">
        <h2>Comments</h2>

        <div id="disqus_thread"></div>
        <script type="text/javascript">
            /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
            var disqus_shortname = 'florianwilhelmblog'; // required: replace example with your forum shortname

                    var disqus_identifier = 'efficient_udfs_with_pyspark';
                var disqus_url = 'https://florianwilhelm.info/2017/10/efficient_udfs_with_pyspark/';

            var disqus_config = function () {
                this.language = "en";
            };

            /* * * DON'T EDIT BELOW THIS LINE * * */
            (function () {
                var dsq = document.createElement('script');
                dsq.type = 'text/javascript';
                dsq.async = true;
                dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
                (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
            })();
        </script>
        <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by
            Disqus.</a></noscript>
        <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>

    </section>
        </article>
    </section>

        </div>
        <div class="col-sm-3" id="sidebar">
            <aside>

<section class="well well-sm">
    <ul class="list-group list-group-flush">
        <li class="list-group-item"><h4><i class="fa fa-home fa-lg"></i><span class="icon-label">Social</span></h4>
              <ul class="list-group" id="social">
                <li class="list-group-item"><a href="https://twitter.com/FlorianWilhelm"><i class="fa fa-twitter-square fa-lg"></i> Twitter</a></li>
                <li class="list-group-item"><a href="https://linkedin.com/in/florian-wilhelm-621ba834"><i class="fa fa-linkedin-square fa-lg"></i> LinkedIn</a></li>
                <li class="list-group-item"><a href="https://github.com/FlorianWilhelm"><i class="fa fa-github-square fa-lg"></i> GitHub</a></li>
              </ul>
            </li>



            <li class="list-group-item"><a href="https://florianwilhelm.info/"><h4><i class="fa fa-tags fa-lg"></i><span class="icon-label">Tags</span></h4></a>
                <ul class="list-group list-inline tagcloud" id="tags">
                    <li class="list-group-item tag-4">
                        <a href="https://florianwilhelm.info/tag/asynchronous/">
                            asynchronous
                        </a>
                    </li>
                    <li class="list-group-item tag-4">
                        <a href="https://florianwilhelm.info/tag/asyncio/">
                            asyncio
                        </a>
                    </li>
                    <li class="list-group-item tag-4">
                        <a href="https://florianwilhelm.info/tag/bayesian/">
                            bayesian
                        </a>
                    </li>
                    <li class="list-group-item tag-2">
                        <a href="https://florianwilhelm.info/tag/big-data/">
                            big data
                        </a>
                    </li>
                    <li class="list-group-item tag-4">
                        <a href="https://florianwilhelm.info/tag/blue-yonder/">
                            Blue Yonder
                        </a>
                    </li>
                    <li class="list-group-item tag-4">
                        <a href="https://florianwilhelm.info/tag/bokeh/">
                            bokeh
                        </a>
                    </li>
                    <li class="list-group-item tag-4">
                        <a href="https://florianwilhelm.info/tag/causal-inference/">
                            causal inference
                        </a>
                    </li>
                    <li class="list-group-item tag-2">
                        <a href="https://florianwilhelm.info/tag/data-science/">
                            data science
                        </a>
                    </li>
                    <li class="list-group-item tag-4">
                        <a href="https://florianwilhelm.info/tag/event-driven/">
                            event-driven
                        </a>
                    </li>
                    <li class="list-group-item tag-4">
                        <a href="https://florianwilhelm.info/tag/google-hangouts/">
                            google hangouts
                        </a>
                    </li>
                    <li class="list-group-item tag-4">
                        <a href="https://florianwilhelm.info/tag/gps/">
                            gps
                        </a>
                    </li>
                    <li class="list-group-item tag-4">
                        <a href="https://florianwilhelm.info/tag/hadoop/">
                            hadoop
                        </a>
                    </li>
                    <li class="list-group-item tag-4">
                        <a href="https://florianwilhelm.info/tag/hive/">
                            hive
                        </a>
                    </li>
                    <li class="list-group-item tag-2">
                        <a href="https://florianwilhelm.info/tag/jupyter/">
                            jupyter
                        </a>
                    </li>
                    <li class="list-group-item tag-4">
                        <a href="https://florianwilhelm.info/tag/kalman/">
                            kalman
                        </a>
                    </li>
                    <li class="list-group-item tag-2">
                        <a href="https://florianwilhelm.info/tag/machine-learning/">
                            machine-learning
                        </a>
                    </li>
                    <li class="list-group-item tag-3">
                        <a href="https://florianwilhelm.info/tag/predictive-analytics/">
                            predictive analytics
                        </a>
                    </li>
                    <li class="list-group-item tag-2">
                        <a href="https://florianwilhelm.info/tag/production/">
                            production
                        </a>
                    </li>
                    <li class="list-group-item tag-4">
                        <a href="https://florianwilhelm.info/tag/programming/">
                            programming
                        </a>
                    </li>
                    <li class="list-group-item tag-1">
                        <a href="https://florianwilhelm.info/tag/python/">
                            python
                        </a>
                    </li>
                    <li class="list-group-item tag-4">
                        <a href="https://florianwilhelm.info/tag/recommendation-system/">
                            recommendation system
                        </a>
                    </li>
                    <li class="list-group-item tag-2">
                        <a href="https://florianwilhelm.info/tag/scikit-learn/">
                            scikit-learn
                        </a>
                    </li>
                    <li class="list-group-item tag-4">
                        <a href="https://florianwilhelm.info/tag/scipy/">
                            scipy
                        </a>
                    </li>
                    <li class="list-group-item tag-3">
                        <a href="https://florianwilhelm.info/tag/spark/">
                            spark
                        </a>
                    </li>
                    <li class="list-group-item tag-4">
                        <a href="https://florianwilhelm.info/tag/template/">
                            template
                        </a>
                    </li>
                </ul>
            </li>


    </ul>
</section>            </aside>
        </div>
    </div>
</div>
<footer>
   <div class="container">
      <hr>
      <div class="row">
         <div class="col-xs-10">&copy; 2018 Florian Wilhelm
            &middot; Powered by <a href="https://github.com/getpelican/pelican-themes/tree/master/pelican-bootstrap3" target="_blank">pelican-bootstrap3</a>,
            <a href="http://docs.getpelican.com/" target="_blank">Pelican</a>,
            <a href="http://getbootstrap.com" target="_blank">Bootstrap</a>         </div>
         <div class="col-xs-2"><p class="pull-right"><i class="fa fa-arrow-up"></i> <a href="#">Back to top</a></p></div>
      </div>
   </div>
</footer>
<script src="https://florianwilhelm.info/theme/js/jquery.min.js"></script>

<!-- Include all compiled plugins (below), or include individual files as needed -->
<script src="https://florianwilhelm.info/theme/js/bootstrap.min.js"></script>

<!-- Enable responsive features in IE8 with Respond.js (https://github.com/scottjehl/Respond) -->
<script src="https://florianwilhelm.info/theme/js/respond.min.js"></script>

    <!-- Disqus -->
    <script type="text/javascript">
        /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
        var disqus_shortname = 'florianwilhelmblog'; // required: replace example with your forum shortname

        /* * * DON'T EDIT BELOW THIS LINE * * */
        (function () {
            var s = document.createElement('script');
            s.async = true;
            s.type = 'text/javascript';
            s.src = '//' + disqus_shortname + '.disqus.com/count.js';
            (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
        }());
    </script>
    <!-- End Disqus Code -->
    <!-- Google Analytics -->
    <script type="text/javascript">

        var _gaq = _gaq || [];
        _gaq.push(['_setAccount', 'UA-71694209-1']);
        _gaq.push(['_trackPageview']);

        (function () {
            var ga = document.createElement('script');
            ga.type = 'text/javascript';
            ga.async = true;
            ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
            var s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(ga, s);
        })();
    </script>
    <!-- End Google Analytics Code -->

<script>
   $(document).ready(function () {
      $("table").attr("class","table table-condensed table-bordered");
   });
</script>
</body>
</html>