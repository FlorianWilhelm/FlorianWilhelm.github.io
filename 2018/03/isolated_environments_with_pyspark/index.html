<!DOCTYPE html>
<html lang="en" prefix="og: http://ogp.me/ns# fb: https://www.facebook.com/2008/fbml">
<head>
    <title>Managing isolated Environments with PySpark - Florian Wilhelm's blog</title>
    <!-- Using the latest rendering mode for IE -->
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">



<link rel="canonical" href="https://florianwilhelm.info/2018/03/isolated_environments_with_pyspark/">

        <meta name="author" content="Florian Wilhelm" />
        <meta name="keywords" content="spark,python,production" />
        <meta name="description" content="The Spark data processing platform becomes more and more important for data scientists using Python. PySpark - the official Python API for Spark - makes it easy to get started but managing applications and their dependencies in isolated environments is no easy task." />

        <meta property="og:site_name" content="Florian Wilhelm's blog" />
        <meta property="og:type" content="article"/>
        <meta property="og:title" content="Managing isolated Environments with PySpark"/>
        <meta property="og:url" content="https://florianwilhelm.info/2018/03/isolated_environments_with_pyspark/"/>
        <meta property="og:description" content="The Spark data processing platform becomes more and more important for data scientists using Python. PySpark - the official Python API for Spark - makes it easy to get started but managing applications and their dependencies in isolated environments is no easy task."/>
        <meta property="article:published_time" content="2018-03-08" />
            <meta property="article:section" content="post" />
            <meta property="article:tag" content="spark" />
            <meta property="article:tag" content="python" />
            <meta property="article:tag" content="production" />
            <meta property="article:author" content="Florian Wilhelm" />

    <meta name="twitter:dnt" content="on">
    <meta name="twitter:card" content="summary">
        <meta name="twitter:site" content="@FlorianWilhelm">
        <meta name="twitter:creator" content="@FlorianWilhelm">
    <meta name="twitter:domain" content="https://florianwilhelm.info">


    <!-- Bootstrap -->
        <link rel="stylesheet" href="https://florianwilhelm.info/theme/css/bootstrap.min.css" type="text/css"/>
    <link href="https://florianwilhelm.info/theme/css/font-awesome.min.css" rel="stylesheet">

    <link href="https://florianwilhelm.info/theme/css/pygments/native.css" rel="stylesheet">
        <link href="https://florianwilhelm.info/theme/css/typogrify.css" rel="stylesheet">
    <link rel="stylesheet" href="https://florianwilhelm.info/theme/css/style.css" type="text/css"/>


        <link href="https://florianwilhelm.info/feeds/post.atom.xml" type="application/atom+xml" rel="alternate"
              title="Florian Wilhelm's blog post ATOM Feed"/>
</head>
<body>

<div class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
        <div class="navbar-header">
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-ex1-collapse">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a href="https://florianwilhelm.info/" class="navbar-brand">
Florian Wilhelm's blog            </a>
        </div>
        <div class="collapse navbar-collapse navbar-ex1-collapse">
            <ul class="nav navbar-nav">
                    <li><a href="/about/">About me</a></li>
            </ul>
            <ul class="nav navbar-nav navbar-right">
              <li><a href="https://florianwilhelm.info/archives.html"><i class="fa fa-th-list"></i><span class="icon-label">Archives</span></a></li>
            </ul>
        </div>
        <!-- /.navbar-collapse -->
    </div>
</div> <!-- /.navbar -->

<!-- Banner -->
<!-- End Banner -->

<!-- Content Container -->
<div class="container">
    <div class="row">
        <div class="col-sm-9">
    <section id="content">
        <article>
            <header class="page-header">
                <h1>
                    <a href="https://florianwilhelm.info/2018/03/isolated_environments_with_pyspark/"
                       rel="bookmark"
                       title="Permalink to Managing isolated Environments with PySpark">
                        Managing isolated Environments with&nbsp;PySpark
                    </a>
                </h1>
            </header>
            <div class="entry-content">
                <div class="panel">
                    <div class="panel-body">
<footer class="post-info">
    <span class="label label-default">Date</span>
    <span class="published">
        <i class="fa fa-calendar"></i><time datetime="2018-03-08T15:10:00+01:00"> Mar. 08, 2018</time>
    </span>





<span class="label label-default">Tags</span>
	<a href="https://florianwilhelm.info/tag/spark/">spark</a>
        /
	<a href="https://florianwilhelm.info/tag/python/">python</a>
        /
	<a href="https://florianwilhelm.info/tag/production/">production</a>
    
</footer><!-- /.post-info -->                    </div>
                </div>
                <h2>Motivation</h2>
<p>With the sustained success of the Spark data processing platform even data scientists with a strong focus on the Python ecosystem can no longer ignore it.
Fortunately, it is easy to get started with <a href="https://spark.apache.org/docs/latest/api/python/">PySpark</a> - the official Python <span class="caps">API</span> for Spark - due to millions of word count tutorials on the web. In contrast to that, resources on how to deploy and use Python packages like Numpy, Pandas, Scikit-Learn in an isolated environment with PySpark are scarce. A nice exception to that is a <a href="https://developerzen.com/best-practices-writing-production-grade-pyspark-jobs-cb688ac4d20f">blog post by Eran Kampf</a>. Being able to install your own Python libraries is especially important if you want to write User-Defined-Functions (UDFs) as explained in the blog post <a href="https://florianwilhelm.info/2017/10/efficient_udfs_with_pyspark/">Efficient <span class="caps">UD</span>(A)Fs with PySpark</a>.</p>
<p>For most Spark/Hadoop distributions, which is Cloudera in my case, there are basically two options for managing isolated&nbsp;environments:</p>
<ol>
<li>
<p>You give all your data scientists <span class="caps">SSH</span> access to all your cluster&#8217;s nodes and let them do whatever they want like installing virtual environments with <a href="https://virtualenv.pypa.io/en/stable/">virtualenv</a> or <a href="https://conda.io/docs/intro.html">conda</a> as detailed in the <a href="https://www.cloudera.com/documentation/enterprise/5-6-x/topics/spark_python.html#spark_python__section_kr2_4zs_b5">Cloudera documentation</a>.</p>
</li>
<li>
<p>Your sysadmins install Anaconda Parcels using the Cloudera Manager Admin Console to provide the most popular Python packages in a one size fits all fashion for all your data scientists as described in a <a href="http://blog.cloudera.com/blog/2016/02/making-python-on-apache-hadoop-easier-with-anaconda-and-cdh/">Cloudera blog post</a>. </p>
</li>
</ol>
<p>Both options have drawbacks which are as severe as obvious. Do you really want to let a bunch of data scientists run processes on your cluster and fill up the local hard-drives? The second option is not even a real isolated environment at all since all your applications would use the same libraries and maybe break after an update of a&nbsp;library.   </p>
<p>Therefore, we need to empower our data scientists developing a predictive application to manage isolated environments with their dependencies themselves. This was also recognized as a problem and several issues (<a href="https://issues.apache.org/jira/browse/SPARK-13587"><span class="caps">SPARK</span>-13587</a> <span class="amp">&amp;</span> <a href="https://issues.apache.org/jira/browse/SPARK-16367"><span class="caps">SPARK</span>-16367</a>) suggest solutions, but none of them have been integrated yet. The most mature solution is actually <a href="https://github.com/nteract/coffee_boat">coffee boat</a>, which is still in beta and not meant for production. Therefore, we want to present here a simple but viable solution for this problem that we have been using in production for more than a&nbsp;year.</p>
<p>So how can we distribute Python modules and whole packages on our executors? Luckily, PySpark provides the functions <a href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.SparkContext.addFile.html">sc.addFile</a> and <a href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.SparkContext.addPyFile.html">sc.addPyFile</a> which allow us to upload files to every node in our cluster, even Python modules and egg files in case of the latter. Unfortunately, there is no way to upload wheel files which are needed for binary Python packages like Numpy, Pandas and so on. As a data scientist you cannot live without&nbsp;those. </p>
<p>At first sight this looks pretty bad but thanks to the simplicity of the wheel format it&#8217;s not so bad at all. So here is what we do in a nutshell: For a given PySpark application, we will create an isolated environment on <span class="caps">HDFS</span> with the help of wheel files. When submitting our PySpark application, we copy the content of our environment to the driver and executors using <a href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.SparkContext.addFile.html">sc.addFile</a>. Simple but&nbsp;effective.</p>
<h2>Generating the&nbsp;environment</h2>
<p>In order to create our aforementioned environment we start by creating a directory that will contain our isolated environment, e.g. <code>venv</code>, on our local Linux machine. Then we will populate this directory with the wheel files of all libraries that our PySpark application uses. Since wheel files contain compiled code they are dependent on the exact Python version and platform. 
For us this means we have to make sure that we use the same platform and Python version locally as we gonna use on the Spark cluster. In my case the cluster runs Ubuntu Trusty Linux with Python 3.4. To replicate this locally it&#8217;s best to use a conda&nbsp;environment:</p>
<div class="highlight"><pre><span></span>conda<span class="w"> </span>create<span class="w"> </span>-n<span class="w"> </span>py34<span class="w"> </span><span class="nv">python</span><span class="o">=</span><span class="m">3</span>.4
<span class="nb">source</span><span class="w"> </span>activate<span class="w"> </span>py34
</pre></div>


<p>Having activated the conda environment, we just use <code>pip download</code> to download all the requirements of our PySpark application as wheel files. In case there is no wheel file available, <code>pip</code> will download a source-based <code>tar.gz</code> file instead but we can easily generate a wheel from it. To do so, we just unpack the archive, change into the directory and type <code>python setup.py bdist_wheel</code>. A wheel file should now reside in the <code>dist</code> subdirectory. At this point one should also be aware that some wheel files come with low-level Linux dependencies that just need to be installed by a sysadmin on every host, e.g. <code>python3-dev</code> and <code>unixodbc-dev</code>.   </p>
<p>Now we copy the wheel files of all our PySpark application&#8217;s dependencies into the <code>venv</code> directory. After that, we unpack them with <code>unzip</code> since they are just normal zip files with a strange suffix. Finally, we push everything to <span class="caps">HDFS</span>, e.g. <code>/my_venvs/venv</code>, using <code>hdfs dfs -put ./venv /my_venvs/venv</code> and make sure that the files are readable by&nbsp;anyone.</p>
<h2>Bootstrapping the&nbsp;environment</h2>
<p>When our PySpark application runs the first thing we do is calling <code>sc.addFile</code> on every file in <code>/my_venvs/venv</code>. Since this will also set the <code>PYTHONPATH</code> correctly, importing any library which resides in <code>venv</code> will just work. If our Python application itself is also nicely structured as a Python package (maybe using <a href="http://pyscaffold.org/">PyScaffold</a>) we can also push it to <code>/my_venvs/venv</code>. This allows us to roll a full-blown PySpark application and nicely separate the boilerplate code that bootstraps our isolated environment from&nbsp;it.</p>
<p>Let&#8217;s assume our PySpark application is a Python package called <code>my_pyspark_app</code>. The boilerplate code to bootstrap <code>my_pyspark_app</code>, i.e. to activate the isolated environment on Spark, will be in the module <code>activate_env.py</code>. When we submit our Spark job we will specify this module and specify the environment as an argument,&nbsp;e.g.:</p>
<div class="highlight"><pre><span></span><span class="nv">PYSPARK_PYTHON</span><span class="o">=</span>python3.4<span class="w"> </span>/opt/spark/bin/spark-submit<span class="w"> </span>--master<span class="w"> </span>yarn<span class="w"> </span>--deploy-mode<span class="w"> </span>cluster<span class="w"> </span><span class="se">\</span>
--num-executors<span class="w"> </span><span class="m">4</span><span class="w"> </span>--driver-memory<span class="w"> </span>12g<span class="w"> </span>--executor-memory<span class="w"> </span>4g<span class="w"> </span>--executor-cores<span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
--files<span class="w"> </span>/etc/spark/conf/hive-site.xml<span class="w"> </span>--queue<span class="w"> </span>default<span class="w"> </span>--conf<span class="w"> </span>spark.yarn.maxAppAttempts<span class="o">=</span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
activate_env.py<span class="w"> </span>/my_venvs/venv
</pre></div>


<p>Easy and quite flexible! We are even able to change from one environment to another by just passing another <span class="caps">HDFS</span> directory. Here is how <code>activate_env.py</code> which does the actual heavy lifting with <code>sc.addFile</code> looks&nbsp;like:</p>
<div class="highlight"><pre><span></span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Bootstrapping an isolated environment for `my_pyspark_app` on Spark</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">import</span> <span class="nn">logging</span>

<span class="kn">from</span> <span class="nn">pyspark.context</span> <span class="kn">import</span> <span class="n">SparkContext</span>
<span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">SparkSession</span>
<span class="kn">from</span> <span class="nn">pyspark.sql.functions</span> <span class="kn">import</span> <span class="o">*</span>

<span class="n">_logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="vm">__name__</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">list_path_names</span><span class="p">(</span><span class="n">path</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;List files and directories in an HDFS path</span>

<span class="sd">    Args:</span>
<span class="sd">        path (str): HDFS path to directory</span>

<span class="sd">    Returns:</span>
<span class="sd">        [str]: list of file/directory names</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="o">.</span><span class="n">getOrCreate</span><span class="p">()</span>
    <span class="c1"># low-level access to hdfs driver</span>
    <span class="n">hadoop</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">_gateway</span><span class="o">.</span><span class="n">jvm</span><span class="o">.</span><span class="n">org</span><span class="o">.</span><span class="n">apache</span><span class="o">.</span><span class="n">hadoop</span>
    <span class="n">path</span> <span class="o">=</span> <span class="n">hadoop</span><span class="o">.</span><span class="n">fs</span><span class="o">.</span><span class="n">Path</span><span class="p">(</span><span class="n">path</span><span class="p">)</span>
    <span class="n">config</span> <span class="o">=</span> <span class="n">hadoop</span><span class="o">.</span><span class="n">conf</span><span class="o">.</span><span class="n">Configuration</span><span class="p">()</span>

    <span class="n">status</span> <span class="o">=</span> <span class="n">hadoop</span><span class="o">.</span><span class="n">fs</span><span class="o">.</span><span class="n">FileSystem</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">config</span><span class="p">)</span><span class="o">.</span><span class="n">listStatus</span><span class="p">(</span><span class="n">path</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">path_status</span><span class="o">.</span><span class="n">getPath</span><span class="p">()</span><span class="o">.</span><span class="n">getName</span><span class="p">()</span> <span class="k">for</span> <span class="n">path_status</span> <span class="ow">in</span> <span class="n">status</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">distribute_hdfs_files</span><span class="p">(</span><span class="n">hdfs_path</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Distributes recursively a given directory in HDFS to Spark</span>

<span class="sd">    Args:</span>
<span class="sd">        hdfs_path (str): path to directory</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="o">.</span><span class="n">getOrCreate</span><span class="p">()</span>

    <span class="k">for</span> <span class="n">path_name</span> <span class="ow">in</span> <span class="n">list_path_names</span><span class="p">(</span><span class="n">hdfs_path</span><span class="p">):</span>
        <span class="n">path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">hdfs_path</span><span class="p">,</span> <span class="n">path_name</span><span class="p">)</span>
        <span class="n">_logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Distributing </span><span class="si">{}</span><span class="s2">...&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">path</span><span class="p">))</span>
        <span class="n">sc</span><span class="o">.</span><span class="n">addFile</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">recursive</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">main</span><span class="p">(</span><span class="n">args</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Main entry point allowing external calls</span>

<span class="sd">    Args:</span>
<span class="sd">      args ([str]): command line parameter list</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># setup logging for driver</span>
    <span class="n">logging</span><span class="o">.</span><span class="n">basicConfig</span><span class="p">(</span><span class="n">level</span><span class="o">=</span><span class="n">logging</span><span class="o">.</span><span class="n">DEBUG</span><span class="p">,</span> <span class="n">stream</span><span class="o">=</span><span class="n">sys</span><span class="o">.</span><span class="n">stdout</span><span class="p">)</span>

    <span class="n">_logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="vm">__name__</span><span class="p">)</span>
    <span class="n">_logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Starting up...&quot;</span><span class="p">)</span>

    <span class="c1"># Create the singleton instance</span>
    <span class="n">spark</span> <span class="o">=</span> <span class="p">(</span><span class="n">SparkSession</span>
             <span class="o">.</span><span class="n">builder</span>
             <span class="o">.</span><span class="n">appName</span><span class="p">(</span><span class="s2">&quot;My PySpark App in its own environment&quot;</span><span class="p">)</span>
             <span class="o">.</span><span class="n">enableHiveSupport</span><span class="p">()</span>
             <span class="o">.</span><span class="n">getOrCreate</span><span class="p">())</span>

    <span class="c1"># For simplicity we assume that the first argument is the environment on HDFS</span>
    <span class="n">VENV_DIR</span> <span class="o">=</span> <span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="c1"># make sure we have the latest version available on HDFS</span>
    <span class="n">distribute_hdfs_files</span><span class="p">(</span><span class="s1">&#39;hdfs://&#39;</span> <span class="o">+</span> <span class="n">VENV_DIR</span><span class="p">)</span>

    <span class="kn">from</span> <span class="nn">my_pyspark_app</span> <span class="kn">import</span> <span class="n">main</span>
    <span class="n">main</span><span class="p">(</span><span class="n">args</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span>


<span class="k">def</span> <span class="nf">run</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Entry point for console_scripts</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">main</span><span class="p">(</span><span class="n">sys</span><span class="o">.</span><span class="n">argv</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span>


<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="n">run</span><span class="p">()</span>
</pre></div>


<p>It is actually easier than it looks. In the <code>main</code> function we initialize the <code>SparkSession</code> the first time so that later calls to the session builder will use this instance. Thereafter, the passed path argument when doing the <code>spark-submit</code> is extracted. Subsequently, this is passed to <code>distribute_hdfs_files</code> which calls <code>sc.addFile</code> recursively on every file to set up the isolated environment on the driver and executors. After this we are able to import our <code>my_pyspark_app</code> package and call for instance its <code>main</code> method. The following graphic illustrates the whole&nbsp;concept: </p>
<figure>
<p align="center">
<img class="noZoom" src="/images/pyspark_venv.png" alt="Isolated environment with PySpark">
<figcaption><strong>Figure:</strong> Executing <em>spark-submit</em> uploads our <em>activate_env.py</em> module and starts a Spark driver process. Thereafter, <em>activate_env.py</em> is executed within the driver and bootstraps our <em>venv</em> environment on the Spark driver as well as on the executors. Finally, <em>activate_env.py</em> relinquishes control to <em>my_pyspark_app</em>.</figcaption>
</p>
</figure>

<h2>Conclusion</h2>
<p>Setting up an isolated environment like this is a bit cumbersome and surely also somewhat hacky. Still, in our use-case it served us quite well and allowed the data scientists to set up their specific environments without access to the cluster&#8217;s nodes. Since the explained method also works with <a href="http://jupyter.org/">Jupyter</a> this is not only useful for production but also for proof-of-concepts. That being said, we still hope that soon there will be an official solution by the Spark project itself. As a final note, I want to mention the tool <a href="https://border-patrol.readthedocs.io/">Border-Patrol</a> that helps us quite a lot in debugging Spark environments and is really easy to use. It tells you about all imported packages by your application, their versions and if these packages were taken from the environment or from the system installation of the cluster&nbsp;nodes. </p>
            </div>
            <!-- /.entry-content -->
<section class="well" id="related-posts">
    <h4>Related Posts:</h4>
    <ul>
        <li><a href="https://florianwilhelm.info/2018/07/how_mobilede_brings_ds_to_prod_for_a_personalized_web_experience/">How mobile.de brings Data Science to Production for a Personalized Web&nbsp;Experience</a></li>
        <li><a href="https://florianwilhelm.info/2019/04/more_efficient_udfs_with_pyspark/">More Efficient <span class="caps">UD</span>(A)Fs with&nbsp;PySpark</a></li>
        <li><a href="https://florianwilhelm.info/2017/10/efficient_udfs_with_pyspark/">Efficient <span class="caps">UD</span>(A)Fs with&nbsp;PySpark</a></li>
        <li><a href="https://florianwilhelm.info/2022/01/configuration_via_yaml_and_cli_with_hydra/">Effective and Consistent Configuration via <span class="caps">YAML</span> <span class="amp">&amp;</span> <span class="caps">CLI</span> with&nbsp;Hydra</a></li>
        <li><a href="https://florianwilhelm.info/2021/09/Handling_Anaconda_without_getting_constricted/">Handling Anaconda without getting&nbsp;Constricted</a></li>
    </ul>
</section>
    <hr/>
    <section class="comments" id="comments">
        <h2>Comments</h2>

        <div id="disqus_thread"></div>
        <script type="text/javascript">
            /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
            var disqus_shortname = 'florianwilhelmblog'; // required: replace example with your forum shortname

            var disqus_config = function () {
                this.language = "en";

                        this.page.identifier = '2018-03-08-isolated_environments_with_pyspark';
                        this.page.url = 'https://florianwilhelm.info/2018/03/isolated_environments_with_pyspark/';
            };

            /* * * DON'T EDIT BELOW THIS LINE * * */
            (function () {
                var dsq = document.createElement('script');
                dsq.type = 'text/javascript';
                dsq.async = true;
                dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
                (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
            })();
        </script>
        <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by
            Disqus.</a></noscript>
        <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>

    </section>
        </article>
    </section>

        </div>
        <div class="col-sm-3" id="sidebar">
            <aside>
<!-- Sidebar -->
<section class="well well-sm">
  <ul class="list-group list-group-flush">

<!-- Sidebar/Social -->
<li class="list-group-item">
  <h4><i class="fa fa-home fa-lg"></i><span class="icon-label">Social</span></h4>
  <ul class="list-group" id="social">
    <li class="list-group-item"><a href="https://twitter.com/FlorianWilhelm"><i class="fa fa-twitter-square fa-lg"></i> Twitter</a></li>
    <li class="list-group-item"><a href="https://linkedin.com/in/florian-wilhelm-621ba834"><i class="fa fa-linkedin-square fa-lg"></i> LinkedIn</a></li>
    <li class="list-group-item"><a href="https://github.com/FlorianWilhelm"><i class="fa fa-github-square fa-lg"></i> GitHub</a></li>
  </ul>
</li>
<!-- End Sidebar/Social -->

<!-- Sidebar/Tag Cloud -->
<li class="list-group-item">
  <a href="https://florianwilhelm.info/"><h4><i class="fa fa-tags fa-lg"></i><span class="icon-label">Tags</span></h4></a>
  <ul class="list-group list-inline tagcloud" id="tags">
    <li class="list-group-item tag-4">
      <a href="https://florianwilhelm.info/tag/ai/">ai</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="https://florianwilhelm.info/tag/airbyte/">airbyte</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="https://florianwilhelm.info/tag/asynchronous/">asynchronous</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="https://florianwilhelm.info/tag/asyncio/">asyncio</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="https://florianwilhelm.info/tag/bayesian/">bayesian</a>
    </li>
    <li class="list-group-item tag-2">
      <a href="https://florianwilhelm.info/tag/big-data/">big data</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="https://florianwilhelm.info/tag/bokeh/">bokeh</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="https://florianwilhelm.info/tag/causal-inference/">causal inference</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="https://florianwilhelm.info/tag/conference/">conference</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="https://florianwilhelm.info/tag/configuration/">configuration</a>
    </li>
    <li class="list-group-item tag-1">
      <a href="https://florianwilhelm.info/tag/data-science/">data science</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="https://florianwilhelm.info/tag/dbt/">dbt</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="https://florianwilhelm.info/tag/deep-learning/">deep learning</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="https://florianwilhelm.info/tag/event-driven/">event-driven</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="https://florianwilhelm.info/tag/gans/">GANs</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="https://florianwilhelm.info/tag/google-hangouts/">google hangouts</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="https://florianwilhelm.info/tag/gps/">gps</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="https://florianwilhelm.info/tag/hadoop/">hadoop</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="https://florianwilhelm.info/tag/hive/">hive</a>
    </li>
    <li class="list-group-item tag-2">
      <a href="https://florianwilhelm.info/tag/jupyter/">jupyter</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="https://florianwilhelm.info/tag/kalman-filter/">kalman filter</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="https://florianwilhelm.info/tag/lightdash/">lightdash</a>
    </li>
    <li class="list-group-item tag-2">
      <a href="https://florianwilhelm.info/tag/machine-learning/">machine-learning</a>
    </li>
    <li class="list-group-item tag-2">
      <a href="https://florianwilhelm.info/tag/mathematics/">mathematics</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="https://florianwilhelm.info/tag/nlp/">nlp</a>
    </li>
    <li class="list-group-item tag-3">
      <a href="https://florianwilhelm.info/tag/predictive-analytics/">predictive analytics</a>
    </li>
    <li class="list-group-item tag-2">
      <a href="https://florianwilhelm.info/tag/production/">production</a>
    </li>
    <li class="list-group-item tag-3">
      <a href="https://florianwilhelm.info/tag/programming/">programming</a>
    </li>
    <li class="list-group-item tag-1">
      <a href="https://florianwilhelm.info/tag/python/">Python</a>
    </li>
    <li class="list-group-item tag-3">
      <a href="https://florianwilhelm.info/tag/recommender-systems/">recommender systems</a>
    </li>
    <li class="list-group-item tag-2">
      <a href="https://florianwilhelm.info/tag/scikit-learn/">scikit-learn</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="https://florianwilhelm.info/tag/scipy/">scipy</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="https://florianwilhelm.info/tag/semi-supervised/">semi-supervised</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="https://florianwilhelm.info/tag/snowflake/">Snowflake</a>
    </li>
    <li class="list-group-item tag-2">
      <a href="https://florianwilhelm.info/tag/spark/">spark</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="https://florianwilhelm.info/tag/template/">template</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="https://florianwilhelm.info/tag/uncertainty-quantification/">uncertainty quantification</a>
    </li>
  </ul>
</li>
<!-- End Sidebar/Tag Cloud -->
  </ul>
</section>
<!-- End Sidebar -->            </aside>
        </div>
    </div>
</div>
<!-- End Content Container -->

<footer>
   <div class="container">
      <hr>
      <div class="row">
         <div class="col-xs-10">&copy; 2023 Florian Wilhelm
            &middot; Powered by <a href="https://github.com/getpelican/pelican-themes/tree/master/pelican-bootstrap3" target="_blank">pelican-bootstrap3</a>,
            <a href="http://docs.getpelican.com/" target="_blank">Pelican</a>,
            <a href="http://getbootstrap.com" target="_blank">Bootstrap</a>         </div>
         <div class="col-xs-2"><p class="pull-right"><i class="fa fa-arrow-up"></i> <a href="#">Back to top</a></p></div>
      </div>
   </div>
</footer>
<script src="https://florianwilhelm.info/theme/js/jquery.min.js"></script>

<!-- Include all compiled plugins (below), or include individual files as needed -->
<script src="https://florianwilhelm.info/theme/js/bootstrap.min.js"></script>

<!-- Enable responsive features in IE8 with Respond.js (https://github.com/scottjehl/Respond) -->
<script src="https://florianwilhelm.info/theme/js/respond.min.js"></script>


    <!-- Disqus -->
    <script type="text/javascript">
        /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
        var disqus_shortname = 'florianwilhelmblog'; // required: replace example with your forum shortname

        /* * * DON'T EDIT BELOW THIS LINE * * */
        (function () {
            var s = document.createElement('script');
            s.async = true;
            s.type = 'text/javascript';
            s.src = '//' + disqus_shortname + '.disqus.com/count.js';
            (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
        }());
    </script>
    <!-- End Disqus Code -->
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-G6WH8SW3M6"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-G6WH8SW3M6');
    </script>


<script>
   $(document).ready(function () {
      $("table").attr("class","table table-condensed table-bordered");
   });
</script>
</body>
</html>