<!DOCTYPE html>
<html lang="en" prefix="og: http://ogp.me/ns# fb: https://www.facebook.com/2008/fbml">
<head>
    <title>Efficient UD(A)Fs with PySpark - Florian Wilhelm</title>
    <!-- Using the latest rendering mode for IE -->
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">




<style type="text/css">

/*some stuff for output/input prompts*/
div.cell{border:1px solid transparent;display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch}div.cell.selected{border-radius:4px;border:thin #ababab solid}
div.cell.edit_mode{border-radius:4px;border:thin #008000 solid}
div.cell{width:100%;padding:5px 5px 5px 0;margin:0;outline:none}
div.prompt{min-width:11ex;padding:.4em;margin:0;font-family:monospace;text-align:right;line-height:1.21429em}
@media (max-width:480px){div.prompt{text-align:left}}div.inner_cell{display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch;-webkit-box-flex:1;-moz-box-flex:1;box-flex:1;flex:1}
div.input_area{border:1px solid #cfcfcf;border-radius:4px;background:#f7f7f7;line-height:1.21429em}
div.prompt:empty{padding-top:0;padding-bottom:0}
div.input{page-break-inside:avoid;display:-webkit-box;-webkit-box-orient:horizontal;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:horizontal;-moz-box-align:stretch;display:box;box-orient:horizontal;box-align:stretch;}
div.inner_cell{width:90%;}
div.input_area{border:1px solid #cfcfcf;border-radius:4px;background:#f7f7f7;}
div.input_prompt{color:navy;border-top:1px solid transparent;}
div.output_wrapper{margin-top:5px;position:relative;display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;width:100%;}
div.output_scroll{height:24em;width:100%;overflow:auto;border-radius:4px;-webkit-box-shadow:inset 0 2px 8px rgba(0, 0, 0, 0.8);-moz-box-shadow:inset 0 2px 8px rgba(0, 0, 0, 0.8);box-shadow:inset 0 2px 8px rgba(0, 0, 0, 0.8);}
div.output_collapsed{margin:0px;padding:0px;display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;width:100%;}
div.out_prompt_overlay{height:100%;padding:0px 0.4em;position:absolute;border-radius:4px;}
div.out_prompt_overlay:hover{-webkit-box-shadow:inset 0 0 1px #000000;-moz-box-shadow:inset 0 0 1px #000000;box-shadow:inset 0 0 1px #000000;background:rgba(240, 240, 240, 0.5);}
div.output_prompt{color:darkred;}

a.anchor-link:link{text-decoration:none;padding:0px 20px;visibility:hidden;}
h1:hover .anchor-link,h2:hover .anchor-link,h3:hover .anchor-link,h4:hover .anchor-link,h5:hover .anchor-link,h6:hover .anchor-link{visibility:visible;}
/* end stuff for output/input prompts*/


.highlight-ipynb .hll { background-color: #ffffcc }
.highlight-ipynb  { background: #f8f8f8; }
.highlight-ipynb .c { color: #408080; font-style: italic } /* Comment */
.highlight-ipynb .err { border: 1px solid #FF0000 } /* Error */
.highlight-ipynb .k { color: #008000; font-weight: bold } /* Keyword */
.highlight-ipynb .o { color: #666666 } /* Operator */
.highlight-ipynb .cm { color: #408080; font-style: italic } /* Comment.Multiline */
.highlight-ipynb .cp { color: #BC7A00 } /* Comment.Preproc */
.highlight-ipynb .c1 { color: #408080; font-style: italic } /* Comment.Single */
.highlight-ipynb .cs { color: #408080; font-style: italic } /* Comment.Special */
.highlight-ipynb .gd { color: #A00000 } /* Generic.Deleted */
.highlight-ipynb .ge { font-style: italic } /* Generic.Emph */
.highlight-ipynb .gr { color: #FF0000 } /* Generic.Error */
.highlight-ipynb .gh { color: #000080; font-weight: bold } /* Generic.Heading */
.highlight-ipynb .gi { color: #00A000 } /* Generic.Inserted */
.highlight-ipynb .go { color: #888888 } /* Generic.Output */
.highlight-ipynb .gp { color: #000080; font-weight: bold } /* Generic.Prompt */
.highlight-ipynb .gs { font-weight: bold } /* Generic.Strong */
.highlight-ipynb .gu { color: #800080; font-weight: bold } /* Generic.Subheading */
.highlight-ipynb .gt { color: #0044DD } /* Generic.Traceback */
.highlight-ipynb .kc { color: #008000; font-weight: bold } /* Keyword.Constant */
.highlight-ipynb .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */
.highlight-ipynb .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */
.highlight-ipynb .kp { color: #008000 } /* Keyword.Pseudo */
.highlight-ipynb .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */
.highlight-ipynb .kt { color: #B00040 } /* Keyword.Type */
.highlight-ipynb .m { color: #666666 } /* Literal.Number */
.highlight-ipynb .s { color: #BA2121 } /* Literal.String */
.highlight-ipynb .na { color: #7D9029 } /* Name.Attribute */
.highlight-ipynb .nb { color: #008000 } /* Name.Builtin */
.highlight-ipynb .nc { color: #0000FF; font-weight: bold } /* Name.Class */
.highlight-ipynb .no { color: #880000 } /* Name.Constant */
.highlight-ipynb .nd { color: #AA22FF } /* Name.Decorator */
.highlight-ipynb .ni { color: #999999; font-weight: bold } /* Name.Entity */
.highlight-ipynb .ne { color: #D2413A; font-weight: bold } /* Name.Exception */
.highlight-ipynb .nf { color: #0000FF } /* Name.Function */
.highlight-ipynb .nl { color: #A0A000 } /* Name.Label */
.highlight-ipynb .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */
.highlight-ipynb .nt { color: #008000; font-weight: bold } /* Name.Tag */
.highlight-ipynb .nv { color: #19177C } /* Name.Variable */
.highlight-ipynb .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */
.highlight-ipynb .w { color: #bbbbbb } /* Text.Whitespace */
.highlight-ipynb .mf { color: #666666 } /* Literal.Number.Float */
.highlight-ipynb .mh { color: #666666 } /* Literal.Number.Hex */
.highlight-ipynb .mi { color: #666666 } /* Literal.Number.Integer */
.highlight-ipynb .mo { color: #666666 } /* Literal.Number.Oct */
.highlight-ipynb .sb { color: #BA2121 } /* Literal.String.Backtick */
.highlight-ipynb .sc { color: #BA2121 } /* Literal.String.Char */
.highlight-ipynb .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */
.highlight-ipynb .s2 { color: #BA2121 } /* Literal.String.Double */
.highlight-ipynb .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */
.highlight-ipynb .sh { color: #BA2121 } /* Literal.String.Heredoc */
.highlight-ipynb .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */
.highlight-ipynb .sx { color: #008000 } /* Literal.String.Other */
.highlight-ipynb .sr { color: #BB6688 } /* Literal.String.Regex */
.highlight-ipynb .s1 { color: #BA2121 } /* Literal.String.Single */
.highlight-ipynb .ss { color: #19177C } /* Literal.String.Symbol */
.highlight-ipynb .bp { color: #008000 } /* Name.Builtin.Pseudo */
.highlight-ipynb .vc { color: #19177C } /* Name.Variable.Class */
.highlight-ipynb .vg { color: #19177C } /* Name.Variable.Global */
.highlight-ipynb .vi { color: #19177C } /* Name.Variable.Instance */
.highlight-ipynb .il { color: #666666 } /* Literal.Number.Integer.Long */
</style>

<style type="text/css">
/* Overrides of notebook CSS for static HTML export */
div.entry-content {
  overflow: visible;
  padding: 8px;
}
.input_area {
  padding: 0.2em;
}

a.heading-anchor {
 white-space: normal;
}

.rendered_html
code {
 font-size: .8em;
}

pre.ipynb {
  color: black;
  background: #f7f7f7;
  border: none;
  box-shadow: none;
  margin-bottom: 0;
  padding: 0;
  margin: 0px;
  font-size: 13px;
}

/* remove the prompt div from text cells */
div.text_cell .prompt {
    display: none;
}

/* remove horizontal padding from text cells, */
/* so it aligns with outer body text */
div.text_cell_render {
    padding: 0.5em 0em;
}

img.anim_icon{padding:0; border:0; vertical-align:middle; -webkit-box-shadow:none; -box-shadow:none}

div.collapseheader {
    width=100%;
    padding: 2px;
    cursor: pointer;
    font-family:"Helvetica Neue",Helvetica,Arial,sans-serif;
}

</style>

<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_HTML" type="text/javascript"></script>
<script type="text/javascript">
init_mathjax = function() {
    if (window.MathJax) {
        // MathJax loaded
        MathJax.Hub.Config({
            tex2jax: {
                inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                displayMath: [ ['$$','$$'], ["\\[","\\]"] ]
            },
            displayAlign: 'center', // Change this to 'center' to center equations.
            "HTML-CSS": {
                styles: {'.MathJax_Display': {"margin": 0}}
            }
        });
        MathJax.Hub.Queue(["Typeset",MathJax.Hub]);
    }
}
init_mathjax();
</script>

<link rel="canonical" href="http://www.florianwilhelm.info/drafts/efficient_udfs_with_pyspark.html">

        <meta name="author" content="Florian Wilhelm" />
        <meta name="keywords" content="spark,python,big data" />
        <meta name="description" content="Nowadays, Spark surely is one of the most prevalent technologies in the fields of data science and big data. Luckily, even though it’s built on a Java Stack it comes with Python bindings also known as PySpark whose API was heavily influenced by Pandas. With respect to the functionality …" />

        <meta property="og:site_name" content="Florian Wilhelm" />
        <meta property="og:type" content="article"/>
        <meta property="og:title" content="Efficient UD(A)Fs with PySpark"/>
        <meta property="og:url" content="http://www.florianwilhelm.info/drafts/efficient_udfs_with_pyspark.html"/>
        <meta property="og:description" content="Nowadays, Spark surely is one of the most prevalent technologies in the fields of data science and big data. Luckily, even though it’s built on a Java Stack it comes with Python bindings also known as PySpark whose API was heavily influenced by Pandas. With respect to the functionality …"/>
        <meta property="article:published_time" content="2017-06-01" />
            <meta property="article:section" content="article" />
            <meta property="article:tag" content="spark" />
            <meta property="article:tag" content="python" />
            <meta property="article:tag" content="big data" />
            <meta property="article:author" content="Florian Wilhelm" />

    <meta name="twitter:card" content="summary">
        <meta name="twitter:site" content="@FlorianWilhelm">
        <meta name="twitter:creator" content="@FlorianWilhelm">
    <meta name="twitter:domain" content="http://www.florianwilhelm.info">

    <!-- Bootstrap -->
        <link rel="stylesheet" href="http://www.florianwilhelm.info/theme/css/bootstrap.min.css" type="text/css"/>
    <link href="http://www.florianwilhelm.info/theme/css/font-awesome.min.css" rel="stylesheet">

    <link href="http://www.florianwilhelm.info/theme/css/pygments/native.css" rel="stylesheet">
        <link href="http://www.florianwilhelm.info/theme/css/typogrify.css" rel="stylesheet">
    <link rel="stylesheet" href="http://www.florianwilhelm.info/theme/css/style.css" type="text/css"/>




        <link href="http://www.florianwilhelm.info/feeds/article.atom.xml" type="application/atom+xml" rel="alternate"
              title="Florian Wilhelm article ATOM Feed"/>

</head>
<body>

<div class="navbar navbar-inverse navbar-fixed-top" role="navigation">
	<div class="container">
        <div class="navbar-header">
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-ex1-collapse">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a href="http://www.florianwilhelm.info/" class="navbar-brand">
Florian Wilhelm            </a>
        </div>
        <div class="collapse navbar-collapse navbar-ex1-collapse">
            <ul class="nav navbar-nav">
                    <li><a href="/">Home</a></li>
                    <li><a href="/about/">About me</a></li>
            </ul>
            <ul class="nav navbar-nav navbar-right">
              <li><a href="http://www.florianwilhelm.info/archives.html"><i class="fa fa-th-list"></i><span class="icon-label">Archives</span></a></li>
            </ul>
        </div>
        <!-- /.navbar-collapse -->
    </div>
</div> <!-- /.navbar -->
<!-- Banner -->
<!-- End Banner -->
<div class="container">
    <div class="row">
        <div class="col-sm-9">
    <section id="content">
        <article>
            <header class="page-header">
                <h1>
                    <a href="http://www.florianwilhelm.info/drafts/efficient_udfs_with_pyspark.html"
                       rel="bookmark"
                       title="Permalink to Efficient UD(A)Fs with PySpark">
                        Efficient <span class="caps">UD</span>(A)Fs with&nbsp;PySpark
                    </a>
                </h1>
            </header>
            <div class="entry-content">
                <div class="panel">
                    <div class="panel-body">
<footer class="post-info">
    <span class="label label-default">Date</span>
    <span class="published">
        <i class="fa fa-calendar"></i><time datetime="2017-06-01T12:30:00+02:00"> Jun. 01, 2017</time>
    </span>





<span class="label label-default">Tags</span>
	<a href="http://www.florianwilhelm.info/tag/spark/">spark</a>
        /
	<a href="http://www.florianwilhelm.info/tag/python/">python</a>
        /
	<a href="http://www.florianwilhelm.info/tag/big-data/">big data</a>
    
</footer><!-- /.post-info -->                    </div>
                </div>
                <p>Nowadays, Spark surely is one of the most prevalent technologies in the fields of data science and big data. Luckily, even though it&#8217;s built on a Java Stack it comes with Python bindings also known as <a href="https://spark.apache.org/docs/latest/api/python/index.html">PySpark</a> whose <span class="caps">API</span> was heavily influenced by <a href="http://pandas.pydata.org/">Pandas</a>. With respect to the functionality modern PySpark has about the same functionality as Pandas when it comes to typical <span class="caps">ETL</span> and data wrangling, e.g. groupby, aggregations and so on. As a general rule of thumb, one should consider
 an alternative to Pandas whenever the data set has more than 10,000,000 rows which, depending on the number of columns and data types, translates to about 5-10 <span class="caps">GB</span> of memory usage. At that point PySpark might be an option for you that does the job but of course there are others like for instance <a href="http://dask.pydata.org/en/latest/index.html">Dask</a> which won&#8217;t be addressed in this&nbsp;post. </p>
<p>If you are new to Spark one important thing to note is that Spark has two remarkable features besides its programmatic data wrangling capabilities. One is that Spark comes with <span class="caps">SQL</span> as an alternative way of defining queries and the other is <a href="https://spark.apache.org/docs/latest/api/python/pyspark.ml.html">Spark.ml</a> for machine learning. Both topics are beyond the scope of this post but should be considered if you are considering PySpark as an alternative to Pandas and Scikit-Learn for larger data&nbsp;sets. </p>
<p>But enough praise for PySpark, there are still some ugly sides as well as rough edges to it and we want to address some of them here, of course, in a constructive way. You might have heard the rumours that PySpark is so much slower compared to Spark with Scala and as it is often the case with rumours, there is a tiny bit of truth to it. But before we start, a deeper understanding of how PySpark does its magic is&nbsp;needed.</p>
<p>PySpark is a wrapper around the actual Spark core written in Scala. That means that when you call a method of a PySpark data frame, somewhere your Python call gets translated into the corresponding Scala call. This is in general extremely fast and the overhead can be neglected as long as you don&#8217;t call the function millions of times. The more expensive part in terms of execution time though is the translation of complex object arguments that are passed to a function. That means if a Scala object like a list, dictionary or a row needs to be translated into a Python object and vice versa. This translation internally means that the object from one programming language is serialized into a memory location, then copied over to the memory space of the other programming language where it is deserialized again. Technically, it&#8217;s even a bit more complicated since Scala and Python run in so called Virtual Machines (VMs) but you get the point that it is expensive in terms of execution time. So practically we should keep the number of these translations as low as possible. We can summarize this short low-level excursion with two important&nbsp;insights:</p>
<ol>
<li>Function calls are cheap but avoid excessive number of&nbsp;calls,</li>
<li>Translation of complex objects has a tolerable overhead as long as it happens&nbsp;rarely.</li>
</ol>
<p>With modern PySpark versions higher than 2.1, whenever you do data wrangling, like calling <code>filter</code>, <code>select</code>, <code>groupby</code> and so on, the overhead of the function call is neglectable as we have just learned. The arguments to those functions are mostly simple objects like strings or numbers defining the column name or indices and the costs of their translation to Scale is therefore also neglectable. Okay, so now in which cases do things go south then and when do we hit major performance bumps? Let&#8217;s say we have a data frame <code>df</code> of one billion rows with a boolean <code>is_sold</code> column and we want to filter for rows with sold products. One could accomplish this with the&nbsp;code</p>
<div class="highlight"><pre><span></span><span class="n">df</span><span class="o">.</span><span class="n">filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">is_sold</span> <span class="o">==</span> <span class="bp">True</span><span class="p">)</span>
</pre></div>


<p>which works but will be extremely slow because we just violated our rules 1 and 2 at the same time! Since <code>df</code> has one billion rows we need to evaluate our anonymous lambda function one billion times. Therefore, Scala actually calls the Python lambda an excessive number of times clearly violating rule 1. But that&#8217;s not nearly all, it even gets worse. We pass every row from Scala to Python and do the translation of that complex object also a billion times therefore violating rule 2! The easy solution is do the operation without involving Python at all&nbsp;with:</p>
<div class="highlight"><pre><span></span><span class="n">df</span><span class="o">.</span><span class="n">filter</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">is_sold</span> <span class="o">==</span> <span class="bp">True</span><span class="p">)</span>
</pre></div>


<p>In this case our filter condition will be translated one time to Scale where it is then evaluated a billion times really fast, without any callback to Python!
To give a short summery, as long as we stick to the rules 1 and 2 a PySpark program will be approximately as fast as Spark program based on&nbsp;Scala.</p>
<p>Before we move on, two side notes should also be kept in mind. The first is that what we just learnt not only applies to PySpark but also to Pandas, Numpy and Python in general since all these actually wrap a lot of C/C++ and sometimes even Fortran code. The second remark is that the general problem of object translation at least in the realm of data analytics is currently addressed by the creator of Pandas <a href="http://wesmckinney.com/">Wes McKinney</a>. His <a href="http://arrow.apache.org/">Apache Arrow</a> project tries to standardize the way complex objects are stored in memory so that everyone using Arrow won&#8217;t need to do the cumbersome object translation by serialization and deserialization anymore. Hopefully with version 2.3, as shown in the issues <a href="https://issues.apache.org/jira/browse/SPARK-13534"><span class="caps">SPARK</span>-13534</a> and <a href="https://issues.apache.org/jira/browse/SPARK-21190"><span class="caps">SPARK</span>-21190</a>, Spark will make use of Arrow and translation of complex objects like rows and data frames will have next to no overhead. Still, even in that case we should avoid making a large number of&nbsp;translations.</p>
<p>So far we have only talked about avoiding certain operations to keep the performance up. But what if we actually want to implement a User-Defined Function (<span class="caps">UDF</span>) or User-Defined Aggregation Function (<span class="caps">UDAF</span>)? The <a href="https://docs.databricks.com/spark/latest/spark-sql/udf-in-python.html">databricks documentation</a> explains how to define a <span class="caps">UDF</span> in PySpark in a few and easy steps but clearly violating our rules which leads to bad performance in&nbsp;practice: </p>
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pyspark.sql.functions</span> <span class="kn">import</span> <span class="n">udf</span>
<span class="kn">from</span> <span class="nn">pyspark.sql.types</span> <span class="kn">import</span> <span class="n">LongType</span>


<span class="k">def</span> <span class="nf">squared</span><span class="p">(</span><span class="n">s</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">s</span> <span class="o">*</span> <span class="n">s</span>


<span class="n">squared_udf</span> <span class="o">=</span> <span class="n">udf</span><span class="p">(</span><span class="n">squared</span><span class="p">,</span> <span class="n">LongType</span><span class="p">())</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">sqlContext</span><span class="o">.</span><span class="n">table</span><span class="p">(</span><span class="s2">&quot;test&quot;</span><span class="p">)</span>
<span class="n">display</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="s2">&quot;id&quot;</span><span class="p">,</span> <span class="n">squared_udf</span><span class="p">(</span><span class="s2">&quot;id&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s2">&quot;id_squared&quot;</span><span class="p">)))</span>
</pre></div>


<p>Since <code>squared</code> works on a single row, i.e. entry, of the <code>id</code> column the <code>squared</code> function will be called as many times as there are rows in the table <code>test</code>. Since the values of the <code>id</code> column are of of type integer, therefore a primitive type, the translation overhead of a single number is almost neglectable but again we do it as many times as there are rows. Alternatively, one could apply <code>squared_udf</code> with the <code>df.withColumn(squared_udf("id"))</code> or <code>df.rdd.map(squared_udf)</code> leading essentially to the same problem. Besides UDFs, there seems to be no &#8220;official&#8221; way of defining an arbitrary <span class="caps">UDAF</span> function that would allow us to not operate only on a single row but multiple. Depending on your use-case, this might even be a reason to completely discard PySpark as a viable&nbsp;solution.</p>
<p>The obvious question is now, how can we tackle the problem of using UDFs without sacrificing too much performance and as an additional benefit even define UDAFs? Looking at our little rule set, we see the pattern that if we do something with an overhead we should at least try to do it not so often. This directly leads us to the idea that a <span class="caps">UDF</span> should do the object translation only a few times by working not on single rows but rather on whole partitions. This functionality is provided by the <a href="https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD"><span class="caps">RDD</span></a> method <code>mapPartitions</code>. </p>
<p>As a short reminder, an Resilient Distributed Dataset (<span class="caps">RDD</span>) is the low-level data structures of Spark and a Spark <a href="https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame">DataFrame</a> is built on top of it. As we are mostly dealing with DataFrames in PySpark, we can get access to the underlying <span class="caps">RDD</span> with the help of the <code>rdd</code> attribute and convert it back with <code>toDF()</code>. Putting all ingredients together we can apply an arbitrary Python function <code>my_func</code> to a DataFrame <code>df</code> with:</p>
<div class="highlight"><pre><span></span><span class="n">df</span><span class="o">.</span><span class="n">rdd</span><span class="o">.</span><span class="n">mapPartitions</span><span class="p">(</span><span class="n">my_func</span><span class="p">)</span><span class="o">.</span><span class="n">toDF</span><span class="p">()</span>
</pre></div>


<p>In most cases we would want to control the number of partitions, like 100, or even group by a column, let&#8217;s say <code>country</code>, in which case we would&nbsp;write:</p>
<div class="highlight"><pre><span></span><span class="n">df</span><span class="o">.</span><span class="n">repartition</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span><span class="o">.</span><span class="n">rdd</span><span class="o">.</span><span class="n">mapPartitions</span><span class="p">(</span><span class="n">my_func</span><span class="p">)</span><span class="o">.</span><span class="n">toDF</span><span class="p">()</span>
</pre></div>


<p>or</p>
<div class="highlight"><pre><span></span><span class="n">df</span><span class="o">.</span><span class="n">repartition</span><span class="p">(</span><span class="s1">&#39;country&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">rdd</span><span class="o">.</span><span class="n">mapPartitions</span><span class="p">(</span><span class="n">my_func</span><span class="p">)</span><span class="o">.</span><span class="n">toDF</span><span class="p">()</span>
</pre></div>


<p>The following image shows the difference between the application of the presented <span class="caps">UDF</span> methods to an <span class="caps">RDD</span> of a single partition in terms of the number of Python workers involved as wells as the number of translation&nbsp;operations.</p>
<p><img width="800" style="margin-right: 20px; margin-bottom: 20px" src="/images/spark_udaf.png"/><br></p>
<p>This simply approach solves our main problem by doing the translation of each partition to Python at once, then calling the function <code>my_func</code> with it and translating back to Scala whatever the function returns. Therefore we have reduced the number of translations to two times (back and forth) the number of partitions and because of that we should keep the number of partitions to a reasonable&nbsp;number.</p>
<p>Having solved one problem, as it is quite often in life, we have introduced another problem. As we are working now with the low-level <span class="caps">RDD</span> interface our function <code>my_func</code> will be passed an iterator of PySpark <a href="https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.Row">Row</a> objects and needs to return them as well. A <code>Row</code> object itself is only a container for the column values in one row as you might have guessed. When we return such a <code>Row</code>, the data types of these values therein must be interpretable by Spark in order to translate them back to Scala. This is a lot of low-level stuff to deal with since in most cases we would love to implement our <span class="caps">UDF</span>/<span class="caps">UDAF</span> with the help of Pandas, keeping in mind that one partition should hold less than 10 million rows. Therefore we make a wish to the coding fairy, cross two fingers that someone else already solved this and start googling&#8230; and here we are&nbsp;;-) </p>
<p>So first we need to define a nice function that will convert a <code>Row</code> iterator into a Pandas&nbsp;DataFrame:</p>
<div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span>


<span class="n">_logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="vm">__name__</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">rows_to_pandas</span><span class="p">(</span><span class="n">rows</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Converts a Spark Row iterator of a partition to a Pandas DataFrame</span>

<span class="sd">    Args:</span>
<span class="sd">        rows: iterator over PySpark Row objects</span>

<span class="sd">    Returns:</span>
<span class="sd">        Pandas DataFrame</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">first_row</span><span class="p">,</span> <span class="n">rows</span> <span class="o">=</span> <span class="n">peek</span><span class="p">(</span><span class="n">rows</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">first_row</span><span class="p">:</span>
        <span class="n">_logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s2">&quot;Spark DataFrame is empty! Returning empty Pandas DataFrame!&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>

    <span class="n">first_row_info</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;{} ({}): {}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">rtype</span><span class="p">(</span><span class="n">first_row</span><span class="p">[</span><span class="n">k</span><span class="p">]),</span> <span class="n">first_row</span><span class="p">[</span><span class="n">k</span><span class="p">])</span>
                      <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">first_row</span><span class="o">.</span><span class="n">__fields__</span><span class="p">]</span>
    <span class="n">_logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="s2">&quot;First partition row: {}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">first_row_info</span><span class="p">))</span>
    <span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="o">.</span><span class="n">from_records</span><span class="p">(</span><span class="n">rows</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">first_row</span><span class="o">.</span><span class="n">__fields__</span><span class="p">)</span>
    <span class="n">_logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="s2">&quot;Converted partition to DataFrame of shape {} with types:</span><span class="se">\n</span><span class="s2">{}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">dtypes</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">df</span>
</pre></div>


<p>This function actually does only one thing which is calling <code>df = pd.DataFrame.from_records(rows, columns=first_row.__fields__)</code> in order to generate a DataFrame. The rest of the code makes sure that the iterator is not empty and for debugging reasons we also peek into the first row and print the value as well as the datatype of each column. This has proven in practice to be extremely helpful in case something goes wrong and one needs to debug what&#8217;s going on in the <span class="caps">UDF</span>/<span class="caps">UDAF</span>. The functions <code>peek</code> and <code>rtype</code> are defined as&nbsp;follows:</p>
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">itertools</span> <span class="kn">import</span> <span class="n">chain</span>


<span class="k">def</span> <span class="nf">peek</span><span class="p">(</span><span class="n">iterable</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Peek into the first element and return the whole iterator again</span>

<span class="sd">    Args:</span>
<span class="sd">        iterable: iterable object like list or iterator</span>

<span class="sd">    Returns:</span>
<span class="sd">        tuple of first element and original iterable</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">iterable</span> <span class="o">=</span> <span class="nb">iter</span><span class="p">(</span><span class="n">iterable</span><span class="p">)</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">first_elem</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">iterable</span><span class="p">)</span>
    <span class="k">except</span> <span class="ne">StopIteration</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">None</span><span class="p">,</span> <span class="n">iterable</span>
    <span class="n">iterable</span> <span class="o">=</span> <span class="n">chain</span><span class="p">([</span><span class="n">first_elem</span><span class="p">],</span> <span class="n">iterable</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">first_elem</span><span class="p">,</span> <span class="n">iterable</span>


<span class="k">def</span> <span class="nf">rtype</span><span class="p">(</span><span class="n">var</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Heuristic representation for nested types/containers</span>

<span class="sd">    Args:</span>
<span class="sd">        var: some (nested) variable</span>

<span class="sd">    Returns:</span>
<span class="sd">        str: string representation of nested datatype (NA=Not Available)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">etype</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">type</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span>

    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">var</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
        <span class="n">elem_type</span> <span class="o">=</span> <span class="n">etype</span><span class="p">(</span><span class="n">var</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="k">if</span> <span class="n">var</span> <span class="k">else</span> <span class="s2">&quot;NA&quot;</span>
        <span class="k">return</span> <span class="s2">&quot;List[{}]&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">elem_type</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">var</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
        <span class="n">keys</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">var</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
        <span class="k">if</span> <span class="n">keys</span><span class="p">:</span>
            <span class="n">key</span> <span class="o">=</span> <span class="n">keys</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">key_type</span><span class="p">,</span> <span class="n">val_type</span> <span class="o">=</span> <span class="n">etype</span><span class="p">(</span><span class="n">key</span><span class="p">),</span> <span class="n">etype</span><span class="p">(</span><span class="n">var</span><span class="p">[</span><span class="n">key</span><span class="p">])</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">key_type</span><span class="p">,</span> <span class="n">val_type</span> <span class="o">=</span> <span class="s2">&quot;NA&quot;</span><span class="p">,</span> <span class="s2">&quot;NA&quot;</span>
        <span class="k">return</span> <span class="s2">&quot;Dict[{}, {}]&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">key_type</span><span class="p">,</span> <span class="n">val_type</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">var</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
        <span class="n">elem_types</span> <span class="o">=</span> <span class="s1">&#39;, &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">etype</span><span class="p">(</span><span class="n">elem</span><span class="p">)</span> <span class="k">for</span> <span class="n">elem</span> <span class="ow">in</span> <span class="n">var</span><span class="p">)</span>
        <span class="k">return</span> <span class="s2">&quot;Tuple[{}]&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">elem_types</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">etype</span><span class="p">(</span><span class="n">var</span><span class="p">)</span>
</pre></div>


<p>The next part is to actually convert the result of our <span class="caps">UDF</span>/<span class="caps">UDAF</span> back to an iterator of Row objects. Since our result will most likely be a Pandas DataFrame or Series, we define the&nbsp;following:</p>
<div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">pyspark.sql.types</span> <span class="kn">import</span> <span class="n">Row</span>


<span class="k">def</span> <span class="nf">convert_dtypes</span><span class="p">(</span><span class="n">rows</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Converts some Pandas data types to pure Python data types</span>

<span class="sd">    Args:</span>
<span class="sd">        rows (array): numpy recarray holding all rows</span>

<span class="sd">    Returns:</span>
<span class="sd">        Iterator over lists of row values</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">dtype_map</span> <span class="o">=</span> <span class="p">{</span><span class="n">pd</span><span class="o">.</span><span class="n">Timestamp</span><span class="p">:</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">to_pydatetime</span><span class="p">(),</span>
                 <span class="n">np</span><span class="o">.</span><span class="n">int8</span><span class="p">:</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nb">int</span><span class="p">(</span><span class="n">x</span><span class="p">),</span>
                 <span class="n">np</span><span class="o">.</span><span class="n">int16</span><span class="p">:</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nb">int</span><span class="p">(</span><span class="n">x</span><span class="p">),</span>
                 <span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">:</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nb">int</span><span class="p">(</span><span class="n">x</span><span class="p">),</span>
                 <span class="n">np</span><span class="o">.</span><span class="n">int64</span><span class="p">:</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nb">int</span><span class="p">(</span><span class="n">x</span><span class="p">),</span>
                 <span class="n">np</span><span class="o">.</span><span class="n">float16</span><span class="p">:</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nb">float</span><span class="p">(</span><span class="n">x</span><span class="p">),</span>
                 <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">:</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nb">float</span><span class="p">(</span><span class="n">x</span><span class="p">),</span>
                 <span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">:</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nb">float</span><span class="p">(</span><span class="n">x</span><span class="p">),</span>
                 <span class="n">np</span><span class="o">.</span><span class="n">float128</span><span class="p">:</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nb">float</span><span class="p">(</span><span class="n">x</span><span class="p">)}</span>
    <span class="k">for</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">rows</span><span class="p">:</span>
        <span class="k">yield</span> <span class="p">[</span><span class="n">dtype_map</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">elem</span><span class="p">),</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">)(</span><span class="n">elem</span><span class="p">)</span> <span class="k">for</span> <span class="n">elem</span> <span class="ow">in</span> <span class="n">row</span><span class="p">]</span>


<span class="k">def</span> <span class="nf">pandas_to_rows</span><span class="p">(</span><span class="n">df</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Converts Pandas DataFrame to iterator of Row objects</span>

<span class="sd">    Args:</span>
<span class="sd">        df: Pandas DataFrame</span>

<span class="sd">    Returns:</span>
<span class="sd">        Iterator over PySpark Row objects</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">df</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">_logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="s2">&quot;Returning nothing&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="nb">iter</span><span class="p">([])</span>
    <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">df</span><span class="p">)</span> <span class="ow">is</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">:</span>
        <span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">to_frame</span><span class="p">()</span><span class="o">.</span><span class="n">T</span>
    <span class="k">if</span> <span class="n">df</span><span class="o">.</span><span class="n">empty</span><span class="p">:</span>
        <span class="n">_logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s2">&quot;Pandas DataFrame is empty! Returning nothing!&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="nb">iter</span><span class="p">([])</span>
    <span class="n">_logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="s2">&quot;Convert DataFrame of shape {} to partition with types:</span><span class="se">\n</span><span class="s2">{}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">dtypes</span><span class="p">))</span>
    <span class="n">records</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">to_records</span><span class="p">(</span><span class="n">index</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
    <span class="n">records</span> <span class="o">=</span> <span class="n">convert_dtypes</span><span class="p">(</span><span class="n">records</span><span class="p">)</span>
    <span class="n">first_row</span><span class="p">,</span> <span class="n">records</span> <span class="o">=</span> <span class="n">peek</span><span class="p">(</span><span class="n">records</span><span class="p">)</span>
    <span class="n">first_row_info</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;{} ({}): {}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">rtype</span><span class="p">(</span><span class="n">v</span><span class="p">),</span> <span class="n">v</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">columns</span><span class="p">,</span> <span class="n">first_row</span><span class="p">)]</span>
    <span class="n">_logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="s2">&quot;First record row: {}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">first_row_info</span><span class="p">))</span>
    <span class="n">row</span> <span class="o">=</span> <span class="n">Row</span><span class="p">(</span><span class="o">*</span><span class="n">df</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">row</span><span class="p">(</span><span class="o">*</span><span class="n">elems</span><span class="p">)</span> <span class="k">for</span> <span class="n">elems</span> <span class="ow">in</span> <span class="n">records</span><span class="p">)</span>
</pre></div>


<p>This looks a bit more complicated but essentially we convert a Pandas Serie to a DataFrame if necessary and handle the edge cases of an empty DataFrame or <code>None</code> as return value. We then convert the DataFrame to records, convert some Numpy data types to the Python equivalent and create an iterator over Row objects from the converted&nbsp;records. </p>
<p>With these function at hand we can define a <a href="https://wiki.python.org/moin/PythonDecorators#What_is_a_Decorator">Python decorator</a> that will allow us to automatically call the functions <code>rows_to_pandas</code> and <code>pandas_to_rows</code> at the right&nbsp;time:</p>
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">functools</span> <span class="kn">import</span> <span class="n">wraps</span>

<span class="k">class</span> <span class="nc">pandas_udaf</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Decorator for PySpark UDAFs using Pandas</span>

<span class="sd">    Args:</span>
<span class="sd">        loglevel (int): minimum loglevel for emitting messages</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">loglevel</span><span class="o">=</span><span class="n">logging</span><span class="o">.</span><span class="n">INFO</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">loglevel</span> <span class="o">=</span> <span class="n">loglevel</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">func</span><span class="p">):</span>
        <span class="nd">@wraps</span><span class="p">(</span><span class="n">func</span><span class="p">)</span>
        <span class="k">def</span> <span class="nf">wrapper</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">):</span>
            <span class="c1"># use *args to allow decorating methods (incl. self arg)</span>
            <span class="n">args</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>
            <span class="n">setup_logger</span><span class="p">(</span><span class="n">loglevel</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">loglevel</span><span class="p">)</span>
            <span class="n">args</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">rows_to_pandas</span><span class="p">(</span><span class="n">args</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
            <span class="n">df</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">pandas_to_rows</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">wrapper</span>
</pre></div>


<p>The code is pretty much self-explanatory if you have ever written a Python decorator otherwise you should read about it since it takes some time to wrap your head around it. Basically, we set up a default logger, create a Pandas DataFrame from the Row iterator, pass it to our <span class="caps">UDF</span>/<span class="caps">UDAF</span> and convert its return value back to a Row iterator. The only additional thing that might still raise questions is the usage of <code>args[-1]</code>. This is due to the fact that <code>func</code> might also be a method of an object. In this case, the first argument would be <code>self</code> but the last argument is in either cases the actual argument that <code>mapPartitions</code> will pass to us. The code of <code>setup_logger</code> depends on your Spark installation. In case you are using Spark on Apache <a href="https://hortonworks.com/apache/yarn/"><span class="caps">YARN</span></a>, it might look like&nbsp;this:</p>
<div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">sys</span>

<span class="k">def</span> <span class="nf">setup_logger</span><span class="p">(</span><span class="n">loglevel</span><span class="o">=</span><span class="n">logging</span><span class="o">.</span><span class="n">INFO</span><span class="p">,</span> <span class="n">logfile</span><span class="o">=</span><span class="s2">&quot;pyspark.log&quot;</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Setup basic logging for logging on the executor</span>

<span class="sd">    Args:</span>
<span class="sd">        loglevel (int): minimum loglevel for emitting messages</span>
<span class="sd">        logfile (str): name of the logfile</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">logfile</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;LOG_DIRS&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;,&#39;</span><span class="p">)[</span><span class="mi">0</span><span class="p">],</span> <span class="n">logfile</span><span class="p">)</span>
    <span class="k">except</span> <span class="p">(</span><span class="ne">KeyError</span><span class="p">,</span> <span class="ne">IndexError</span><span class="p">):</span>
        <span class="k">print</span><span class="p">(</span><span class="s2">&quot;LOG_DIRS not in environment variables or is empty&quot;</span><span class="p">)</span>
        <span class="n">sys</span><span class="o">.</span><span class="n">exit</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

    <span class="n">logformat</span> <span class="o">=</span> <span class="s2">&quot;</span><span class="si">%(asctime)s</span><span class="s2"> </span><span class="si">%(levelname)s</span><span class="s2"> </span><span class="si">%(module)s</span><span class="s2">.</span><span class="si">%(funcName)s</span><span class="s2">: </span><span class="si">%(message)s</span><span class="s2">&quot;</span>
    <span class="n">logging</span><span class="o">.</span><span class="n">basicConfig</span><span class="p">(</span><span class="n">level</span><span class="o">=</span><span class="n">loglevel</span><span class="p">,</span>
                        <span class="n">filename</span><span class="o">=</span><span class="n">logfile</span><span class="p">,</span>
                        <span class="n">format</span><span class="o">=</span><span class="n">logformat</span><span class="p">,</span>
                        <span class="n">datefmt</span><span class="o">=</span><span class="s2">&quot;%y/%m/</span><span class="si">%d</span><span class="s2"> %H:%M:%S&quot;</span><span class="p">)</span>
</pre></div>


<p>Now having all parts in place let&#8217;s assume the code above resides in the python module <code>pyspark_utils.py</code>. A future post will cover the topic of deploying dependencies in a systematic way for production requirements. For now we just presume that <code>pyspark_utils.py</code> as well as all its dependencies like Pandas, Numpy, etc. are accessible by the Spark driver as well as the executors. This allows us to then easily define an example <span class="caps">UDAF</span> <code>my_func</code> that collects some basic statistics for
 each country&nbsp;as:</p>
<div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pyspark_utils</span>

<span class="nd">@pyspark_utils.pandas_udaf</span><span class="p">(</span><span class="n">loglevel</span><span class="o">=</span><span class="n">logging</span><span class="o">.</span><span class="n">DEBUG</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">my_func</span><span class="p">(</span><span class="n">df</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">df</span><span class="o">.</span><span class="n">empty</span><span class="p">:</span>
        <span class="k">return</span>
    <span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s1">&#39;country&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s1">&#39;country&#39;</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">describe</span><span class="p">())</span>
    <span class="k">return</span> <span class="n">df</span><span class="o">.</span><span class="n">reset_index</span><span class="p">()</span>
</pre></div>


<p>It is of course not really useful in practice to return some statistics with the help of a <span class="caps">UDAF</span> that could also be retrieved with basic PySpark functionality but this is just an example. We now generate a dummy data frame and apply the function to each partition as above&nbsp;with:</p>
<div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pyspark_utils</span>

<span class="c1"># make pyspark_utils.py available to the executors</span>
<span class="n">sc</span><span class="o">.</span><span class="n">addFile</span><span class="p">(</span><span class="s1">&#39;./pyspark_utils.py&#39;</span><span class="p">)</span> 

<span class="n">df</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">parallelize</span><span class="p">(</span>
    <span class="p">[(</span><span class="s1">&#39;DEU&#39;</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;DEU&#39;</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mf">8.0</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;FRA&#39;</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mf">6.0</span><span class="p">),</span> 
     <span class="p">(</span><span class="s1">&#39;FRA&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">8.0</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;DEU&#39;</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mf">8.0</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;FRA&#39;</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">)]</span>
    <span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">toDF</span><span class="p">([</span><span class="s1">&#39;country&#39;</span><span class="p">,</span> <span class="s1">&#39;feature1&#39;</span><span class="p">,</span> <span class="s1">&#39;feature2&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">cache</span><span class="p">()</span>

<span class="n">stats_df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">repartition</span><span class="p">(</span><span class="s1">&#39;country&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">rdd</span><span class="o">.</span><span class="n">mapPartitions</span><span class="p">(</span><span class="n">my_func</span><span class="p">)</span><span class="o">.</span><span class="n">toDF</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="n">stats_df</span><span class="o">.</span><span class="n">toPandas</span><span class="p">())</span>
</pre></div>


<p>The code above can be easily tested with the help of a Jupyter notebook with PySpark where the SparkContext <code>sc</code> is predefined. One should also note that this proposed method allows the definition of a <span class="caps">UDF</span> as well as an <span class="caps">UDAF</span> since it is up to the function <code>my_func</code> if it returns a DataFrame having as many rows as the input data frame (think <a href="https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.transform.html">Pandas transform</a>), a DataFrame of only a single row or optionally a Series (think <a href="https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.aggregate.html">Pandas aggregate</a>) or a DataFrame with an arbitrary number of rows (think <a href="https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.apply.html">Pandas apply</a>) with even varying columns. Therefore, we can conclude that the proposed method is not only faster than the official way in case of a <span class="caps">UDF</span>, it also even flexible enough to allow the definition of&nbsp;UDAFs.  </p>
            </div>
            <!-- /.entry-content -->
<section class="well" id="related-posts">
    <h4>Related Posts:</h4>
    <ul>
        <li><a href="http://www.florianwilhelm.info/2013/10/handling_big_data_with_python/">Handling Big Data with&nbsp;Python</a></li>
        <li><a href="http://www.florianwilhelm.info/2016/10/python_udf_in_hive/">Hive UDFs and UDAFs with&nbsp;Python</a></li>
        <li><a href="http://www.florianwilhelm.info/2017/07/declarative_thinking_and_programming/">Declarative Thinking and&nbsp;Programming</a></li>
        <li><a href="http://www.florianwilhelm.info/2014/07/extending_scikit-learn_with_your_own_regressor/">Extending Scikit-Learn with your own&nbsp;regressor</a></li>
        <li><a href="http://www.florianwilhelm.info/2014/07/howto_setup_a_new_python_project/">How to setup a new Python&nbsp;project</a></li>
    </ul>
</section>
    <hr/>
    <section class="comments" id="comments">
        <h2>Comments</h2>

        <div id="disqus_thread"></div>
        <script type="text/javascript">
            /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
            var disqus_shortname = 'florianwilhelmblog'; // required: replace example with your forum shortname

                    var disqus_identifier = 'efficient_udfs_with_pyspark';
                var disqus_url = 'http://www.florianwilhelm.info/drafts/efficient_udfs_with_pyspark.html';

            var disqus_config = function () {
                this.language = "en";
            };

            /* * * DON'T EDIT BELOW THIS LINE * * */
            (function () {
                var dsq = document.createElement('script');
                dsq.type = 'text/javascript';
                dsq.async = true;
                dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
                (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
            })();
        </script>
        <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by
            Disqus.</a></noscript>
        <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>

    </section>
        </article>
    </section>

        </div>
        <div class="col-sm-3" id="sidebar">
            <aside>
<!-- Sidebar -->
<section class="well well-sm">
  <ul class="list-group list-group-flush">

<!-- Sidebar/Social -->
<li class="list-group-item">
  <h4><i class="fa fa-home fa-lg"></i><span class="icon-label">Social</span></h4>
  <ul class="list-group" id="social">
    <li class="list-group-item"><a href="https://twitter.com/FlorianWilhelm"><i class="fa fa-twitter-square fa-lg"></i> Twitter</a></li>
    <li class="list-group-item"><a href="https://linkedin.com/in/florian-wilhelm-621ba834"><i class="fa fa-linkedin-square fa-lg"></i> LinkedIn</a></li>
    <li class="list-group-item"><a href="https://github.com/FlorianWilhelm"><i class="fa fa-github-square fa-lg"></i> GitHub</a></li>
  </ul>
</li>
<!-- End Sidebar/Social -->

<!-- Sidebar/Tag Cloud -->
<li class="list-group-item">
  <a href="http://www.florianwilhelm.info/"><h4><i class="fa fa-tags fa-lg"></i><span class="icon-label">Tags</span></h4></a>
  <ul class="list-group list-inline tagcloud" id="tags">
    <li class="list-group-item tag-4">
      <a href="http://www.florianwilhelm.info/tag/asynchronous/">asynchronous</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="http://www.florianwilhelm.info/tag/asyncio/">asyncio</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="http://www.florianwilhelm.info/tag/bayesian/">bayesian</a>
    </li>
    <li class="list-group-item tag-2">
      <a href="http://www.florianwilhelm.info/tag/big-data/">big data</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="http://www.florianwilhelm.info/tag/blue-yonder/">Blue Yonder</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="http://www.florianwilhelm.info/tag/bokeh/">bokeh</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="http://www.florianwilhelm.info/tag/causal-inference/">causal inference</a>
    </li>
    <li class="list-group-item tag-3">
      <a href="http://www.florianwilhelm.info/tag/data-science/">data science</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="http://www.florianwilhelm.info/tag/event-driven/">event-driven</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="http://www.florianwilhelm.info/tag/google-hangouts/">google hangouts</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="http://www.florianwilhelm.info/tag/gps/">gps</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="http://www.florianwilhelm.info/tag/hadoop/">hadoop</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="http://www.florianwilhelm.info/tag/hive/">hive</a>
    </li>
    <li class="list-group-item tag-2">
      <a href="http://www.florianwilhelm.info/tag/jupyter/">jupyter</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="http://www.florianwilhelm.info/tag/kalman/">kalman</a>
    </li>
    <li class="list-group-item tag-2">
      <a href="http://www.florianwilhelm.info/tag/machine-learning/">machine-learning</a>
    </li>
    <li class="list-group-item tag-3">
      <a href="http://www.florianwilhelm.info/tag/predictive-analytics/">predictive analytics</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="http://www.florianwilhelm.info/tag/programming/">programming</a>
    </li>
    <li class="list-group-item tag-1">
      <a href="http://www.florianwilhelm.info/tag/python/">python</a>
    </li>
    <li class="list-group-item tag-2">
      <a href="http://www.florianwilhelm.info/tag/scikit-learn/">scikit-learn</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="http://www.florianwilhelm.info/tag/scipy/">scipy</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="http://www.florianwilhelm.info/tag/template/">template</a>
    </li>
  </ul>
</li>
<!-- End Sidebar/Tag Cloud -->
  </ul>
</section>
<!-- End Sidebar -->            </aside>
        </div>
    </div>
</div>
<footer>
   <div class="container">
      <hr>
      <div class="row">
         <div class="col-xs-10">&copy; 2017 Florian Wilhelm
            &middot; Powered by <a href="https://github.com/getpelican/pelican-themes/tree/master/pelican-bootstrap3" target="_blank">pelican-bootstrap3</a>,
            <a href="http://docs.getpelican.com/" target="_blank">Pelican</a>,
            <a href="http://getbootstrap.com" target="_blank">Bootstrap</a>         </div>
         <div class="col-xs-2"><p class="pull-right"><i class="fa fa-arrow-up"></i> <a href="#">Back to top</a></p></div>
      </div>
   </div>
</footer>
<script src="http://www.florianwilhelm.info/theme/js/jquery.min.js"></script>

<!-- Include all compiled plugins (below), or include individual files as needed -->
<script src="http://www.florianwilhelm.info/theme/js/bootstrap.min.js"></script>

<!-- Enable responsive features in IE8 with Respond.js (https://github.com/scottjehl/Respond) -->
<script src="http://www.florianwilhelm.info/theme/js/respond.min.js"></script>

    <!-- Disqus -->
    <script type="text/javascript">
        /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
        var disqus_shortname = 'florianwilhelmblog'; // required: replace example with your forum shortname

        /* * * DON'T EDIT BELOW THIS LINE * * */
        (function () {
            var s = document.createElement('script');
            s.async = true;
            s.type = 'text/javascript';
            s.src = '//' + disqus_shortname + '.disqus.com/count.js';
            (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
        }());
    </script>
    <!-- End Disqus Code -->
    <!-- Google Analytics -->
    <script type="text/javascript">

        var _gaq = _gaq || [];
        _gaq.push(['_setAccount', 'UA-71694209-1']);
        _gaq.push(['_trackPageview']);

        (function () {
            var ga = document.createElement('script');
            ga.type = 'text/javascript';
            ga.async = true;
            ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
            var s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(ga, s);
        })();
    </script>
    <!-- End Google Analytics Code -->

<script type="text/javascript">
jQuery(document).ready(function($) {
    $("div.collapseheader").click(function () {
    $header = $(this).children("span").first();
    $codearea = $(this).children(".input_area");
    console.log($(this).children());
    $codearea.slideToggle(500, function () {
        $header.text(function () {
            return $codearea.is(":visible") ? "Collapse Code" : "Expand Code";
        });
    });
});
});
</script>
</body>
</html>