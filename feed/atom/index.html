<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Florian Wilhelm</title><link href="/" rel="alternate"></link><link href="/feed/atom/index.html" rel="self"></link><id>/</id><updated>2016-03-26T09:00:00+01:00</updated><entry><title>Interactively visualizing distributions in a Jupyter notebook with Bokeh</title><link href="/2016/03/jupyter_distribution_visualizer/" rel="alternate"></link><updated>2016-03-26T09:00:00+01:00</updated><author><name>Florian Wilhelm</name></author><id>tag:,2016-03-26:2016/03/jupyter_distribution_visualizer/</id><summary type="html">&lt;p&gt;If you are doing probabilistic programming you are dealing with all kinds of
different distributions. That means choosing an ensemble of right distributions
which describe the underlying real-world process in a suitable way but also
choosing the right parameters for prior distributions. At that point I often
start visualizing the distributions with the help of &lt;a href="http://jupyter.org/"&gt;Jupyter&lt;/a&gt; notebooks,
&lt;a href="http://matplotlib.org/"&gt;matplotlib&lt;/a&gt; and &lt;a href="http://www.scipy.org/"&gt;SciPy&lt;/a&gt; to get a feeling how the distribution behaves when
changing its parameters. And please don&amp;#8217;t tell me you are able to visualize all the
distributions &lt;a href="http://docs.scipy.org/doc/scipy/reference/stats.html"&gt;scipy.stats&lt;/a&gt; has to offer just in your&amp;nbsp;head.&lt;/p&gt;
&lt;p&gt;For me, this surely is a repetitive task that every good and lazy programmer tries
to avoid. Additionally, I was never quite satisfied with the interactivity of
matplotlib in a notebook. Granted, the &lt;code&gt;%matplotlib notebook&lt;/code&gt; magic was a huge
step into the right direction but there is still much room for improvement.
The new and shiny kid on the block is &lt;a href="http://bokeh.pydata.org/"&gt;Bokeh&lt;/a&gt; and so far I have not really done
much with it, meaning it is a good candidate for a test ride. The same goes
actually for Jupyter&amp;#8217;s &lt;a href="http://ipywidgets.readthedocs.org/"&gt;ipywidgets&lt;/a&gt; and you see where this going. No evaluation
of a tool without a proper goal and that is now set to developing an interactive
visualization widget for Jupyter based on Bokeh and ipywidgets. So here we&amp;nbsp;go!&lt;/p&gt;
&lt;p&gt;It turned out that this task is easier than expected due the good documentation
and examples of ipywidgets and especially Bokeh. You can read all about the
implementation inside this &lt;a href="https://github.com/FlorianWilhelm/distvis/blob/master/index.ipynb"&gt;notebook&lt;/a&gt; which is hosted in a separate
&lt;a href="https://github.com/FlorianWilhelm/distvis"&gt;Github repository&lt;/a&gt;. This also always me to make use of a new service that I
just recently learned about, &lt;a href="http://mybinder.org/"&gt;binder&lt;/a&gt;. This totally rad service takes any
Github repository with a Jupyter notebook in it, fires up a container with Kubernetes,
installs necessary requirements and finally runs your notebook! By just clicking
on a link! Amazing to see how the ecosystem around Jupyter develops these&amp;nbsp;days.&lt;/p&gt;
&lt;p&gt;And of course to wet your appetite, here are the screenshots of the final tool
that you will experience interactively by &lt;a href="http://mybinder.org/repo/FlorianWilhelm/distvis"&gt;starting the notebook with binder&lt;/a&gt;.&lt;/p&gt;
&lt;figure&gt;
&lt;img class="noZoom" src="/images/alpha_dist.png" alt="Alpha distribution"&gt;
&lt;figcaption&gt;The probability density function of a continuous alpha distribution with shape parameter a=1.3&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;
&lt;figure&gt;
&lt;img class="noZoom" src="/images/binom_dist.png" alt="Binomial distribution"&gt;
&lt;figcaption&gt;The probability mass function of a discrete binomial distribution with shape parameters n=10 and p=0.7&lt;/figcaption&gt;
&lt;/figure&gt;</summary><category term="jupyter"></category><category term="python"></category><category term="scipy"></category><category term="bokeh"></category></entry><entry><title>Explaining the Idea behind ARD and Bayesian Interpolation</title><link href="/2016/03/explaining_the_idea_behind_ard/" rel="alternate"></link><updated>2016-03-13T22:00:00+01:00</updated><author><name>Florian Wilhelm</name></author><id>tag:,2016-03-13:2016/03/explaining_the_idea_behind_ard/</id><summary type="html">&lt;p&gt;This talk presented at the &lt;a href="http://pydata.org/amsterdam2016/schedule/presentation/17/"&gt;PyData Amsterdam 2016&lt;/a&gt; explains the idea of Bayesian
model selection techniques, especially the Automatic Relevance Determination.
The slides of this talk are available on &lt;a href="http://www.slideshare.net/FlorianWilhelm2/explaining-the-idea-behind-automatic-relevance-determination-and-bayesian-interpolation-59498957"&gt;SlideShare&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Even in the era of Big Data there are many real-world problems where the number
of input features has about the some order of magnitude than the number of samples.
Often many of those input features are irrelevant and thus inferring the relevant
ones is an important problem in order to prevent over-fitting. Automatic Relevance
Determination solves this problem by applying Bayesian&amp;nbsp;techniques.&lt;/p&gt;
&lt;p&gt;In order to motivate Automatic Relevance Determination (&lt;span class="caps"&gt;ARD&lt;/span&gt;) an intuition for
the problem of choosing a complex model that fits the data well vs a simple model
that generalizes well is established. Thereby the idea behind Occam&amp;#8217;s razor is
presented as a way of balancing bias and variance. This leads us to the mathematical
framework of Bayesian interpolation and model selection to choose between different
models based on the&amp;nbsp;data.&lt;/p&gt;
&lt;p&gt;To derive &lt;span class="caps"&gt;ARD&lt;/span&gt; as gently as possible the mathematical basics of a simple linear model
are repeated as well as the idea of regularization to prevent over-fitting.
Based on that, the Bayesian Ridge Regression (BayesianRidge in Scikit-Learn) is
introduced. Generalizing the concept of Bayesian Ridge Regression even more gets
us eventually to the the idea behind &lt;span class="caps"&gt;ARD&lt;/span&gt; (ARDRegression in&amp;nbsp;Scikit-Learn).&lt;/p&gt;
&lt;p&gt;With the help of a practical example, we consolidate what has been learned so far
and compare &lt;span class="caps"&gt;ARD&lt;/span&gt; to an ordinary least square model. Now we dive deep into the
mathematics of &lt;span class="caps"&gt;ARD&lt;/span&gt; and present the algorithm that solves the minimization problem
of &lt;span class="caps"&gt;ARD&lt;/span&gt;. Finally, some details of Scikit-Learn&amp;#8217;s &lt;span class="caps"&gt;ARD&lt;/span&gt; implementation are&amp;nbsp;discussed.&lt;/p&gt;</summary><category term="scikit-learn"></category><category term="machine-learning"></category><category term="bayesian"></category></entry><entry><title>Introduction to the Python Data Science Stack</title><link href="/2016/02/introduction_to_the_python_data_science_stack/" rel="alternate"></link><updated>2016-02-13T12:30:00+01:00</updated><author><name>Florian Wilhelm</name></author><id>tag:,2016-02-13:2016/02/introduction_to_the_python_data_science_stack/</id><summary type="html">&lt;p&gt;Often I get the question as a Data Scientist what the &lt;em&gt;Python Data Science Stack&lt;/em&gt;
actually is and where a beginner should start to learn. The Python ecosystem,
especially around the topics such as data analytics, data mining, data science and
machine learning is so vast and rich that it confuses many&amp;nbsp;rookies.&lt;/p&gt;
&lt;p&gt;For such an audience I created a slide deck that starts with pointing out the
benefits of the Python language for analytics. Even beginners in Python are
addressed by some slides that explain the syntax of Python and how to get
started. After that some slides present the most important packages of the data
science stack, namely &lt;a href="http://www.numpy.org/"&gt;NumPy&lt;/a&gt;, &lt;a href="http://www.scipy.org/"&gt;SciPy&lt;/a&gt;, &lt;a href="http://pandas.pydata.org/"&gt;Pandas&lt;/a&gt;, &lt;a href="http://scikit-learn.org/"&gt;Scikit-Learn&lt;/a&gt;,
&lt;a href="http://jupyter.org/"&gt;Jupyter&lt;/a&gt; and &lt;a href="http://ipython.org/"&gt;IPython&lt;/a&gt;. The merits of Jupyter are best shown in a live
demonstration to convey its power. The interplay between Pandas and
Scikit-Learn is shown based on Kaggle&amp;#8217;s &lt;a href="https://www.kaggle.com/c/titanic"&gt;Titanic: Machine Learning from Disaster&lt;/a&gt;
dataset. Eventually, an outlook to further libraries in the data science domain
are&amp;nbsp;presented.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href="http://nbviewer.jupyter.org/format/slides/github/FlorianWilhelm/python_data-science_stack/blob/master/notebook.ipynb#/"&gt;View in fullscreen&lt;/a&gt;&lt;/strong&gt;
&lt;iframe src="http://nbviewer.jupyter.org/format/slides/github/FlorianWilhelm/python_data-science_stack/blob/master/notebook.ipynb#/"
style="width: 100%; height: 600px" scrolling="no" marginwidth="0" marginheight="0" frameborder="0" vspace="0" hspace="0"&gt;
&lt;/iframe&gt;&lt;/p&gt;</summary><category term="python"></category><category term="data science"></category><category term="jupyter"></category></entry><entry><title>How to write a friendly reminder bot</title><link href="/2015/07/howto_write_a_friendly_reminder_bot/" rel="alternate"></link><updated>2015-12-22T19:30:00+01:00</updated><author><name>Florian Wilhelm</name></author><id>tag:,2015-07-24:2015/07/howto_write_a_friendly_reminder_bot/</id><summary type="html">&lt;p&gt;In this presentation given at the &lt;a href="https://ep2015.europython.eu/"&gt;EuroPython 2015&lt;/a&gt; in Bilbao,
I show how the &lt;a href="https://github.com/tdryer/hangups"&gt;hangups&lt;/a&gt; library can be used
in order to write a small chatbot that connects to Google Hangouts
and reminds you or someone else to take his/her medication.
The secure and recommended OAuth2 protocol is used to authorize the bot application
in the Google Developers Console in order to access the Google+ Hangouts &lt;span class="caps"&gt;API&lt;/span&gt;.
Subsequently, I explain how to use an event-driven library to write a bot
that sends scheduled messages, waits for a proper reply and repeats the question if need be.
Thereby, a primer on event-driven, asynchronous architectures is&amp;nbsp;given.&lt;/p&gt;
&lt;p&gt;The source code can be downloaded on &lt;a href="https://github.com/blue-yonder/medbot"&gt;GitHub&lt;/a&gt;
and the slides are available as &lt;a href="http://htmlpreview.github.io/?https://github.com/blue-yonder/medbot/blob/master/medbot.slides.html?theme=solarized#/"&gt;html preview&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;div class="videobox"&gt;
                &lt;iframe width="800" height="500"
                    src='https://www.youtube.com/embed/ztfdv9jcxtw'
                    frameborder='0' webkitAllowFullScreen mozallowfullscreen
                    allowFullScreen&gt;
                &lt;/iframe&gt;
            &lt;/div&gt;&lt;/p&gt;</summary><category term="python"></category><category term="google hangouts"></category><category term="asyncio"></category><category term="event-driven"></category><category term="asynchronous"></category></entry><entry><title>Extending Scikit-Learn with your own regressor</title><link href="/2014/07/extending_scikit-learn_with_your_own_regressor/" rel="alternate"></link><updated>2015-12-22T19:30:00+01:00</updated><author><name>Florian Wilhelm</name></author><id>tag:,2014-07-25:2014/07/extending_scikit-learn_with_your_own_regressor/</id><summary type="html">&lt;p&gt;&lt;a href="http://scikit-learn.org/"&gt;Scikit-Learn&lt;/a&gt; is a well-known and popular framework for
machine learning that is used by Data Scientists all over the world.
In this tutorial presented at the &lt;a href="https://ep2014.europython.eu/"&gt;EuroPython 2014&lt;/a&gt; in Berlin,
I show in a practical way how you can add your own estimator following the interfaces of Scikit-Learn.
First a small introduction to the design of Scikit-Learn and its inner workings is given.
Then I show how easily Scikit-Learn can be extended by creating an own estimator.
In order to demonstrate this, I extend Scikit-Learn by the popular and robust
&lt;a href="http://en.wikipedia.org/wiki/Theil%E2%80%93Sen_estimator"&gt;Theil-Sen Estimator&lt;/a&gt;
that was not in Scikit-Learn until version 0.16.
I also motivate this estimator by outlining some of its superior properties compared
to the ordinary least squares method (LinearRegression in&amp;nbsp;Scikit-Learn).&lt;/p&gt;
&lt;p&gt;&lt;div class="videobox"&gt;
                &lt;iframe width="800" height="500"
                    src='https://www.youtube.com/embed/u2tnvWyO3U0'
                    frameborder='0' webkitAllowFullScreen mozallowfullscreen
                    allowFullScreen&gt;
                &lt;/iframe&gt;
            &lt;/div&gt;&lt;/p&gt;</summary><category term="python"></category><category term="scikit-learn"></category><category term="machine-learning"></category></entry></feed>