<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Florian Wilhelm</title><link href="https://florianwilhelm.info/" rel="alternate"></link><link href="https://florianwilhelm.info/feed/atom/index.html" rel="self"></link><id>https://florianwilhelm.info/</id><updated>2019-12-20T18:00:00+01:00</updated><entry><title>Are you sure about that?! Uncertainty Quantification in AI</title><link href="https://florianwilhelm.info/2019/10/uncertainty_quantification_in_ai/" rel="alternate"></link><published>2019-10-10T18:00:00+02:00</published><updated>2019-12-20T18:00:00+01:00</updated><author><name>Florian Wilhelm</name></author><id>tag:florianwilhelm.info,2019-10-10:/2019/10/uncertainty_quantification_in_ai/</id><summary type="html">&lt;p&gt;With the advent of Deep Learning (&lt;span class="caps"&gt;DL&lt;/span&gt;), the field of &lt;span class="caps"&gt;AI&lt;/span&gt; made a giant leap forward and it is nowadays applied in many industrial use-cases. Especially critical systems like autonomous driving, require that &lt;span class="caps"&gt;DL&lt;/span&gt; methods not only produce a prediction but also state the certainty about the prediction in order …&lt;/p&gt;</summary><content type="html">&lt;p&gt;With the advent of Deep Learning (&lt;span class="caps"&gt;DL&lt;/span&gt;), the field of &lt;span class="caps"&gt;AI&lt;/span&gt; made a giant leap forward and it is nowadays applied in many industrial use-cases. Especially critical systems like autonomous driving, require that &lt;span class="caps"&gt;DL&lt;/span&gt; methods not only produce a prediction but also state the certainty about the prediction in order to assess risks and&amp;nbsp;failure.&lt;/p&gt;
&lt;p&gt;In my talk, I will give an introduction to different kinds of uncertainty, i.e. epistemic and aleatoric. To have a baseline for comparison, the classical method of Gaussian Processes for regression problems is presented. I then elaborate on different &lt;span class="caps"&gt;DL&lt;/span&gt; methods for uncertainty quantification like Quantile Regression, Monte-Carlo Dropout, and Deep Ensembles. The talk is concluded with a comparison of these techniques to Gaussian Processes and the current state of the&amp;nbsp;art.&lt;/p&gt;
&lt;p&gt;This talk was presented at &lt;a href="https://de.pycon.org/"&gt;PyCon.&lt;span class="caps"&gt;DE&lt;/span&gt; &lt;span class="amp"&gt;&amp;amp;&lt;/span&gt; PyData Berlin 2019&lt;/a&gt; and &lt;a href="https://bigdataconference.lt/2019/"&gt;Big Data Conference Vilnius 2019&lt;/a&gt;. The slides are available on &lt;a href="https://www.slideshare.net/FlorianWilhelm2/uncertainty-quantification-in-ai"&gt;SlideShare&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;span class="videobox"&gt;
                &lt;iframe width="800" height="500"
                    src='https://www.youtube.com/embed/LCDIqL-8bHs'
                    frameborder='0' webkitAllowFullScreen mozallowfullscreen
                    allowFullScreen&gt;
                &lt;/iframe&gt;
            &lt;/span&gt;&lt;/p&gt;</content><category term="python"></category><category term="machine-learning"></category><category term="uncertainty quantification"></category><category term="data science"></category></entry><entry><title>More Efficient UD(A)Fs with PySpark</title><link href="https://florianwilhelm.info/2019/04/more_efficient_udfs_with_pyspark/" rel="alternate"></link><published>2019-04-19T12:30:00+02:00</published><updated>2019-04-19T12:30:00+02:00</updated><author><name>Florian Wilhelm</name></author><id>tag:florianwilhelm.info,2019-04-19:/2019/04/more_efficient_udfs_with_pyspark/</id><summary type="html">&lt;p&gt;With the release of Spark 2.3 implementing user defined functions with PySpark became a lot easier and faster. Unfortunately, there are still some rough edges when it comes to complex data types that need to be worked&amp;nbsp;around.&lt;/p&gt;</summary><content type="html">&lt;p&gt;Some time has passed since my blog post on &lt;a href="https://florianwilhelm.info/2017/10/efficient_udfs_with_pyspark/"&gt;Efficient &lt;span class="caps"&gt;UD&lt;/span&gt;(A)Fs with PySpark&lt;/a&gt; which demonstrated how to define &lt;em&gt;User-Defined Aggregation Function&lt;/em&gt; (&lt;span class="caps"&gt;UDAF&lt;/span&gt;) with &lt;a href="https://spark.apache.org/docs/latest/api/python/index.html"&gt;PySpark&lt;/a&gt; 2.1 that allow you to use &lt;a href="http://pandas.pydata.org/"&gt;Pandas&lt;/a&gt;. Meanwhile, things got a lot easier with the release of Spark 2.3 which provides the &lt;a href="http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.pandas_udf"&gt;pandas_udf&lt;/a&gt; decorator. This decorator gives you the same functionality as our custom &lt;code&gt;pandas_udaf&lt;/code&gt; in the former post but performs much faster if &lt;a href="https://arrow.apache.org/"&gt;Apache Arrow&lt;/a&gt; is activated. &lt;em&gt;Nice, so life is good now? No more workarounds!? Well,&amp;nbsp;almost&amp;#8230;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;If you are just using simple data types in your Spark dataframes everything will work and even blazingly fast if you got Arrow activated but don&amp;#8217;t you dare dealing with complex data types like maps (dictionaries), arrays (lists) and structs. In that case, all you will get is a &lt;code&gt;TypeError: Unsupported type in conversion to Arrow&lt;/code&gt; which is already tracked under issue &lt;a href="https://jira.apache.org/jira/browse/SPARK-21187"&gt;&lt;span class="caps"&gt;SPARK&lt;/span&gt;-21187&lt;/a&gt;. Even a simple &lt;code&gt;toPandas()&lt;/code&gt; does not work which might get you to deactivate Arrow support altogether but this would also keep you from using &lt;code&gt;pandas_udf&lt;/code&gt; which is really&amp;nbsp;nice&amp;#8230; &lt;/p&gt;
&lt;p&gt;To save you from this dilemma, this blog post will demonstrate how to work around the current limitations of Arrow without too much hassle. I tested this on Spark 2.3 and it should also work on Spark 2.4. But before we start, let&amp;#8217;s first take a look into which features &lt;code&gt;pandas_udf&lt;/code&gt; provides and why we should make use of&amp;nbsp;it.&lt;/p&gt;
&lt;h2&gt;Features of Spark 2.3&amp;#8217;s&amp;nbsp;pandas_udf&lt;/h2&gt;
&lt;p&gt;Just to give you a little overview about the functionality, take a look at the table&amp;nbsp;below.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;function type&lt;/th&gt;
&lt;th&gt;Operation&lt;/th&gt;
&lt;th&gt;Input → Output&lt;/th&gt;
&lt;th&gt;Pandas equivalent&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;SCALAR&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Mapping&lt;/td&gt;
&lt;td&gt;Series → Series&lt;/td&gt;
&lt;td&gt;&lt;code&gt;df.transform(...)&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;GROUPED_MAP&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Group &lt;span class="amp"&gt;&amp;amp;&lt;/span&gt; Map&lt;/td&gt;
&lt;td&gt;DataFrame → DataFrame&lt;/td&gt;
&lt;td&gt;&lt;code&gt;df.apply(...)&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;GROUPED_AGG&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Reduce&lt;/td&gt;
&lt;td&gt;Series → Scalar&lt;/td&gt;
&lt;td&gt;&lt;code&gt;df.aggregate(...)&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Besides the return type of your &lt;span class="caps"&gt;UDF&lt;/span&gt;, the &lt;code&gt;pandas_udf&lt;/code&gt; needs you to specify a function type which describes the general behavior of your &lt;span class="caps"&gt;UDF&lt;/span&gt;. If you just want to map a scalar onto a scalar or equivalently a vector onto a vector with the same length, you would pass &lt;code&gt;PandasUDFType.SCALAR&lt;/code&gt;. This would also determine that your &lt;span class="caps"&gt;UDF&lt;/span&gt; retrieves a Pandas series as input and needs to return a series of the same length. It basically does the same as the &lt;code&gt;transform&lt;/code&gt; method of a Pandas dataframe. A &lt;code&gt;GROUPED_MAP&lt;/code&gt; &lt;span class="caps"&gt;UDF&lt;/span&gt; is the most flexible one since it gets a Pandas dataframe and is allowed to return a modified or new dataframe with an arbitrary shape. From Spark 2.4 on you also have the reduce operation &lt;code&gt;GROUPED_AGG&lt;/code&gt; which takes a Pandas Series as input and needs to return a scalar. Read more details about &lt;code&gt;pandas_udf&lt;/code&gt; in the &lt;a href="https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.pandas_udf"&gt;official Spark documentation&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Basic&amp;nbsp;idea&lt;/h2&gt;
&lt;p&gt;Our workaround will be quite simple. We make use of the &lt;a href="https://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=to_json#pyspark.sql.functions.to_json"&gt;to_json&lt;/a&gt; function and convert all columns with complex data types to &lt;span class="caps"&gt;JSON&lt;/span&gt; strings. Since Arrow can easily handle strings, we are able to use the &lt;a href="http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.pandas_udf"&gt;pandas_udf&lt;/a&gt; decorator. Within our &lt;span class="caps"&gt;UDF&lt;/span&gt;, we convert these columns back to their original types and do our actual work. If we want to return columns with complex types, we just do everything the other way around. That means we convert those columns to &lt;span class="caps"&gt;JSON&lt;/span&gt; within our &lt;span class="caps"&gt;UDF&lt;/span&gt;, return the Pandas dataframe and convert eventually the corresponding columns in the Spark dataframe from &lt;span class="caps"&gt;JSON&lt;/span&gt; to complex types with &lt;a href="https://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=to_json#pyspark.sql.functions.from_json"&gt;from_json&lt;/a&gt;. The following figure illustrates the&amp;nbsp;process.&lt;/p&gt;
&lt;figure&gt;
&lt;p align="center"&gt;
&lt;img class="noZoom" src="/images/pandas_udf_complex.png" alt="Converting complex data types to JSON before applying the UDF"&gt;
&lt;/p&gt;
&lt;/figure&gt;

&lt;p&gt;Our workaround involves a lot of bookkeeping and surely is not that user-friendly. Like we did in the last blog post, it is again possible to hide much of the details with the help of a &lt;a href="https://wiki.python.org/moin/PythonDecorators#What_is_a_Decorator"&gt;Python decorator&lt;/a&gt; from a user. So let&amp;#8217;s get&amp;nbsp;started!&lt;/p&gt;
&lt;h2&gt;Implementation&lt;/h2&gt;
&lt;p&gt;We split our implementation into three different kinds of functionalities: 1. functions that convert a Spark dataframe to and from &lt;span class="caps"&gt;JSON&lt;/span&gt;, 2. functions that do the same for Pandas dataframes and 3. we combine all of them in one decorator. The final and extended implementation can be found in the file &lt;a href="https://florianwilhelm.info/src/pyspark23_udaf.py"&gt;pyspark23_udaf.py&lt;/a&gt; where also some logging mechanism for easier debugging of UDFs was&amp;nbsp;added. &lt;/p&gt;
&lt;h3&gt;1. Conversion of Spark&amp;nbsp;Dataframe&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;pyspark.sql.types&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;MapType&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;StructType&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ArrayType&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;StructField&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;pyspark.sql.functions&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;to_json&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;from_json&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;is_complex_dtype&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;Check if dtype is a complex type&lt;/span&gt;

&lt;span class="sd"&gt;    Args:&lt;/span&gt;
&lt;span class="sd"&gt;        dtype: Spark Datatype&lt;/span&gt;

&lt;span class="sd"&gt;    Returns:&lt;/span&gt;
&lt;span class="sd"&gt;        Bool: if dtype is complex&lt;/span&gt;
&lt;span class="sd"&gt;    &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="nb"&gt;isinstance&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;MapType&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;StructType&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ArrayType&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;complex_dtypes_to_json&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;Converts all columns with complex dtypes to JSON&lt;/span&gt;

&lt;span class="sd"&gt;    Args:&lt;/span&gt;
&lt;span class="sd"&gt;        df: Spark dataframe&lt;/span&gt;

&lt;span class="sd"&gt;    Returns:&lt;/span&gt;
&lt;span class="sd"&gt;        tuple: Spark dataframe and dictionary of converted columns and their data types&lt;/span&gt;
&lt;span class="sd"&gt;    &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;conv_cols&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;dict&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;selects&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;field&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;schema&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;is_complex_dtype&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;field&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dataType&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="n"&gt;conv_cols&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;field&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;field&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dataType&lt;/span&gt;
            &lt;span class="n"&gt;selects&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;to_json&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;field&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;alias&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;field&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
        &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;selects&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;field&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;select&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;selects&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;conv_cols&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;complex_dtypes_from_json&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;col_dtypes&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;Converts JSON columns to complex types&lt;/span&gt;

&lt;span class="sd"&gt;    Args:&lt;/span&gt;
&lt;span class="sd"&gt;        df: Spark dataframe&lt;/span&gt;
&lt;span class="sd"&gt;        col_dtypes (dict): dictionary of columns names and their datatype&lt;/span&gt;

&lt;span class="sd"&gt;    Returns:&lt;/span&gt;
&lt;span class="sd"&gt;        Spark dataframe&lt;/span&gt;
&lt;span class="sd"&gt;    &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;selects&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;column&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;columns&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;column&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;col_dtypes&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;keys&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
            &lt;span class="n"&gt;schema&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;StructType&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;StructField&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;root&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;col_dtypes&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;column&lt;/span&gt;&lt;span class="p"&gt;])])&lt;/span&gt;
            &lt;span class="n"&gt;selects&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;from_json&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;column&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;schema&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;getItem&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;root&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;alias&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;column&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
        &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;selects&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;column&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;select&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;selects&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The function &lt;code&gt;complex_dtypes_to_json&lt;/code&gt; converts a given Spark dataframe to a new dataframe with all columns that have complex types replaced by &lt;span class="caps"&gt;JSON&lt;/span&gt; strings. Besides the converted dataframe, it also returns a dictionary with column names and their original data types which where converted. This information is used by &lt;code&gt;complex_dtypes_from_json&lt;/code&gt; to convert exactly those columns back to their original type. You might find it strange that we define some &lt;code&gt;root&lt;/code&gt; node in the schema. This is necessary due to some restrictions of Spark&amp;#8217;s &lt;a href="https://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=to_json#pyspark.sql.functions.from_json"&gt;from_json&lt;/a&gt; that we circumvent by this. After the conversion, we drop this &lt;code&gt;root&lt;/code&gt; struct again so that &lt;code&gt;complex_dtypes_to_json&lt;/code&gt; and &lt;code&gt;complex_dtypes_from_json&lt;/code&gt; are inverses of each other. We can now also easily define a &lt;code&gt;toPandas&lt;/code&gt; which also works with complex Spark&amp;nbsp;dataframes.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;toPandas&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;Same as df.toPandas() but converts complex types to JSON first&lt;/span&gt;

&lt;span class="sd"&gt;    Args:&lt;/span&gt;
&lt;span class="sd"&gt;        df: Spark dataframe&lt;/span&gt;

&lt;span class="sd"&gt;    Returns:&lt;/span&gt;
&lt;span class="sd"&gt;        Pandas dataframe&lt;/span&gt;
&lt;span class="sd"&gt;    &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;complex_dtypes_to_json&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;)[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;toPandas&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;2. Conversion of Pandas&amp;nbsp;Dataframe&lt;/h3&gt;
&lt;p&gt;Analogously, we define the same functions as above but for Pandas dataframes. The difference is that we need to know which columns to convert to complex types for our actual &lt;span class="caps"&gt;UDF&lt;/span&gt; since we want to avoid probing every column containing strings. In the conversion to &lt;span class="caps"&gt;JSON&lt;/span&gt;, we add the &lt;code&gt;root&lt;/code&gt; node as explained&amp;nbsp;above. &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;json&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;cols_from_json&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;columns&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;Converts Pandas dataframe colums from json&lt;/span&gt;

&lt;span class="sd"&gt;    Args:&lt;/span&gt;
&lt;span class="sd"&gt;        df (dataframe): Pandas DataFrame&lt;/span&gt;
&lt;span class="sd"&gt;        columns (iter): list of or iterator over column names&lt;/span&gt;

&lt;span class="sd"&gt;    Returns:&lt;/span&gt;
&lt;span class="sd"&gt;        dataframe: new dataframe with converted columns&lt;/span&gt;
&lt;span class="sd"&gt;    &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;column&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;columns&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;column&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;column&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;apply&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;json&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;loads&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;ct_val_to_json&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;Convert a scalar complex type value to JSON&lt;/span&gt;

&lt;span class="sd"&gt;    Args:&lt;/span&gt;
&lt;span class="sd"&gt;        value: map or list complex value&lt;/span&gt;

&lt;span class="sd"&gt;    Returns:&lt;/span&gt;
&lt;span class="sd"&gt;        str: JSON string&lt;/span&gt;
&lt;span class="sd"&gt;    &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;json&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dumps&lt;/span&gt;&lt;span class="p"&gt;({&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;root&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;cols_to_json&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;columns&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;Converts Pandas dataframe columns to json and adds root handle&lt;/span&gt;

&lt;span class="sd"&gt;    Args:&lt;/span&gt;
&lt;span class="sd"&gt;        df (dataframe): Pandas DataFrame&lt;/span&gt;
&lt;span class="sd"&gt;        columns ([str]): list of column names&lt;/span&gt;

&lt;span class="sd"&gt;    Returns:&lt;/span&gt;
&lt;span class="sd"&gt;        dataframe: new dataframe with converted columns&lt;/span&gt;
&lt;span class="sd"&gt;    &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;column&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;columns&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;column&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;column&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;apply&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ct_val_to_json&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;3.&amp;nbsp;Decorator&lt;/h3&gt;
&lt;p&gt;At this point we got everything we need for our final decorators named &lt;code&gt;pandas_udf_ct&lt;/code&gt; combining all our ingredients. Like Spark&amp;#8217;s official &lt;a href="http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.pandas_udf"&gt;pandas_udf&lt;/a&gt;, our decorator takes the arguments &lt;code&gt;returnType&lt;/code&gt; and &lt;code&gt;functionType&lt;/code&gt;. It&amp;#8217;s just a tad more complicated in the sense that you first have to pass &lt;code&gt;returnType&lt;/code&gt;, &lt;code&gt;functionType&lt;/code&gt; which leaves you with some special decorator. A function decorated with such a decorator takes the parameters &lt;code&gt;cols_in&lt;/code&gt; and &lt;code&gt;cols_out&lt;/code&gt; which specify which columns need to be converted to and from &lt;span class="caps"&gt;JSON&lt;/span&gt;. Only after passing those you end up with the actual &lt;span class="caps"&gt;UDF&lt;/span&gt; that you defined. No need to despair, an example below illustrates the usage but first we take a look at the&amp;nbsp;implementation.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;json&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;functools&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;wraps&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;pyspark.sql.functions&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;pandas_udf&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;PandasUDFType&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pandas&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pd&lt;/span&gt;

&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;pandas_udf_ct&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;object&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;Decorator for UDAFs with Spark &amp;gt;= 2.3 and complex types&lt;/span&gt;

&lt;span class="sd"&gt;    Args:&lt;/span&gt;
&lt;span class="sd"&gt;        returnType: the return type of the user-defined function. The value can be either a &lt;/span&gt;
&lt;span class="sd"&gt;                    pyspark.sql.types.DataType object or a DDL-formatted type string.&lt;/span&gt;
&lt;span class="sd"&gt;        functionType: an enum value in pyspark.sql.functions.PandasUDFType. Default: SCALAR.&lt;/span&gt;

&lt;span class="sd"&gt;    Returns:&lt;/span&gt;
&lt;span class="sd"&gt;        Function with arguments `cols_in` and `cols_out` defining column names having complex &lt;/span&gt;
&lt;span class="sd"&gt;        types that need to be transformed during input and output for GROUPED_MAP. In case of &lt;/span&gt;
&lt;span class="sd"&gt;        SCALAR, we are dealing with a series and thus transformation is done if `cols_in` or &lt;/span&gt;
&lt;span class="sd"&gt;        `cols_out` evaluates to `True`. &lt;/span&gt;
&lt;span class="sd"&gt;        Calling this functions with these arguments returns the actual UDF.&lt;/span&gt;
&lt;span class="sd"&gt;    &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;returnType&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;functionType&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;return_type&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;returnType&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;function_type&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;functionType&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__call__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;func&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="nd"&gt;@wraps&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;func&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;converter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cols_in&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cols_out&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;cols_in&lt;/span&gt; &lt;span class="ow"&gt;is&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                &lt;span class="n"&gt;cols_in&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
            &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;cols_out&lt;/span&gt; &lt;span class="ow"&gt;is&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                &lt;span class="n"&gt;cols_out&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

            &lt;span class="nd"&gt;@pandas_udf&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;return_type&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;function_type&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;udf_wrapper&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
                &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="nb"&gt;isinstance&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataFrame&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
                    &lt;span class="n"&gt;values&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;cols_from_json&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cols_in&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
                &lt;span class="k"&gt;elif&lt;/span&gt; &lt;span class="nb"&gt;isinstance&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Series&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="ow"&gt;and&lt;/span&gt; &lt;span class="n"&gt;cols_in&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                    &lt;span class="n"&gt;values&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;apply&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;json&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;loads&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
                &lt;span class="n"&gt;res&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;func&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
                &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;function_type&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;PandasUDFType&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;GROUPED_MAP&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="nb"&gt;isinstance&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;res&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Series&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
                        &lt;span class="n"&gt;res&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;res&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;to_frame&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt;
                    &lt;span class="n"&gt;res&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;cols_to_json&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;res&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cols_out&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
                &lt;span class="k"&gt;elif&lt;/span&gt; &lt;span class="n"&gt;cols_out&lt;/span&gt; &lt;span class="ow"&gt;and&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;function_type&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;PandasUDFType&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;SCALAR&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                    &lt;span class="n"&gt;res&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;res&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;apply&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ct_val_to_json&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
                &lt;span class="k"&gt;elif&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;isinstance&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;res&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;dict&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="ow"&gt;and&lt;/span&gt; 
                      &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;function_type&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;PandasUDFType&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;GROUPED_AGG&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
                    &lt;span class="n"&gt;res&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ct_val_to_json&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;res&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
                &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;res&lt;/span&gt;

            &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;udf_wrapper&lt;/span&gt;

        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;converter&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;It&amp;#8217;s just a typical decorator-with-parameters implementation but with one more layer of wrapping for &lt;code&gt;cols_in&lt;/code&gt; and &lt;code&gt;cols_out&lt;/code&gt;.  &lt;/p&gt;
&lt;h2&gt;Usage&lt;/h2&gt;
&lt;p&gt;An example says more than one thousand words of explanation. Let&amp;#8217;s first create some dummy Spark dataframe with complex data&amp;nbsp;types:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;pyspark.sql.types&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Row&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;pyspark.sql&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;SparkSession&lt;/span&gt;

&lt;span class="n"&gt;spark&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;SparkSession&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;builder&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;getOrCreate&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;spark&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;conf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;spark.sql.execution.arrow.enabled&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;true&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;spark&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;createDataFrame&lt;/span&gt;&lt;span class="p"&gt;([(&lt;/span&gt;&lt;span class="mf"&gt;1.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;a&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;},&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;a&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;a&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;Row&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt;
                            &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;2.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;b&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;},&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;a&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;b&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;Row&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;42&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt;
                            &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;3.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;a&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;b&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;},&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;d&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;e&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;Row&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))],&lt;/span&gt;
                           &lt;span class="n"&gt;schema&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;vals&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;maps&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;lists&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;structs&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;drop&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;lists&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# only Spark 2.4 supports ArrayTypes in to_json!&lt;/span&gt;
&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;For sake of simplicity, let&amp;#8217;s say we just want to add to the dictionaries in the &lt;code&gt;maps&lt;/code&gt; column a key &lt;code&gt;x&lt;/code&gt; with value &lt;code&gt;42&lt;/code&gt;. But first, we use &lt;code&gt;complex_dtypes_to_json&lt;/code&gt; to get a converted Spark dataframe &lt;code&gt;df_json&lt;/code&gt; and the converted columns &lt;code&gt;ct_cols&lt;/code&gt;. We define then the &lt;span class="caps"&gt;UDF&lt;/span&gt; &lt;code&gt;normalize&lt;/code&gt; and decorate it with our &lt;code&gt;pandas_udf_ct&lt;/code&gt; specifying the return type using &lt;code&gt;dfj_json.schema&lt;/code&gt; (since we only want simple data types) and the function type &lt;code&gt;GROUPED_MAP&lt;/code&gt;. &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;df_json&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ct_cols&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;complex_dtypes_to_json&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;change_vals&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dct&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;dct&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;x&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;42&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;dct&lt;/span&gt;

&lt;span class="nd"&gt;@pandas_udf_ct&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;df_json&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;schema&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;PandasUDFType&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;GROUPED_MAP&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;normalize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pdf&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;pdf&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;maps&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;apply&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;change_vals&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;pdf&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Just for demonstration, we now group by the &lt;code&gt;vals&lt;/code&gt; column of &lt;code&gt;df_json&lt;/code&gt; and apply our &lt;code&gt;normalize&lt;/code&gt; &lt;span class="caps"&gt;UDF&lt;/span&gt; on each group. Instead of just passing &lt;code&gt;normalize&lt;/code&gt; we have to call it first with parameters &lt;code&gt;cols_in&lt;/code&gt; and &lt;code&gt;cols_out&lt;/code&gt; as explained before. As input columns, we pass the output &lt;code&gt;ct_cols&lt;/code&gt; from our &lt;code&gt;complex_dtypes_to_json&lt;/code&gt; function and since we do not change the shape of our dataframe within the &lt;span class="caps"&gt;UDF&lt;/span&gt;, we use the same for the output &lt;code&gt;cols_out&lt;/code&gt;. In case your &lt;span class="caps"&gt;UDF&lt;/span&gt; removes columns or adds additional ones with complex data types, you would have to change &lt;code&gt;cols_out&lt;/code&gt; accordingly. As a final step we use &lt;code&gt;complex_dtypes_from_json&lt;/code&gt; to convert the &lt;span class="caps"&gt;JSON&lt;/span&gt; strings of our transformed Spark dataframe back to complex data&amp;nbsp;types.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;df_json&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;df_json&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;groupby&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;vals&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;apply&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;normalize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cols_in&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;ct_cols&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cols_out&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;ct_cols&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;df_final&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;complex_dtypes_from_json&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;df_json&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ct_cols&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;df_final&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;We have shown a practical workaround to deal with UDFs and complex data types for Spark 2.3/4. As with every workaround, it&amp;#8217;s far from perfect and hopefully the issue &lt;a href="https://jira.apache.org/jira/browse/SPARK-21187"&gt;&lt;span class="caps"&gt;SPARK&lt;/span&gt;-21187&lt;/a&gt; will be resolved soon rendering this workaround unnecessary. That being said, the presented workaround has been running smoothly in production for quite a while now and my data science colleagues adapted this framework to write their own UDFs based on&amp;nbsp;it.&lt;/p&gt;</content><category term="spark"></category><category term="python"></category><category term="big data"></category></entry><entry><title>Querying NoSQL with Deep Learning to Answer Natural Language Questions</title><link href="https://florianwilhelm.info/2019/01/querying_nosql_with_deep_learning_to_answer_natural_language_questions/" rel="alternate"></link><published>2019-01-29T11:30:00+01:00</published><updated>2019-01-29T11:30:00+01:00</updated><author><name>Florian Wilhelm</name></author><id>tag:florianwilhelm.info,2019-01-29:/2019/01/querying_nosql_with_deep_learning_to_answer_natural_language_questions/</id><summary type="html">&lt;p&gt;Almost all of today’s knowledge is stored in databases and thus can only be accessed with the help of domain specific query languages, strongly limiting the number of people which can access the data. In our work, we demonstrate an end-to-end trainable question answering (&lt;span class="caps"&gt;QA&lt;/span&gt;) system that allows a …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Almost all of today’s knowledge is stored in databases and thus can only be accessed with the help of domain specific query languages, strongly limiting the number of people which can access the data. In our work, we demonstrate an end-to-end trainable question answering (&lt;span class="caps"&gt;QA&lt;/span&gt;) system that allows a user to query an external NoSQL database by using natural language. A major challenge of such a system is the non-differentiability of database operations which we overcome by applying policy-based reinforcement learning. We evaluate our approach on Facebook’s bAbI Movie Dialog dataset and achieve a competitive score of 84.2% compared to several benchmark models. We conclude that our approach excels with regard to real-world scenarios where knowledge resides in external databases and intermediate labels are too costly to gather for non-end-to-end trainable &lt;span class="caps"&gt;QA&lt;/span&gt;&amp;nbsp;systems.&lt;/p&gt;
&lt;p&gt;Our paper &lt;em&gt;Querying NoSQL with Deep Learning to Answer Natural Language Questions&lt;/em&gt; was presented at &lt;a href="https://aaai.org/Conferences/AAAI-19/"&gt;&lt;span class="caps"&gt;IAAI&lt;/span&gt;-19&lt;/a&gt; and can be downloaded on &lt;a href="https://aaai.org/Papers/AAAI/2019/IAAI-BlankS.88.pdf"&gt;aaai.org&lt;/a&gt;. There is also a simplified blog post about the same topic on the &lt;a href="https://www.inovex.de/blog/seqpolicynet-nlp-elasticsearch/"&gt;inovex blog&lt;/a&gt; by my colleague Sebastian&amp;nbsp;Blank.&lt;/p&gt;</content><category term="data science"></category><category term="nlp"></category></entry><entry><title>Working efficiently with JupyterLab Notebooks</title><link href="https://florianwilhelm.info/2018/11/working_efficiently_with_jupyter_lab/" rel="alternate"></link><published>2018-11-08T14:00:00+01:00</published><updated>2018-11-08T14:00:00+01:00</updated><author><name>Florian Wilhelm</name></author><id>tag:florianwilhelm.info,2018-11-08:/2018/11/working_efficiently_with_jupyter_lab/</id><summary type="html">&lt;p&gt;Being in the data science domain for quite some years, I have seen good Jupyter notebooks but also a lot of ugly. Notebooks can have the perfect balance between text, code and visualisations but how often do your notebooks rather get messy and incomprehensible after a while? Follow some simple best practices to work more efficiently with your&amp;nbsp;notebooks.&lt;/p&gt;</summary><content type="html">&lt;h2&gt;Motivation&lt;/h2&gt;
&lt;p&gt;If you have ever done something analytical or anything closely related to data science in Python, there is just no way you have not heard of Jupyter or IPython notebooks. In a nutshell, a notebook is an interactive document displayed in your browser which contains source code, e.g. Python and R, as well as rich text elements like paragraphs, equations, figures, links, etc. This combination makes it extremely useful for explorative tasks where the source code, documentation and even visualisations of your analysis are strongly intertwined. Due to this unique characteristic, Jupyter notebooks have achieved a strong adoption particularly in the data science community. But as Pythagoras already noted &amp;#8220;If there be light, then there is darkness.&amp;#8221; and with Jupyter notebooks it&amp;#8217;s no difference of&amp;nbsp;course.&lt;/p&gt;
&lt;p&gt;Being in the data science domain for quite some years, I have seen good but also a lot of ugly. Notebooks that are beautifully designed and perfectly convey ideas and concepts by having the perfect balance between text, code and visualisations like in my all time favourite &lt;a href="https://github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers"&gt;Probabilistic Programming and Bayesian Methods for Hackers&lt;/a&gt;. In strong contrast to this, and actually more often to find in practise, are notebooks with cells containing pages of incomprehensible source code, distracting you from the actual analysis. Also sharing these notebooks is quite often an unnecessary pain. Notebooks that need you to tamper with the &lt;code&gt;PYTHONPATH&lt;/code&gt; or to start Jupyter from a certain directory for modules to import correctly. In this blog post I will introduce several best practices and techniques that will help you to create notebooks which are focused, easy to comprehend and to work&amp;nbsp;with. &lt;/p&gt;
&lt;figure&gt;
&lt;p align="center"&gt;
&lt;img class="noZoom" src="/images/jupyter_worker.png" alt="Worker carrying JupyterLab"&gt;
&lt;/p&gt;
&lt;/figure&gt;

&lt;h2&gt;History&lt;/h2&gt;
&lt;p&gt;Before we get into the actual subject let&amp;#8217;s take some time to understand how &lt;a href="http://jupyter.org/"&gt;Project Jupyter&lt;/a&gt; evolved and where it came from. This will also clarify the confusion people sometimes have over IPython, Jupyter and JupyterLab notebooks. In 2001 Fernando Pérez was quite dissatisfied with the capabilities of Python&amp;#8217;s interactive prompt compared to the commercial notebook environments of Maple and Mathematica which he really liked. In order to improve upon this situation he laid the foundation for a notebook environment by building &lt;a href="https://ipython.org/"&gt;IPython&lt;/a&gt; (Interactive Python), a command shell for interactive computing. IPython quickly became a success as the &lt;a href="https://en.wikipedia.org/wiki/Read%E2%80%93eval%E2%80%93print_loop"&gt;&lt;span class="caps"&gt;REPL&lt;/span&gt;&lt;/a&gt; of choice for many users but it was only a small step towards a graphical interactive notebook environment. Several years and many failed attempts later, it took until late 2010 for Grain Granger and several others to develop a first graphical console, named &lt;a href="https://qtconsole.readthedocs.io/"&gt;QTConsole&lt;/a&gt; which was based on &lt;a href="https://www.qt.io/"&gt;&lt;span class="caps"&gt;QT&lt;/span&gt;&lt;/a&gt;. As the speed of development picked up, IPython 0.12 was released only one year later in December 2011 and included for the first time a browser-based IPython notebook environment. People were psyched about the possibilities &lt;em&gt;IPython notebook&lt;/em&gt; provided them and the adoption rose&amp;nbsp;quickly. &lt;/p&gt;
&lt;p&gt;In 2014, &lt;a href="http://jupyter.org/"&gt;Project Jupyter&lt;/a&gt; started as a spin-off project from IPython for several reasons. At that time IPython encompassed an interactive shell, the notebook server, the &lt;span class="caps"&gt;QT&lt;/span&gt; console and other parts in a single repository with the obvious organisational downsides. After the spin-off, IPython concentrated on providing solely an interactive shell for Python while Project Jupyter itself started as an umbrella organisation for several components like &lt;a href="https://jupyter-notebook.readthedocs.io/"&gt;Jupyter notebook&lt;/a&gt; and &lt;a href="https://qtconsole.readthedocs.io/"&gt;QTConsole&lt;/a&gt;, which were moved over from IPython, as well as many others. Another reason for the split was the fact that Jupyter wanted to support other languages besides Python like &lt;a href="https://www.r-project.org/"&gt;R&lt;/a&gt;, &lt;a href="https://julialang.org/"&gt;Julia&lt;/a&gt; and more. The name Jupyter itself was chosen to reflect the fact that the three most popular languages in data science are supported among others, thus Jupyter is actually an acronym for &lt;strong&gt;Ju&lt;/strong&gt;lia, &lt;strong&gt;Pyt&lt;/strong&gt;hon, &lt;strong&gt;R&lt;/strong&gt;. &lt;/p&gt;
&lt;p&gt;But evolution never stops and the source code of Jupyter notebook built on the web technologies of 2011 started to show its age. As the code grew bigger, people also started to realise that it actually is more than just a notebook. Some parts of it rather dealt with managing files, running notebooks and parallel workers. This eventually led again to the idea of splitting these functionalities and laid the foundation for &lt;a href="https://jupyterlab.readthedocs.io/"&gt;JupyterLab&lt;/a&gt;. JupyterLab is an interactive development environment for working with notebooks, code and data. It has full support for Jupyter notebooks and enables you to use text editors, terminals, data file viewers, and other custom components side by side with notebooks in a tabbed work area. Since February 2018 it&amp;#8217;s officially considered to be &lt;a href="https://blog.jupyter.org/jupyterlab-is-ready-for-users-5a6f039b8906"&gt;ready for users&lt;/a&gt; and the 1.0 release is expected to happen end of&amp;nbsp;2018. &lt;/p&gt;
&lt;p&gt;According to my experience in the last months, JupyterLab is absolutely ready and I recommend everyone to migrate to it. In this post, I will thus focus on JupyterLab and the term notebook or sometimes even Jupyter notebook actually refers to a notebook that was opened with JupyterLab. Practically this means that you run &lt;code&gt;jupyter lab&lt;/code&gt; instead of &lt;code&gt;jupyter notebook&lt;/code&gt;. If you are interested in more historical details read the blog posts of &lt;a href="http://blog.fperez.org/2012/01/ipython-notebook-historical.html"&gt;Fernando Pérez&lt;/a&gt; and &lt;a href="https://www.datacamp.com/community/blog/ipython-jupyter"&gt;Karlijn Willems&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Preparation &lt;span class="amp"&gt;&amp;amp;&lt;/span&gt;&amp;nbsp;Installation&lt;/h2&gt;
&lt;p&gt;The first good practice can actually be learnt before even starting JupyterLab. Since we want our analysis to be reproducible and shareable with colleagues it&amp;#8217;s a good practice to create a clean, isolated environment for every task. For Python you got basically two options &lt;a href="https://virtualenv.pypa.io/"&gt;virtualenv&lt;/a&gt; (also descendants like &lt;a href="https://pipenv.readthedocs.io/"&gt;pipenv&lt;/a&gt;) or &lt;a href="https://conda.io/"&gt;conda&lt;/a&gt; to achieve this. My favorite is conda for several reasons. First of all conda is a package manager of the &lt;a href="https://www.anaconda.com/distribution/"&gt;Anaconda&lt;/a&gt; distribution and allows you to install more than just Python packages. Anaconda is more like a whole operation system coming with packages for Python, R and C/C++ system libraries like libc. From this point of view it&amp;#8217;s much more than what virtualenv provides, since conda will also install system libraries like glibc if need be. Also the Python interpreter itself is installed separately into an isolated environment and thus independent of the one provided by your system. This makes it possible to easily pin down even the Python version of your environment. The tool &lt;a href="https://github.com/pyenv/pyenv"&gt;pyenv&lt;/a&gt; allows you to do the same within the virtualenv ecosystem but conda feels just more integrated and gives a unified approach. In total, conda allows for much more fined-grained control of what is going on in your virtual environment than virtualenv with less side effects induced by your&amp;nbsp;system. &lt;/p&gt;
&lt;p&gt;For these reasons conda is much more common than virtualenv in the field of data science, thus we will use it in this tutorial. Still, everything shown here can analogously be conducted with the help of &lt;a href="https://virtualenv.pypa.io/"&gt;virtualenv&lt;/a&gt;/&lt;a href="https://pipenv.readthedocs.io/"&gt;pipenv&lt;/a&gt; and all the concepts still apply as is also illustrated in a &lt;a href="https://cprohm.de/article/notebooks-and-modules.html"&gt;blog post of Christopher Prohm&lt;/a&gt;. For this tutorial, I assume you have &lt;a href="https://conda.io/miniconda.html"&gt;Miniconda&lt;/a&gt; installed on your system. Besides this, every programmer&amp;#8217;s machine should have &lt;a href="https://git-scm.com/"&gt;Git&lt;/a&gt; installed and set up. The result of the following demonstration can be found in the &lt;a href="https://github.com/FlorianWilhelm/boston_housing"&gt;boston_housing repository&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;0. Use an isolated&amp;nbsp;environment&lt;/h3&gt;
&lt;p&gt;In the spirit of Phil Karlton who supposedly said &amp;#8220;There are only two hard things in Computer Science: cache invalidation and naming things.&amp;#8221;, we gonna select a specific task, namely an analysis based on the all familiar &lt;a href="https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html"&gt;Boston housing dataset&lt;/a&gt;, to help us finding crisp names. Based on our task we create an environment &lt;code&gt;boston_housing&lt;/code&gt; including Python and some common data science libraries&amp;nbsp;with:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="err"&gt;conda create -n boston_housing python=3.6 jupyterlab pandas scikit-learn seaborn&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;After less than a minute the environment is ready to be used and we can activate it with &lt;code&gt;conda activate boston_housing&lt;/code&gt;.&lt;/p&gt;
&lt;h2&gt;Efficient&amp;nbsp;Workflow&lt;/h2&gt;
&lt;p&gt;The code in notebooks tends to grow and grow to the point of being incomprehensible. To overcome this problem, the only way is to extract parts of it into Python modules once in a while. Since it only makes sense to extract functions and classes into Python modules, I often start cleaning up a messy notebook by thinking about the actual task a group of cells is accomplishing. This helps me to refactor those cells into a proper function which I can then migrate into a Python&amp;nbsp;module. &lt;/p&gt;
&lt;p&gt;At the point where you create custom modules, things get trickier. By default Python will only allow you to import modules that are installed in your environment or in your current working directory. Due to this behaviour many people start creating their custom modules in the directory holding their notebook. Since JupyterLab is nice enough to set the current working directory to the directory containing your notebook, everything is fine at the beginning. But as the number of notebooks that share common functionality imported from modules grows, the single directory containing notebooks and modules will get messier as you go. The obvious split of notebooks and modules into different folders or even organizing your notebooks into different folders will not work with this approach since then your imports will&amp;nbsp;fail. &lt;/p&gt;
&lt;p&gt;This observation brings us to one of the most important best practices: &lt;strong&gt;develop your code as a Python package&lt;/strong&gt;. A Python package will allow you to structure your code nicely over several modules and even subpackages, you can easily create unit tests and the best part of it is that distributing and sharing it with your colleagues comes for free. &lt;em&gt;But creating a Python package is so much overhead; surely it&amp;#8217;s not worth this small little analysis I will complete in half a day anyway and then forget about it&lt;/em&gt;, I hear you say. Well, how often is this actually true? Things always start out small but then get bigger and messier if you don&amp;#8217;t adhere to a certain structure right from the start. About half a year later then, your boss will ask you about that specific analysis you did back then and if you could repeat it with the new data and some additional KPIs. But more importantly coming back to the first part of your comment, if you know how, it&amp;#8217;s no overhead at&amp;nbsp;all!&lt;/p&gt;
&lt;h3&gt;1. Develop your code in a Python&amp;nbsp;Package&lt;/h3&gt;
&lt;p&gt;With the help of &lt;a href="https://pyscaffold.org/"&gt;PyScaffold&lt;/a&gt; it is possible to create a proper and standard-compliant Python package within a second. Just install it while having the conda environment activated&amp;nbsp;with:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="err"&gt;conda install -c conda-forge pyscaffold&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This package adds the &lt;code&gt;putup&lt;/code&gt; command into our environment which we use to create a Python package&amp;nbsp;with:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="err"&gt;putup boston_housing&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Now we can change into the new &lt;code&gt;boston_housing&lt;/code&gt; directory and install the package inside our environment in development&amp;nbsp;mode:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="err"&gt;python setup.py develop&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The development mode installs the package in the conda environment by linking to the source code which resides in &lt;code&gt;boston_housing/src/boston_housing&lt;/code&gt;. By doing so all your changes to the code will be directly available without any need to reinstall the package&amp;nbsp;again.&lt;/p&gt;
&lt;p&gt;Let&amp;#8217;s start JupyterLab with &lt;code&gt;jupyter lab&lt;/code&gt; from the root of your new project where &lt;code&gt;setup.py&lt;/code&gt; resides. To keep everything tight and clean, we start by creating a new folder &lt;code&gt;notebooks&lt;/code&gt; using the file browser in the left sidebar. Within this empty folder we create a new notebook using the launcher and rename it to &lt;code&gt;housing_model&lt;/code&gt;. Within the notebook we can now directly test our package by&amp;nbsp;typing:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;boston_housing.skeleton&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;fib&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The &lt;code&gt;skeleton&lt;/code&gt; module is just a test module that &lt;a href="https://pyscaffold.org/"&gt;PyScaffold&lt;/a&gt; provides (omit it with &lt;code&gt;putup --no-skeleton ...&lt;/code&gt;) and we import the Fibonacci function &lt;code&gt;fib&lt;/code&gt; from it. You can now just test this function by calling &lt;code&gt;fib(42)&lt;/code&gt; for&amp;nbsp;instance. &lt;/p&gt;
&lt;p&gt;At that point after having only adhered to a single good practice, we already benefit from many advantages. Since we have nicely separated our notebook from the actual implementation, we can package and distribute our code by just calling &lt;code&gt;python setup.py bdist_wheel&lt;/code&gt; and use &lt;a href="https://twine.readthedocs.io/"&gt;twine&lt;/a&gt; to upload it to some artefact store like &lt;a href="https://pypi.org/"&gt;PyPI&lt;/a&gt; or &lt;a href="https://devpi.net/"&gt;devpi&lt;/a&gt; for internal-only use. Another big plus is that having a package allows us to collaboratively work on the source code in your package using Git. On the other hand using Git with notebooks is a big pain since it its format is not really designed to be human-readable and thus merge conflicts are a horror. 
Still we haven&amp;#8217;t yet added any functionality, so let&amp;#8217;s see how we do about&amp;nbsp;that.&lt;/p&gt;
&lt;h3&gt;2. Extract functionality from the&amp;nbsp;notebook&lt;/h3&gt;
&lt;p&gt;We start with loading the &lt;a href="https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html"&gt;Boston housing dataset&lt;/a&gt; into a dataframe with columns of the lower-cased feature names and the target variable &lt;em&gt;price&lt;/em&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pandas&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pd&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.datasets&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;load_boston&lt;/span&gt;

&lt;span class="n"&gt;boston&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;load_boston&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataFrame&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;boston&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;columns&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;lower&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;boston&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;feature_names&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;price&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;boston&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;target&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Now imagine we would go on like this, do some preprocessing etc., and after a while we would have a pretty extensive notebook of statements and expressions without any structure leading to name collisions and confusion. Since notebooks allow the executing of cells in different order this can be extremely harmful. For these reasons, we create a function&amp;nbsp;instead:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;get_boston_df&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="n"&gt;boston&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;load_boston&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataFrame&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;boston&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;columns&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;lower&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;boston&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;feature_names&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;price&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;boston&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;target&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We test it inside the notebook but then directly extract and move it into a module &lt;code&gt;model.py&lt;/code&gt; that we create within our package under &lt;code&gt;src/boston_boston&lt;/code&gt;. Now, inside our notebook, we can just import and use&amp;nbsp;it:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;boston_housing.model&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;get_boston_df&lt;/span&gt;

&lt;span class="n"&gt;df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;get_boston_df&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Now that looks much cleaner and allows also for other notebooks to just use this bit of functionality without using copy &lt;span class="amp"&gt;&amp;amp;&lt;/span&gt; paste! This leads us to another best practice: Use JupyterLab only for integrating code from your package and keep complex functionality inside the package. Thus, extract larger bits of code from a notebook and move it into a package or directly develop code in a proper &lt;span class="caps"&gt;IDE&lt;/span&gt;.&lt;/p&gt;
&lt;h3&gt;3. Use a proper &lt;span class="caps"&gt;IDE&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;At that point the natural question comes up how to edit the code within your package. Of course JupyterLab will do the job but let&amp;#8217;s face it, it just sucks compared to a real Integrated Development Environment (&lt;span class="caps"&gt;IDE&lt;/span&gt;) for such tasks. On the other hand our package structure is just perfect for a proper &lt;span class="caps"&gt;IDE&lt;/span&gt; like &lt;a href="https://www.jetbrains.com/pycharm/"&gt;PyCharm&lt;/a&gt;, &lt;a href="https://code.visualstudio.com/"&gt;Visual Studio Code&lt;/a&gt; or &lt;a href="https://atom.io/"&gt;Atom&lt;/a&gt; among others. PyCharm which is my favourite &lt;span class="caps"&gt;IDE&lt;/span&gt; has for instance many code inspection and refactoring features that support you in writing high-quality, clean code. Figure 1 illustrates the current state of our little&amp;nbsp;project.   &lt;/p&gt;
&lt;figure&gt;
&lt;p align="center"&gt;
&lt;img class="noZoom" src="/images/pycharm_boston_housing.png" alt="Boston-Housing project view in PyCharm"&gt;
&lt;figcaption&gt;&lt;strong&gt;Figure 1:&lt;/strong&gt; Project structure of the &lt;em&gt;boston-housing&lt;/em&gt; package as created with PyScaffold. The &lt;code&gt;notebooks&lt;/code&gt; folder holds the notebooks for JupyterLab while the &lt;code&gt;src/boston_housing&lt;/code&gt; folder contains the actual code (&lt;code&gt;model.py&lt;/code&gt;) and defines an actual Python package.&lt;/figcaption&gt;
&lt;/p&gt;
&lt;/figure&gt;

&lt;p&gt;If we use an &lt;span class="caps"&gt;IDE&lt;/span&gt; for development we will run into an obvious problem. How can we modify a function in our package and have these modifications reflected in our notebook without restarting the kernel every time? At this point I want to introduce you to your new best friend, the &lt;a href="https://ipython.readthedocs.io/en/stable/config/extensions/autoreload.html"&gt;autoreload extension&lt;/a&gt;. Just add in the first cell of your&amp;nbsp;notebook &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="n"&gt;load_ext&lt;/span&gt; &lt;span class="n"&gt;autoreload&lt;/span&gt;
&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="n"&gt;autoreload&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;and execute. This extension reloads modules before executing user code and thus allows you to use your &lt;span class="caps"&gt;IDE&lt;/span&gt; for development while executing it inside of&amp;nbsp;JupyterLab.&lt;/p&gt;
&lt;h3&gt;4. Know your&amp;nbsp;tool&lt;/h3&gt;
&lt;p&gt;JupyterLab is a powerful tool and knowing how to handle it brings you many advantages. Covering everything would exceed the scope of this blog post and thus I will mention here only practices that I apply&amp;nbsp;commonly.&lt;/p&gt;
&lt;h4&gt;Use Shortcuts to speed up your&amp;nbsp;work.&lt;/h4&gt;
&lt;p&gt;&lt;kbd&gt;Accel&lt;/kbd&gt; means &lt;kbd&gt;Cmd&lt;/kbd&gt; on Mac and &lt;kbd&gt;Ctrl&lt;/kbd&gt; on&amp;nbsp;Windows/Linux.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Command&lt;/th&gt;
&lt;th&gt;Shortcut&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Enter Command Mode&lt;/td&gt;
&lt;td&gt;&lt;kbd&gt;Esc&lt;/kbd&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Run Cell&lt;/td&gt;
&lt;td&gt;&lt;kbd&gt;Ctrl&lt;/kbd&gt; &lt;kbd&gt;Enter&lt;/kbd&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Run Cell &lt;span class="amp"&gt;&amp;amp;&lt;/span&gt; Select Next&lt;/td&gt;
&lt;td&gt;&lt;kbd&gt;Shift&lt;/kbd&gt; &lt;kbd&gt;Enter&lt;/kbd&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Add Cell Above/Below&lt;/td&gt;
&lt;td&gt;&lt;kbd&gt;A&lt;/kbd&gt; / &lt;kbd&gt;B&lt;/kbd&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Copy/Cut/Paste Cell&lt;/td&gt;
&lt;td&gt;&lt;kbd&gt;C&lt;/kbd&gt; / &lt;kbd&gt;X&lt;/kbd&gt; / &lt;kbd&gt;V&lt;/kbd&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Look Around Up/Down&lt;/td&gt;
&lt;td&gt;&lt;kbd&gt;Alt&lt;/kbd&gt; &lt;kbd&gt;⇧&lt;/kbd&gt; / &lt;kbd&gt;⇩&lt;/kbd&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Markdown Cell&lt;/td&gt;
&lt;td&gt;&lt;kbd&gt;M&lt;/kbd&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Code Cell&lt;/td&gt;
&lt;td&gt;&lt;kbd&gt;Y&lt;/kbd&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Delete Cell Output&lt;/td&gt;
&lt;td&gt;&lt;kbd&gt;M&lt;/kbd&gt;, &lt;kbd&gt;Y&lt;/kbd&gt; (workaround)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Delete Cell&lt;/td&gt;
&lt;td&gt;&lt;kbd&gt;D&lt;/kbd&gt; &lt;kbd&gt;D&lt;/kbd&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Toggle Line Numbers&lt;/td&gt;
&lt;td&gt;&lt;kbd&gt;Shift&lt;/kbd&gt; &lt;kbd&gt;L&lt;/kbd&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Comment Line&lt;/td&gt;
&lt;td&gt;&lt;kbd&gt;Ctrl&lt;/kbd&gt; &lt;kbd&gt;/&lt;/kbd&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Command Palette&lt;/td&gt;
&lt;td&gt;&lt;kbd&gt;Accel&lt;/kbd&gt; &lt;kbd&gt;Shift&lt;/kbd&gt; &lt;kbd&gt;C&lt;/kbd&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;File Explorer&lt;/td&gt;
&lt;td&gt;&lt;kbd&gt;Accel&lt;/kbd&gt; &lt;kbd&gt;Shift&lt;/kbd&gt; &lt;kbd&gt;F&lt;/kbd&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Toggle Bar&lt;/td&gt;
&lt;td&gt;&lt;kbd&gt;Accel&lt;/kbd&gt; &lt;kbd&gt;B&lt;/kbd&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Fullscreen Mode&lt;/td&gt;
&lt;td&gt;&lt;kbd&gt;Accel&lt;/kbd&gt; &lt;kbd&gt;Shift&lt;/kbd&gt; &lt;kbd&gt;D&lt;/kbd&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Close Tab&lt;/td&gt;
&lt;td&gt;&lt;kbd&gt;Ctrl&lt;/kbd&gt; &lt;kbd&gt;Q&lt;/kbd&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Launcher&lt;/td&gt;
&lt;td&gt;&lt;kbd&gt;Accel&lt;/kbd&gt; &lt;kbd&gt;Shift&lt;/kbd&gt; &lt;kbd&gt;L&lt;/kbd&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h4&gt;Quickly access&amp;nbsp;documentation&lt;/h4&gt;
&lt;p&gt;If you have ever used a notebook or IPython you surely know that executing a command prefixed with &lt;code&gt;?&lt;/code&gt; gets you the docstring (and with &lt;code&gt;??&lt;/code&gt; the source code). Even easier than that is actually moving the cursor over the command and pressing &lt;kbd&gt;Shift&lt;/kbd&gt; &lt;kbd&gt;Tab&lt;/kbd&gt;. This will open a small drop-down menu displaying the help that closes automatically after the next key&amp;nbsp;stroke.  &lt;/p&gt;
&lt;h4&gt;Avoid unintended&amp;nbsp;outputs&lt;/h4&gt;
&lt;p&gt;Using &lt;code&gt;;&lt;/code&gt; in Python is actually frowned upon but in Jupyterlab you can put it to good use. You surely have noticed outputs like &lt;code&gt;&amp;lt;matplotlib.axes._subplots.AxesSubplot at 0x7fce2e03a208&amp;gt;&lt;/code&gt; when you use a library like Matplotlib for plotting. This is due to the fact that Jupyter renders in the output cell the return value of the function as well as the graphical output. You can easily suppress and only show the plot by appending &lt;code&gt;;&lt;/code&gt; to a command like &lt;code&gt;plt.plot(...);&lt;/code&gt;.&lt;/p&gt;
&lt;h4&gt;Arrange cells and windows according to your&amp;nbsp;needs&lt;/h4&gt;
&lt;p&gt;You can easily arrange two notebooks side by side or in many other ways by clicking and holding on a notebook&amp;#8217;s tab then moving it around. The same applies to cells. Just click on the cell&amp;#8217;s number, hold and move it up or&amp;nbsp;down.&lt;/p&gt;
&lt;h4&gt;Access a cell&amp;#8217;s&amp;nbsp;result&lt;/h4&gt;
&lt;p&gt;Surely you have experienced this facepalm moment when your cell with &lt;code&gt;long_running_transformation(df)&lt;/code&gt; is finally finished but you forgot to store the result in another variable. Don&amp;#8217;t despair! You can just use &lt;code&gt;result = _NUMBER&lt;/code&gt;, e.g. &lt;code&gt;result = _42&lt;/code&gt;, where &lt;code&gt;NUMBER&lt;/code&gt; is the execution number of your cell, e.g. &lt;code&gt;In [42]&lt;/code&gt;, to access and save your result. An alternative to &lt;code&gt;_NUMBER&lt;/code&gt; is &lt;code&gt;Out[NUMBER]&lt;/code&gt;.&lt;/p&gt;
&lt;h4&gt;Use the multicursor&amp;nbsp;support&lt;/h4&gt;
&lt;p&gt;Why should you be satisfied with only one cursor if you can have multiple? Just press &lt;kbd&gt;Alt&lt;/kbd&gt; while holding down your left mouse button to select several rows. Then type as you would normally do to insert or&amp;nbsp;delete. &lt;/p&gt;
&lt;h4&gt;Activate line&amp;nbsp;numbers&lt;/h4&gt;
&lt;p&gt;Let&amp;#8217;s assume you have to debug a cell with lots of code, I know you wouldn&amp;#8217;t have cells with tons of code so let&amp;#8217;s say your colleague caused that mess. To find the line corresponding to the error output more easily, you can just hit &lt;kbd&gt;Shift&lt;/kbd&gt; &lt;kbd&gt;L&lt;/kbd&gt; to show the line numbers for a&amp;nbsp;moment.&lt;/p&gt;
&lt;h4&gt;Search all available&amp;nbsp;actions&lt;/h4&gt;
&lt;p&gt;The Command Palette is surely one of the most powerful features of JupyterLab. Just hit the shortcut &lt;kbd&gt;Accel&lt;/kbd&gt; &lt;kbd&gt;Shift&lt;/kbd&gt; &lt;kbd&gt;C&lt;/kbd&gt; and use the incremental search to find whatever action you are looking for. No more browsing menu drop downs for&amp;nbsp;minutes!&lt;/p&gt;
&lt;h3&gt;5. Create your personal notebook&amp;nbsp;template&lt;/h3&gt;
&lt;p&gt;After I have been using notebooks for a while I realized that in many cases the content of the first cell looks quite similar over many of the notebooks I created. Still, whenever I started something new I typed down the same imports and searched StackOverflow for some Pandas, Seaborn etc. settings. Consequently, a good advise is to have a &lt;code&gt;template.ipynb&lt;/code&gt; notebook somewhere that includes imports of popular packages and often used settings. Instead of creating a new notebook with JupyterLab you then just right-click the &lt;code&gt;template.ipynb&lt;/code&gt; notebook and click &lt;em&gt;Duplicate&lt;/em&gt;. &lt;/p&gt;
&lt;p&gt;The content of my &lt;code&gt;template.ipynb&lt;/code&gt; is&amp;nbsp;basically:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;sys&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;logging&lt;/span&gt;

&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;scipy&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;sp&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;sklearn&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;statsmodels.api&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;sm&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;statsmodels.formula.api&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;ols&lt;/span&gt;

&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="n"&gt;load_ext&lt;/span&gt; &lt;span class="n"&gt;autoreload&lt;/span&gt;
&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="n"&gt;autoreload&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;

&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;matplotlib&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;mpl&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;plt&lt;/span&gt;
&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="n"&gt;matplotlib&lt;/span&gt; &lt;span class="n"&gt;inline&lt;/span&gt;
&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="n"&gt;config&lt;/span&gt; &lt;span class="n"&gt;InlineBackend&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;figure_format&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;retina&amp;#39;&lt;/span&gt;

&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;seaborn&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;sns&lt;/span&gt;
&lt;span class="n"&gt;sns&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_context&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;poster&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;sns&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;rc&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;figure.figsize&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;9.&lt;/span&gt;&lt;span class="p"&gt;)})&lt;/span&gt;
&lt;span class="n"&gt;sns&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_style&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;whitegrid&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pandas&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pd&lt;/span&gt;
&lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_option&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;display.max_rows&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;120&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_option&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;display.max_columns&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;120&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;logging&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;basicConfig&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;level&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;logging&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;INFO&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;stream&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;sys&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;stdout&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;6. Document your&amp;nbsp;analysis&lt;/h3&gt;
&lt;p&gt;A really old programmer&amp;#8217;s joke goes like &amp;#8220;When I wrote this code, only God and I understood what it did. Now&amp;#8230; only God knows.&amp;#8221; The same goes for an analysis or creating a predictive model. Therefore your future self will be very thankful for documentation of your code and even some general information about goals and context. Notebooks allow you to use &lt;a href="https://daringfireball.net/projects/markdown/syntax"&gt;Markdown syntax&lt;/a&gt; to annotate your analysis and you should make plenty use of it. Even mathematical expressions can be embedded using the &lt;code&gt;$...$&lt;/code&gt; notation. More general information about the whole project can be put into &lt;code&gt;README.rst&lt;/code&gt; which was also created by PyScaffold. This file will also be used as long description when the package is built and thus be displayed by an artefact store like &lt;a href="https://pypi.org/"&gt;PyPI&lt;/a&gt; or &lt;a href="https://devpi.net/"&gt;devpi&lt;/a&gt;. Also GitHub and GitLab will display &lt;code&gt;README.rst&lt;/code&gt; and thus provide a good entry point into your project. If you are more into the &lt;a href="https://daringfireball.net/projects/markdown/syntax"&gt;Markdown syntax&lt;/a&gt; and thus rather want a &lt;code&gt;README.md&lt;/code&gt;, you can install the &lt;a href="https://github.com/pyscaffold/pyscaffoldext-markdown"&gt;pyscaffoldext-markdown&lt;/a&gt; extension for PyScaffold which adds a &lt;code&gt;--markdown&lt;/code&gt; flag to PyScaffold&amp;#8217;s &lt;code&gt;putup&lt;/code&gt; command.&lt;/p&gt;
&lt;p&gt;The actual source code in your package should be documented using docstrings which brings us to a famous joke of Andrew Tanenbaum &amp;#8220;The nice thing about standards is that you have so many to choose from&amp;#8221;. The three most common docstring standards for Python are the default &lt;a href="https://www.sphinx-doc.org/en/master/usage/restructuredtext/domains.html#the-python-domain"&gt;Sphinx RestructuredText&lt;/a&gt;, &lt;a href="http://www.sphinx-doc.org/en/master/usage/extensions/napoleon.html"&gt;Numpy and Google style&lt;/a&gt; which are all supported by PyCharm. Personally I like the Google style the most but tastes are different and more important is to be consistent after you have picked one. In case you have lots of documentation which would blow the scope of a single readme file, maybe you came up with a new &lt;span class="caps"&gt;ML&lt;/span&gt; algorithms and want to document the concept behind it, you should take a look at &lt;a href="https://www.sphinx-doc.org/"&gt;Sphinx&lt;/a&gt;. Our project setup already includes a &lt;code&gt;docs&lt;/code&gt; folder with an &lt;code&gt;index.rst&lt;/code&gt; as a starting point and new pages can be easily added. After you have installed Sphinx you can build your documentation as &lt;span class="caps"&gt;HTML&lt;/span&gt;&amp;nbsp;pages:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="err"&gt;conda install spinx&lt;/span&gt;
&lt;span class="err"&gt;python setup.py docs&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;It&amp;#8217;s also possible to create a nice &lt;span class="caps"&gt;PDF&lt;/span&gt; and even serve your documentation as a web page using &lt;a href="https://readthedocs.org/"&gt;ReadTheDocs&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;7. State your dependencies for&amp;nbsp;reproducibility&lt;/h3&gt;
&lt;p&gt;Python and its ecosystem evolve steady and quick, thus things that worked today might break tomorrow after a version of one of your dependencies changed. If you consider yourself a data &lt;em&gt;scientist&lt;/em&gt;, you should always guarantee &lt;strong&gt;reproducibility&lt;/strong&gt; of whatever you do since it&amp;#8217;s the most fundamental pillar of any real science. Reproducibility means that given the same data and code your future you and of course others should be able to run your analysis or model receiving the same results. To achieve this technically we need to record all dependencies and their versions. Using &lt;code&gt;conda&lt;/code&gt; we can do this with our &lt;code&gt;boston_housing&lt;/code&gt; project&amp;nbsp;as:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="err"&gt;conda env export -n boston_housing -f environment.lock.yaml&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This creates a file &lt;code&gt;environment.lock.yaml&lt;/code&gt; that recursively states all dependencies and their version as well as the Python version that was used to allow anyone to deterministically reproduce this environment in the future. This is as easy&amp;nbsp;as &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="err"&gt;conda env create -f environment.lock.yaml --force&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Besides a &lt;em&gt;concrete&lt;/em&gt; environment file that exhaustively lists all dependencies, it&amp;#8217;s also common practice to define an &lt;code&gt;environment.yaml&lt;/code&gt; where you state your &lt;em&gt;abstract&lt;/em&gt; dependencies. These abstract dependencies comprise only libraries which are directly imported with no specific version. In our case this file looks&amp;nbsp;like:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nt"&gt;name&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;boston_housing&lt;/span&gt;
&lt;span class="nt"&gt;channels&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
  &lt;span class="p p-Indicator"&gt;-&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;defaults&lt;/span&gt;
&lt;span class="nt"&gt;dependencies&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
  &lt;span class="p p-Indicator"&gt;-&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;jupyterlab&lt;/span&gt;
  &lt;span class="p p-Indicator"&gt;-&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;pandas&lt;/span&gt;
  &lt;span class="p p-Indicator"&gt;-&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;scikit-learn&lt;/span&gt;
  &lt;span class="p p-Indicator"&gt;-&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;seaborn&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This file keeps track of all libraries you are directly using. If you added a new library you can use this file to update your current environment&amp;nbsp;with:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="err"&gt;conda env update --file environment.yaml&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Remember to regularly update and commit changes to these files in Git. Whenever you are satisfied with an iteration of your work also make use of Git tags in order to have reference points for later. These tags will also be used automatically as version numbers for your Python package which is another benefit of having used PyScaffold for your project&amp;nbsp;setup.&lt;/p&gt;
&lt;p&gt;Reproducible environments are only one aspect of reproducibility. Since many machine learning algorithms (most prominently Deep Learning) use random numbers it&amp;#8217;s important to keep them deterministic by fixing the random seed. This sounds easier at it is since depending on the used framework, there are different ways to accomplish this. A good overview for many common frameworks is provided in the talk &lt;a href="https://www.youtube.com/watch?v=MOBs6MNepDk&amp;amp;feature=youtu.be"&gt;Reproducibility, and Selection Bias in Machine Learning&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;8. Develop locally, execute&amp;nbsp;remotely&lt;/h3&gt;
&lt;p&gt;Quite often when you want to do some heavy lifting, your laptop won&amp;#8217;t be enough and thus you might use some powerful workstation by remote access. Running JupyterLab on the workstation and accessing it, maybe through some &lt;a href="https://www.ssh.com/ssh/tunneling/example"&gt;&lt;span class="caps"&gt;SSH&lt;/span&gt; tunnel&lt;/a&gt;, is no problem at all but how can we now work on the modules in our package? One way would be to run your &lt;span class="caps"&gt;IDE&lt;/span&gt; on the workstation but this comes potentially with many downsides depending on your connection. A flaky connection might lead to increased latencies when typing or reduced resolution. For this reason it&amp;#8217;s best to do the actual coding locally in your &lt;span class="caps"&gt;IDE&lt;/span&gt; and sync every change automatically to the workstation where JupyterLab runs. The general setup for the workstation is analogue to the local setup. We &lt;code&gt;git clone&lt;/code&gt; our repository and use the &lt;code&gt;environment.lock.yaml&lt;/code&gt; to create the exact same environment which we run locally, followed by a &lt;code&gt;python setup.py develop&lt;/code&gt;. If we now start JupyterLab within this environment we will be able to import our&amp;nbsp;package. &lt;/p&gt;
&lt;p&gt;Now comes the interesting part: every change in one of our local modules needs to be reflected also on the remote workstation. You can use a classical command line tool like &lt;a href="https://rsync.samba.org/"&gt;rsync&lt;/a&gt; for that or just rely on the features of your &lt;span class="caps"&gt;IDE&lt;/span&gt;. Over the last years I have grown quite fond of PyCharm&amp;#8217;s Deployment feature as illustrated in Figure 2, which is unfortunately only available in the Professional version. It allows you to configure remote servers and if &lt;em&gt;Automatic Upload&lt;/em&gt; is checked it syncs each file when saving. This convenient feature allows for blazing fast iterations. You make some changes to your model, maybe implement a new transformation function, hit &lt;kbd&gt;Accel&lt;/kbd&gt; &lt;kbd&gt;S&lt;/kbd&gt; to save, hit &lt;kbd&gt;Accel&lt;/kbd&gt; &lt;kbd&gt;Tab&lt;/kbd&gt; to switch to your browser with the JupyterLab tab and then rerun the modified model on the&amp;nbsp;workstation.  &lt;/p&gt;
&lt;p&gt;From time to time, we also need to commit our changes using Git. Since we developed mostly on our local machine we only need to download the content of the &lt;code&gt;notebooks&lt;/code&gt; folder from the remote workstation. For this we can again use rsync or the &lt;em&gt;Download from &amp;#8230;&lt;/em&gt; deployment feature of PyCharm Professional. Thus also all our git operations are executed locally avoiding merge conflicts between the local and remote repository. Git should not be used for syncing tasks&amp;nbsp;anyway.&lt;/p&gt;
&lt;figure&gt;
&lt;p align="center"&gt;
&lt;img class="noZoom" src="/images/pycharm_deployment.png" alt="Deployment tool of PyCharm"&gt;
&lt;figcaption&gt;&lt;strong&gt;Figure 2:&lt;/strong&gt; PyCharm Professional allows you to easily develop locally your Python modules and run them remotely in JupyterLab. It will keep track of local changes and upload them automatically what triggers JupterLab&amp;#8217;s autoreload extension.&lt;/figcaption&gt;
&lt;/p&gt;
&lt;/figure&gt;

&lt;p&gt;Another reason for running JupyterLab on a remote machine might be due to some firewall restrictions. Quite often in order to access sensitive data sources or a &lt;a href="https://spark.apache.org/"&gt;Spark&lt;/a&gt; cluster, you need to run JupyterLab on a gateway server. To invoke JupyterLab with Spark capabilities there are two ways. An ad hoc method is to just state on the command line that JupyterLab should use pyspark as kernel. For instance starting JupyterLab with Python 3.6 (needs to be consistent with your Spark distribution), 20 executors each having 5 cores might look like&amp;nbsp;this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="err"&gt;PYSPARK_PYTHON=python3.6 PYSPARK_DRIVER_PYTHON=&amp;quot;jupyter&amp;quot; PYSPARK_DRIVER_PYTHON_OPTS=&amp;quot;notebook --no-browser --port=8899&amp;quot; /usr/bin/pyspark2 --master yarn --deploy-mode client --num-executors 20  --executor-memory 10g --executor-cores 5 --conf spark.dynamicAllocation.enabled=false&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;In order to be able to create notebooks with a specific PySpark kernel directly from JupyterLab, just create a file &lt;code&gt;~/.local/share/jupyter/kernels/pyspark/kernel.json&lt;/code&gt; holding:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
 &lt;span class="nt"&gt;&amp;quot;display_name&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;PySpark&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
 &lt;span class="nt"&gt;&amp;quot;language&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;python&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
 &lt;span class="nt"&gt;&amp;quot;argv&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;
  &lt;span class="s2"&gt;&amp;quot;/usr/local/anaconda-py3/bin/python&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="s2"&gt;&amp;quot;-m&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="s2"&gt;&amp;quot;ipykernel&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="s2"&gt;&amp;quot;-f&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="s2"&gt;&amp;quot;{connection_file}&amp;quot;&lt;/span&gt;
 &lt;span class="p"&gt;],&lt;/span&gt;
 &lt;span class="nt"&gt;&amp;quot;env&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;quot;HADOOP_CONF_DIR&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;/etc/hadoop/conf&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;quot;HADOOP_USER_NAME&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;username&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;quot;HADOOP_CONF_LIB_NATIVE_DIR&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;/var/lib/cloudera/parcels/CDH/lib/hadoop/lib/native&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;quot;YARN_CONF_DIR&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;/etc/hadoop/conf&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;quot;SPARK_YARN_QUEUE&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;dev&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;quot;PYTHONPATH&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;/usr/local/anaconda-py3/bin/python:/usr/local/anaconda-py3/lib/python3.6/site-packages:/var/lib/cloudera/parcels/SPARK2/lib/spark2/python:/var/lib/cloudera/parcels/SPARK2/lib/spark2/python/lib/py4j-0.10.4-src.zip&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;quot;SPARK_HOME&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;/var/lib/cloudera/parcels/SPARK2/lib/spark2/&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;quot;PYTHONSTARTUP&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;/var/lib/cloudera/parcels/SPARK2/lib/spark2/python/pyspark/shell.py&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;quot;PYSPARK_SUBMIT_ARGS&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;--queue dev --conf spark.dynamicAllocation.enabled=false --conf spark.scheduler.minRegisteredResourcesRatio=1 --conf spark.sql.autoBroadcastJoinThreshold=-1 --master yarn --num-executors 5 --driver-memory 2g --executor-memory 20g --executor-cores 3 pyspark-shell&amp;quot;&lt;/span&gt;
 &lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;We have seen that using an own Python package in conjunction with JupyterLab gives us means to program much cleaner and the ability to use a proper &lt;span class="caps"&gt;IDE&lt;/span&gt;. JupyterLab is a mighty and flexible tool and thus all the more it&amp;#8217;s important to adhere to some best practices and processes to guarantee quality in your software and analysis. The &lt;a href="https://github.com/FlorianWilhelm/boston_housing"&gt;boston_housing repository&lt;/a&gt; demonstrates a simple analysis of the Boston Housing Dataset in accordance with the outlined points&amp;nbsp;above.&lt;/p&gt;
&lt;p&gt;JupyterLab also offers many powerful &lt;a href="https://jupyterlab.readthedocs.io/en/stable/user/extensions.html"&gt;extensions&lt;/a&gt;, e.g. &lt;a href="https://github.com/jupyterlab/jupyterlab-git"&gt;jupyterlab-git&lt;/a&gt;, &lt;a href="https://github.com/jupyterlab/jupyterlab-toc"&gt;jupyterlab-toc&lt;/a&gt;, etc., for improved productivity that are worth checking out. If you have any additions or neat tricks for JupyterLab that were not covered, please let me know by using the comments below. Since general concepts are transferable but the specific workflow may be different, also read the &lt;a href="https://cprohm.de/article/notebooks-and-modules.html"&gt;blog post of Christopher Prohm&lt;/a&gt; about the same topic but using partly a different&amp;nbsp;tooling.&lt;/p&gt;</content><category term="python"></category><category term="jupyter"></category></entry><entry><title>Performance evaluation of GANs in a semi-supervised OCR use case</title><link href="https://florianwilhelm.info/2018/10/performance_evalution_of_gans_in_a_semi-supervised_ocr_use_case/" rel="alternate"></link><published>2018-10-24T18:00:00+02:00</published><updated>2018-10-24T18:00:00+02:00</updated><author><name>Florian Wilhelm</name></author><id>tag:florianwilhelm.info,2018-10-24:/2018/10/performance_evalution_of_gans_in_a_semi-supervised_ocr_use_case/</id><summary type="html">&lt;p&gt;Even in the age of big data labelled data is a scarce resource in many machine learning use cases. We evaluate generative adversarial networks (GANs) at the task of extracting information from vehicle registrations under a varying amount of labelled data and compare the performance with supervised learning techniques. Using unlabelled data shows a significant&amp;nbsp;improvement.&lt;/p&gt;</summary><content type="html">&lt;p&gt;Online vehicle marketplaces are embracing artificial intelligence to ease the process of selling a vehicle on their platform. The tedious work of copying information from the vehicle registration document into some web form can be automated with the help of smart text spotting systems. The seller takes a picture of the document and the necessary information is extracted&amp;nbsp;automatically.&lt;/p&gt;
&lt;p&gt;We introduce the components of a text spotting system including the subtasks of object detection and character object recognition (&lt;span class="caps"&gt;OCR&lt;/span&gt;). In view of our use case, we elaborate on the challenges of &lt;span class="caps"&gt;OCR&lt;/span&gt; in documents with various distortions and artefacts which rule out off-the-shelve products for this&amp;nbsp;task.&lt;/p&gt;
&lt;p&gt;After an introduction of semi-supervised learning based on generative adversarial networks (GANs), we evaluate the performance gains of this method compared to supervised learning. More specifically, for a varying amount of labelled data the accuracy of a convolution neural network (&lt;span class="caps"&gt;CNN&lt;/span&gt;) is compared to a &lt;span class="caps"&gt;GAN&lt;/span&gt; which uses additionally unlabelled data during the training&amp;nbsp;phase.&lt;/p&gt;
&lt;p&gt;We conclude that GANs significantly outperform classical CNNs in use cases with a lack of labelled data. Regarding our use case of extracting information from vehicle registration documents, we show that our text spotting system easily exceeds an accuracy of 99.5% thus making it applicable in a real-world use&amp;nbsp;case.&lt;/p&gt;
&lt;p&gt;This talk was presented at &lt;a href="https://conferences.oreilly.com/artificial-intelligence/ai-eu-2018/public/schedule/detail/70158"&gt;O&amp;#8217;Reilly &lt;span class="caps"&gt;AI&lt;/span&gt; Conference 2018&lt;/a&gt; and &lt;a href="https://de.pycon.org/schedule/talks/performance-evaluation-of-gans-in-a-semi-supervised-ocr-use-case/"&gt;PyCon.de 2018 Karlsruhe&lt;/a&gt;. The slides are available on &lt;a href="https://de.slideshare.net/FlorianWilhelm2/performance-evaluation-of-gans-in-a-semisupervised-ocr-use-case"&gt;SlideShare&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;span class="videobox"&gt;
                &lt;iframe width="800" height="500"
                    src='https://www.youtube.com/embed/XniwzOCWi2c'
                    frameborder='0' webkitAllowFullScreen mozallowfullscreen
                    allowFullScreen&gt;
                &lt;/iframe&gt;
            &lt;/span&gt;&lt;/p&gt;</content><category term="machine-learning"></category><category term="python"></category><category term="GANs"></category><category term="semi-supervised"></category></entry></feed>