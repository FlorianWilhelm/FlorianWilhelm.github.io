<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Florian Wilhelm</title><link href="/" rel="alternate"></link><link href="/feed/atom/index.html" rel="self"></link><id>/</id><updated>2016-04-07T12:00:00+02:00</updated><entry><title>Leveraging the Value of Big Data with Automated Decision Making</title><link href="/2016/04/leveraging_the_value_of-big_data_with_automated_decision_making/" rel="alternate"></link><updated>2016-04-07T12:00:00+02:00</updated><author><name>Florian Wilhelm</name></author><id>tag:,2016-04-07:2016/04/leveraging_the_value_of-big_data_with_automated_decision_making/</id><summary type="html">&lt;p&gt;It is a widely accepted fact that we are living in the era of Big Data. Many
traditional companies are looking for ways to improve their business through
the virtues of Big Data and Data Science. While matured startups born in this
era like Facebook and Twitter seem to naturally exploit the value of their data,
many traditional companies struggle to find new ways of utilizing their data to
leverage its value for their classical&amp;nbsp;businesses.&lt;/p&gt;
&lt;p&gt;In this post I elaborate on the specific domain of decision making where Big Data
and Data Science can help to improve the efficiency of conventional businesses.
Proving the benefits of Big Data in a lighthouse project is of utmost importance
in long-established companies with regard to overcoming initial resistance in
the digital transformation of business processes. We will see that the
automatization of operational decisions, i.e. routine decisions related to the
day-to-day running of the business, are especially suitable candidates for
lighthouse projects to prove the value of Big Data in a&amp;nbsp;company.&lt;/p&gt;
&lt;p&gt;The notion of automating data-driven decisions with the help of Data Science is
often denoted with the term Prescriptive Analytics, which can be regarded as the
conclusive step after Predictive Analytics. In other words, the predictions
generated with the help of Predictive Analytics are used to optimize a predefined
metric under consideration of side conditions, strategic direction, business
processes etc. to derive excellent business decisions. The
&lt;a href="http://www.gartner.com/it-glossary/predictive-analytics/"&gt;predictive analytics diagram from Gartner&lt;/a&gt; illustrates the business value
compared to the difficulty of different analytical&amp;nbsp;approaches.&lt;/p&gt;
&lt;p&gt;In many businesses repetitive operational decisions consume lots of working time.
For instance pricing of articles and services, replenishment of stores or stocks,
demand forecasts and customer services involve operational decisions which are
often conducted in a manual process supported by traditional, rule based decision
support systems. Automating these decisions with the help of data-driven decision
systems has several&amp;nbsp;benefits:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Labor costs are reduced and scarce expertise can be leveraged for non-routine,
exceptional decisions. Less routine decisions means having more time for decisions
in extraordinary circumstances as well as decisions that are of a more tactical
or strategical nature. This encompasses also decisions in situations where data
is lacking as well as decisions about creative and visionary solutions. For
instance, no machine learning algorithm could have ever predicted the success of
the first iPhone since it was something completely new and its success was a
consequence of not only that but also many other soft&amp;nbsp;factors.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The quality of decisions is improved given that all information sources used in
the manual decision process are available as machine readable data. Modern machine
learning algorithm are able to quickly analyze huge amounts of data that a human
being could never even read in a lifetime. This plethora of data allows the
inference of patterns that lead to fast, consistent, high quality decisions which
are resistant to the &lt;a href="https://en.wikipedia.org/wiki/List_of_cognitive_biases"&gt;long list of cognitive biases&lt;/a&gt; or &lt;a href="https://en.wikipedia.org/wiki/Decision_fatigue"&gt;decision fatigue&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Prescriptive Analytics allows to scale the number of decisions. Too often in
traditional businesses, decisions are made by not actually taking a decision which
consequently leads to idleness and thus unexploited potential. Being able to scale
the number of decisions, enables this untapped potential to be fully realized and
can also generate new services. Imagine for instance that one marketing tool of a
company is to give special offerings and product recommendations based on different
market segments, not single customers. Being able to scale the number of decisions
due to automation would allow special offerings and recommendations for individual
customers, just like Amazon’s recommendation system.
To justify our statement that the automation of routine decisions with Prescriptive
Analytics is exceptionally well suited as a pioneer project in a traditional company,
it is necessary to elaborate on certain characteristics that many operational decisions&amp;nbsp;hold.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;While a single operational decision, e.g. a small change in the price of a single
article, may have an insignificantly small but direct impact on the revenue of
the whole business, the sum of all decisions quite often has great economic impact.
This is due to the fact that the frequency of operational decisions is often huge,
meaning that a small overall improvement in decision quality is highly profitable.
Obviously, candidates for a Prescriptive Analytics project should have exactly
these properties of high and direct economic impact. The ability to measure such
an impact requires that a performance metric or key performance indicator (&lt;span class="caps"&gt;KPI&lt;/span&gt;)
is already established. This is another important prerequisite for a successful
Prescriptive Analytics&amp;nbsp;project.&lt;/p&gt;
&lt;p&gt;Since operational decisions are often related to the core of the business even in
traditional companies huge amounts of data are already collected and available.
Often, routine decisions that are taken by analyzing spreadsheets and personal
experience are based on data with high predictive power. A quote, often attributed
to Mark Twain, says that “history doesn’t repeat itself, but it does rhyme” which
captures the essence of what automated decision making is about. Having lots of
data about past events allows us to find patterns and relationships which can
predict future events to some extent. The goal is to develop a model that describes
what happened in the past without being bound to the past and thus allowing to
apply the model to the future. In just the same way as our brain learns from
experiences and infers future outcomes in similar&amp;nbsp;situations.&lt;/p&gt;
&lt;p&gt;Consequently, the high frequency of routine decision with a direct economic impact
combined with an abundance of data and a metric to measure performance are
favorable characteristic of a business process that can be successfully automated.
In order to quantify the added value of Prescriptive Analytics an estimation of
the gain in decision quality and its impact on revenue is needed with the help
of the predefined metric or &lt;span class="caps"&gt;KPI&lt;/span&gt;. For this complex estimation it is recommended
for traditional companies to have an experienced partner alongside and optionally
a proof of concept to evaluate the predictive power of the data and the business
case as a&amp;nbsp;whole.&lt;/p&gt;
&lt;p&gt;We should not ignore the fact that automation also includes costs encompassing the
maintenance, licence &lt;span class="amp"&gt;&amp;amp;&lt;/span&gt; support etc. of an established automated decision system. The
automation costs depend mainly on the order of magnitude of performed decisions as well
as the time frame. For instance the &lt;span class="caps"&gt;IT&lt;/span&gt; setup of a decision system for one million decisions
per day will be much smaller than the setup for one billion that need to be determined in real-
time. The initial costs for the implementation of an automated decision system varies largely
depending on the domain of application, the necessary changes in the business processes
and other factors. An estimation of these costs is needed to determine the time-to-value.
Since the added value of an automated decision system quite often heavily surpasses
automation costs by at least one order of magnitude time-to-value is often&amp;nbsp;low.&lt;/p&gt;
&lt;p&gt;We conclude that the scaling in the number of decisions and the improved effectiveness of
the decisions are the main drivers of the added value in automated decision making.
Subsequently, operational decisions that are ubiquitous and directly influence the business
value are well suited for a Predictive Analytics light-house&amp;nbsp;project.&lt;/p&gt;
&lt;p&gt;Following mnemonic recaps the main qualities of a successful Prescriptive Analytics project.
It consists of the following questions that should be answered&amp;nbsp;positively:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Are decisions taken &lt;strong&gt;F&lt;/strong&gt;requently?&lt;/li&gt;
&lt;li&gt;Does the business process allow the &lt;strong&gt;A&lt;/strong&gt;utomation of&amp;nbsp;decisions?&lt;/li&gt;
&lt;li&gt;Are &lt;strong&gt;M&lt;/strong&gt;etrics defined to determine the quality of a&amp;nbsp;decision?&lt;/li&gt;
&lt;li&gt;Do decisions have a direct &lt;strong&gt;E&lt;/strong&gt;conomic&amp;nbsp;impact?&lt;/li&gt;
&lt;li&gt;Is enough and suitable &lt;strong&gt;D&lt;/strong&gt;ata available to base decisions&amp;nbsp;on?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Prescriptive Analytics projects with these properties are very likely to become &lt;strong&gt;famed&lt;/strong&gt;
in your company. The successful implementation of a lighthouse project in the business process
generates momentum for new projects. This drives the digital transformation of a classical
business in an iterative&amp;nbsp;way.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;This article was originally posted on the &lt;a href="http://www.21stcenturyit.de/leveraging-the-value-of-big-data-with-automated-decision-making/"&gt;&lt;span class="caps"&gt;CSC&lt;/span&gt; 21st Century &lt;span class="caps"&gt;IT&lt;/span&gt; blog&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;</summary><category term="big data"></category><category term="data science"></category></entry><entry><title>Interactively visualizing distributions in a Jupyter notebook with Bokeh</title><link href="/2016/03/jupyter_distribution_visualizer/" rel="alternate"></link><updated>2016-03-26T09:00:00+01:00</updated><author><name>Florian Wilhelm</name></author><id>tag:,2016-03-26:2016/03/jupyter_distribution_visualizer/</id><summary type="html">&lt;p&gt;If you are doing probabilistic programming you are dealing with all kinds of
different distributions. That means choosing an ensemble of right distributions
which describe the underlying real-world process in a suitable way but also
choosing the right parameters for prior distributions. At that point I often
start visualizing the distributions with the help of &lt;a href="http://jupyter.org/"&gt;Jupyter&lt;/a&gt; notebooks,
&lt;a href="http://matplotlib.org/"&gt;matplotlib&lt;/a&gt; and &lt;a href="http://www.scipy.org/"&gt;SciPy&lt;/a&gt; to get a feeling how the distribution behaves when
changing its parameters. And please don&amp;#8217;t tell me you are able to visualize all the
distributions &lt;a href="http://docs.scipy.org/doc/scipy/reference/stats.html"&gt;scipy.stats&lt;/a&gt; has to offer just in your&amp;nbsp;head.&lt;/p&gt;
&lt;p&gt;For me, this surely is a repetitive task that every good and lazy programmer tries
to avoid. Additionally, I was never quite satisfied with the interactivity of
matplotlib in a notebook. Granted, the &lt;code&gt;%matplotlib notebook&lt;/code&gt; magic was a huge
step into the right direction but there is still much room for improvement.
The new and shiny kid on the block is &lt;a href="http://bokeh.pydata.org/"&gt;Bokeh&lt;/a&gt; and so far I have not really done
much with it, meaning it is a good candidate for a test ride. The same goes
actually for Jupyter&amp;#8217;s &lt;a href="http://ipywidgets.readthedocs.org/"&gt;ipywidgets&lt;/a&gt; and you see where this going. No evaluation
of a tool without a proper goal and that is now set to developing an interactive
visualization widget for Jupyter based on Bokeh and ipywidgets. So here we&amp;nbsp;go!&lt;/p&gt;
&lt;p&gt;It turned out that this task is easier than expected due the good documentation
and examples of ipywidgets and especially Bokeh. You can read all about the
implementation inside this &lt;a href="https://github.com/FlorianWilhelm/distvis/blob/master/index.ipynb"&gt;notebook&lt;/a&gt; which is hosted in a separate
&lt;a href="https://github.com/FlorianWilhelm/distvis"&gt;Github repository&lt;/a&gt;. This also always me to make use of a new service that I
just recently learned about, &lt;a href="http://mybinder.org/"&gt;binder&lt;/a&gt;. This totally rad service takes any
Github repository with a Jupyter notebook in it, fires up a container with Kubernetes,
installs necessary requirements and finally runs your notebook! By just clicking
on a link! Amazing to see how the ecosystem around Jupyter develops these&amp;nbsp;days.&lt;/p&gt;
&lt;p&gt;And of course to wet your appetite, here are the screenshots of the final tool
that you will experience interactively by &lt;a href="http://mybinder.org/repo/FlorianWilhelm/distvis"&gt;starting the notebook with binder&lt;/a&gt;.&lt;/p&gt;
&lt;figure&gt;
&lt;img class="noZoom" src="/images/alpha_dist.png" alt="Alpha distribution"&gt;
&lt;figcaption&gt;The probability density function of a continuous alpha distribution with shape parameter a=1.3&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;
&lt;figure&gt;
&lt;img class="noZoom" src="/images/binom_dist.png" alt="Binomial distribution"&gt;
&lt;figcaption&gt;The probability mass function of a discrete binomial distribution with shape parameters n=10 and p=0.7&lt;/figcaption&gt;
&lt;/figure&gt;</summary><category term="jupyter"></category><category term="python"></category><category term="scipy"></category><category term="bokeh"></category></entry><entry><title>Explaining the Idea behind ARD and Bayesian Interpolation</title><link href="/2016/03/explaining_the_idea_behind_ard/" rel="alternate"></link><updated>2016-03-13T22:00:00+01:00</updated><author><name>Florian Wilhelm</name></author><id>tag:,2016-03-13:2016/03/explaining_the_idea_behind_ard/</id><summary type="html">&lt;p&gt;This talk presented at the &lt;a href="http://pydata.org/amsterdam2016/schedule/presentation/17/"&gt;PyData Amsterdam 2016&lt;/a&gt; explains the idea of Bayesian
model selection techniques, especially the Automatic Relevance Determination.
The slides of this talk are available on &lt;a href="http://www.slideshare.net/FlorianWilhelm2/explaining-the-idea-behind-automatic-relevance-determination-and-bayesian-interpolation-59498957"&gt;SlideShare&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Even in the era of Big Data there are many real-world problems where the number
of input features has about the some order of magnitude than the number of samples.
Often many of those input features are irrelevant and thus inferring the relevant
ones is an important problem in order to prevent over-fitting. Automatic Relevance
Determination solves this problem by applying Bayesian&amp;nbsp;techniques.&lt;/p&gt;
&lt;p&gt;In order to motivate Automatic Relevance Determination (&lt;span class="caps"&gt;ARD&lt;/span&gt;) an intuition for
the problem of choosing a complex model that fits the data well vs a simple model
that generalizes well is established. Thereby the idea behind Occam&amp;#8217;s razor is
presented as a way of balancing bias and variance. This leads us to the mathematical
framework of Bayesian interpolation and model selection to choose between different
models based on the&amp;nbsp;data.&lt;/p&gt;
&lt;p&gt;To derive &lt;span class="caps"&gt;ARD&lt;/span&gt; as gently as possible the mathematical basics of a simple linear model
are repeated as well as the idea of regularization to prevent over-fitting.
Based on that, the Bayesian Ridge Regression (BayesianRidge in Scikit-Learn) is
introduced. Generalizing the concept of Bayesian Ridge Regression even more gets
us eventually to the the idea behind &lt;span class="caps"&gt;ARD&lt;/span&gt; (ARDRegression in&amp;nbsp;Scikit-Learn).&lt;/p&gt;
&lt;p&gt;With the help of a practical example, we consolidate what has been learned so far
and compare &lt;span class="caps"&gt;ARD&lt;/span&gt; to an ordinary least square model. Now we dive deep into the
mathematics of &lt;span class="caps"&gt;ARD&lt;/span&gt; and present the algorithm that solves the minimization problem
of &lt;span class="caps"&gt;ARD&lt;/span&gt;. Finally, some details of Scikit-Learn&amp;#8217;s &lt;span class="caps"&gt;ARD&lt;/span&gt; implementation are&amp;nbsp;discussed.&lt;/p&gt;</summary><category term="scikit-learn"></category><category term="machine-learning"></category><category term="bayesian"></category></entry><entry><title>Introduction to the Python Data Science Stack</title><link href="/2016/02/introduction_to_the_python_data_science_stack/" rel="alternate"></link><updated>2016-02-13T12:30:00+01:00</updated><author><name>Florian Wilhelm</name></author><id>tag:,2016-02-13:2016/02/introduction_to_the_python_data_science_stack/</id><summary type="html">&lt;p&gt;Often I get the question as a Data Scientist what the &lt;em&gt;Python Data Science Stack&lt;/em&gt;
actually is and where a beginner should start to learn. The Python ecosystem,
especially around the topics such as data analytics, data mining, data science and
machine learning is so vast and rich that it confuses many&amp;nbsp;rookies.&lt;/p&gt;
&lt;p&gt;For such an audience I created a slide deck that starts with pointing out the
benefits of the Python language for analytics. Even beginners in Python are
addressed by some slides that explain the syntax of Python and how to get
started. After that some slides present the most important packages of the data
science stack, namely &lt;a href="http://www.numpy.org/"&gt;NumPy&lt;/a&gt;, &lt;a href="http://www.scipy.org/"&gt;SciPy&lt;/a&gt;, &lt;a href="http://pandas.pydata.org/"&gt;Pandas&lt;/a&gt;, &lt;a href="http://scikit-learn.org/"&gt;Scikit-Learn&lt;/a&gt;,
&lt;a href="http://jupyter.org/"&gt;Jupyter&lt;/a&gt; and &lt;a href="http://ipython.org/"&gt;IPython&lt;/a&gt;. The merits of Jupyter are best shown in a live
demonstration to convey its power. The interplay between Pandas and
Scikit-Learn is shown based on Kaggle&amp;#8217;s &lt;a href="https://www.kaggle.com/c/titanic"&gt;Titanic: Machine Learning from Disaster&lt;/a&gt;
dataset. Eventually, an outlook to further libraries in the data science domain
are&amp;nbsp;presented.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href="http://nbviewer.jupyter.org/format/slides/github/FlorianWilhelm/python_data-science_stack/blob/master/notebook.ipynb#/"&gt;View in fullscreen&lt;/a&gt;&lt;/strong&gt;
&lt;iframe src="http://nbviewer.jupyter.org/format/slides/github/FlorianWilhelm/python_data-science_stack/blob/master/notebook.ipynb#/"
style="width: 100%; height: 600px" scrolling="no" marginwidth="0" marginheight="0" frameborder="0" vspace="0" hspace="0"&gt;
&lt;/iframe&gt;&lt;/p&gt;</summary><category term="python"></category><category term="data science"></category><category term="jupyter"></category></entry><entry><title>How to write a friendly reminder bot</title><link href="/2015/07/howto_write_a_friendly_reminder_bot/" rel="alternate"></link><updated>2015-12-22T19:30:00+01:00</updated><author><name>Florian Wilhelm</name></author><id>tag:,2015-07-24:2015/07/howto_write_a_friendly_reminder_bot/</id><summary type="html">&lt;p&gt;In this presentation given at the &lt;a href="https://ep2015.europython.eu/"&gt;EuroPython 2015&lt;/a&gt; in Bilbao,
I show how the &lt;a href="https://github.com/tdryer/hangups"&gt;hangups&lt;/a&gt; library can be used
in order to write a small chatbot that connects to Google Hangouts
and reminds you or someone else to take his/her medication.
The secure and recommended OAuth2 protocol is used to authorize the bot application
in the Google Developers Console in order to access the Google+ Hangouts &lt;span class="caps"&gt;API&lt;/span&gt;.
Subsequently, I explain how to use an event-driven library to write a bot
that sends scheduled messages, waits for a proper reply and repeats the question if need be.
Thereby, a primer on event-driven, asynchronous architectures is&amp;nbsp;given.&lt;/p&gt;
&lt;p&gt;The source code can be downloaded on &lt;a href="https://github.com/blue-yonder/medbot"&gt;GitHub&lt;/a&gt;
and the slides are available as &lt;a href="http://htmlpreview.github.io/?https://github.com/blue-yonder/medbot/blob/master/medbot.slides.html?theme=solarized#/"&gt;html preview&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;div class="videobox"&gt;
                &lt;iframe width="800" height="500"
                    src='https://www.youtube.com/embed/ztfdv9jcxtw'
                    frameborder='0' webkitAllowFullScreen mozallowfullscreen
                    allowFullScreen&gt;
                &lt;/iframe&gt;
            &lt;/div&gt;&lt;/p&gt;</summary><category term="python"></category><category term="google hangouts"></category><category term="asyncio"></category><category term="event-driven"></category><category term="asynchronous"></category></entry></feed>