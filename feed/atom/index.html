<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Florian Wilhelm's blog</title><link href="https://florianwilhelm.info/" rel="alternate"></link><link href="https://florianwilhelm.info/feed/atom/index.html" rel="self"></link><id>https://florianwilhelm.info/</id><updated>2022-01-27T10:00:00+01:00</updated><entry><title>Effective and Consistent Configuration via YAML &amp; CLI with Hydra</title><link href="https://florianwilhelm.info/2022/01/configuration_via_yaml_and_cli_with_hydra/" rel="alternate"></link><published>2022-01-27T10:00:00+01:00</published><updated>2022-01-27T10:00:00+01:00</updated><author><name>Florian Wilhelm</name></author><id>tag:florianwilhelm.info,2022-01-27:/2022/01/configuration_via_yaml_and_cli_with_hydra/</id><summary type="html">&lt;p&gt;Hydra allows you to have consistent configuration of Python applications via a command-line interface and &lt;span class="caps"&gt;YAML&lt;/span&gt;&amp;nbsp;files.&lt;/p&gt;</summary><content type="html">&lt;p&gt;A frequent requirement for productive Python application is that they are configurable via configuration files and/or
the command-line-interface (&lt;span class="caps"&gt;CLI&lt;/span&gt;). This allows you to change the behavior of your application without touching the source code, e.g. configuring
another database &lt;span class="caps"&gt;URL&lt;/span&gt; or the logging verbosity. For the &lt;span class="caps"&gt;CLI&lt;/span&gt;-part, &lt;a href="https://docs.python.org/3/library/argparse.html"&gt;argparse&lt;/a&gt; or &lt;a href="https://click.palletsprojects.com/"&gt;Click&lt;/a&gt; is often used and with &lt;a href="https://pyyaml.org/"&gt;PyYAML&lt;/a&gt; configuration files
can be easily read, so where is the&amp;nbsp;problem?&lt;/p&gt;
&lt;p&gt;The &lt;span class="caps"&gt;CLI&lt;/span&gt; and configuration file of a Python application have many things in common, i.e.,&amp;nbsp;both&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;configure the runtime behaviour of your&amp;nbsp;application,&lt;/li&gt;
&lt;li&gt;need to implement validations, e.g. is the port an integer above&amp;nbsp;1024,&lt;/li&gt;
&lt;li&gt;need to be consistent and mergeable, i.e. a &lt;span class="caps"&gt;CLI&lt;/span&gt; flag should be named like the &lt;span class="caps"&gt;YAML&lt;/span&gt; key and if both are passed the &lt;span class="caps"&gt;CLI&lt;/span&gt;
   overwrites the &lt;span class="caps"&gt;YAML&lt;/span&gt;&amp;nbsp;configuration.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Thus implementing configuration by a &lt;span class="caps"&gt;CLI&lt;/span&gt; and a &lt;span class="caps"&gt;YAML&lt;/span&gt; file separately, leads often to code duplication
and inconsistent behavior, not to mention the enormous amount of work that must be done to get this&amp;nbsp;right.&lt;/p&gt;
&lt;p&gt;With this in mind, Facebook implemented the &lt;a href="https://hydra.cc/"&gt;Hydra&lt;/a&gt; library, which allows you to do hierarchical configuration by
composition and override it through config files and the command-line. In this blog post, we demonstrate in an example project
the most important features of &lt;a href="https://hydra.cc/"&gt;Hydra&lt;/a&gt; and how it can be used in conjunction with &lt;a href="https://pydantic-docs.helpmanual.io/"&gt;pydantic&lt;/a&gt;,
which extends the validation capabilities of &lt;a href="https://omegaconf.readthedocs.io/"&gt;OmegaConf&lt;/a&gt; that is used internally by Hydra. To follow along, check out
this &lt;a href="https://github.com/FlorianWilhelm/hydra-example-project"&gt;repository&lt;/a&gt; that serves as a demonstration, but also as a playground for&amp;nbsp;you.&lt;/p&gt;
&lt;h3&gt;Ok, so give me the gist of how Hydra&amp;nbsp;works&lt;/h3&gt;
&lt;p&gt;Sure, just take a look into &lt;a href="https://github.com/FlorianWilhelm/hydra-example-project/blob/master/src/my_pkg/cli.py"&gt;cli.py&lt;/a&gt; and &lt;a href="https://github.com/FlorianWilhelm/hydra-example-project/blob/master/src/my_pkg/config.py"&gt;config.py&lt;/a&gt; first as these are the only files we added,
roughly 70 lines of code. The hierarchical configuration can be found in the &lt;a href="https://github.com/FlorianWilhelm/hydra-example-project/tree/master/configs"&gt;configs&lt;/a&gt;&amp;nbsp;folder.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;├── configs
│   ├── main.yaml             &amp;lt;- entry point for configuration
│   ├── db                    &amp;lt;- database configuration group
│   │   ├── mysql.yaml        &amp;lt;- configuration of MySQL
│   │   └── postgresql.yaml   &amp;lt;- configuration for PostgreSQL
│   └── experiment            &amp;lt;- experiment configuration group
│       ├── exp1.yaml         &amp;lt;- configuration for experiment 1
│       ├── exp2.yaml         &amp;lt;- configuration for experiment 2
│       ├── missing_key.yaml  &amp;lt;- wrong configuration with missing key
│       └── wrong_type.yaml   &amp;lt;- wrong configuration with wrong type
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Basically, this structure allows you to mix and match your configuration by choosing for instance the MySQL database
with the setup of experiment 2. Each group then corresponds to an attribute in the configuration object, which
includes other attributes, just think of a dictionary where some keys are again&amp;nbsp;dictionaries.&lt;/p&gt;
&lt;p&gt;In our [example project with Hydra], we defined the &lt;span class="caps"&gt;CLI&lt;/span&gt; command &lt;code&gt;hydra-test&lt;/code&gt; by changing in &lt;code&gt;setup.cfg&lt;/code&gt; the following&amp;nbsp;lines:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;# Add here console scripts like:&lt;/span&gt;
&lt;span class="na"&gt;console_scripts&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt;
     &lt;span class="na"&gt;hydra-test&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;my_pkg.cli:main&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;We can thus invoke our application with the console command &lt;code&gt;hydra-test&lt;/code&gt; and this will execute the &lt;code&gt;main&lt;/code&gt; function in &lt;code&gt;cli.py&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nd"&gt;@hydra&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;main&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;config_path&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;config_name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;main&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;main&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cfg&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;Config&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="c1"&gt;# this line actually runs the checks of pydantic&lt;/span&gt;
    &lt;span class="n"&gt;OmegaConf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;to_object&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cfg&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="c1"&gt;# log to console and into the `outputs` folder per default&lt;/span&gt;
    &lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;info&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;OmegaConf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;to_yaml&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cfg&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="c1"&gt;# note that IDEs allow auto-complete for accessing the attributes!&lt;/span&gt;
    &lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sleep&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cfg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;main&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sleep&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Looking at the actual code, we see that we only trigger some &lt;a href="https://pydantic-docs.helpmanual.io/"&gt;pydantic&lt;/a&gt; checks to see if configuration and &lt;span class="caps"&gt;CLI&lt;/span&gt; parameters are alright,
then logging the current configuration and sleeping for some predefined time by our&amp;nbsp;configuration.&lt;/p&gt;
&lt;p&gt;So executing just &lt;code&gt;hydra-test&lt;/code&gt; results&amp;nbsp;in:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;Cannot find primary config &amp;#39;main&amp;#39;. Check that it&amp;#39;s in your config search path.

Config search path:
    provider=hydra, path=pkg://hydra.conf
    provider=main, path=pkg://my_pkg
    provider=schema, path=structured://

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This is due to the fact that we set &lt;code&gt;config_path=None&lt;/code&gt;, which is desirable for productive application. The application
itself doesn&amp;#8217;t know where it is gonna be installed and thus defining a path to the configuration files doesn&amp;#8217;t make sense.
For this reason we pass the configuration at execution with &lt;code&gt;-cd&lt;/code&gt;, short for &lt;code&gt;--config-dir&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;hydra-test -cd configs
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This results in the&amp;nbsp;error:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;Error executing job with overrides: []
Traceback (most recent call last):
  File &amp;quot;.../hydra-example-project/src/my_pkg/cli.py&amp;quot;, line 11, in main
    OmegaConf.to_object(cfg)
omegaconf.errors.MissingMandatoryValue: Structured config of type `Config` has missing mandatory value: experiment
    full_key: experiment
    object_type=Config

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This is exactly as we want it, since by taking a look into &lt;a href="https://github.com/FlorianWilhelm/hydra-example-project/blob/master/src/my_pkg/config.py"&gt;config.py&lt;/a&gt;, we see that the schema of the main configuration&amp;nbsp;is:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nd"&gt;@dataclass&lt;/span&gt;
&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Config&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;main&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;Main&lt;/span&gt;
    &lt;span class="n"&gt;db&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;DataBase&lt;/span&gt;
    &lt;span class="n"&gt;neptune&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;Neptune&lt;/span&gt;
    &lt;span class="n"&gt;experiment&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;Experiment&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;MISSING&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Therefore, experiment is a mandatory parameter that the &lt;span class="caps"&gt;CLI&lt;/span&gt; user needs to provide. Consequently, we add &lt;code&gt;+experiment=exp1&lt;/code&gt; to select the
configuration from &lt;code&gt;exp1.yaml&lt;/code&gt; and finally get what we would&amp;nbsp;expect:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;❯ hydra-test -cd configs +experiment=exp1
[2022-01-27 08:14:34,257][my_pkg.cli][INFO] -
main:
  sleep: 3
neptune:
  project: florian.wilhelm/my_expriments
  api_token: ~/.neptune_api_token
  tags:
  - run-1
  description: Experiment run on GCP
  mode: async
db:
  driver: mysql
  host: server_string
  port: 1028
  username: myself
  password: secret
experiment:
  model: XGBoost
  l2: 0.01
  n_steps: 1000
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Note that we had to use a plus sign in the flag &lt;code&gt;+experiment&lt;/code&gt; as we &lt;em&gt;added&lt;/em&gt; the mandatory experiment parameter. Conveniently, 
Hydra has also set up the logging for us and besides logging to the terminal, all output will also be collected in the &lt;code&gt;./outputs&lt;/code&gt;
folder.&lt;/p&gt;
&lt;p&gt;So the section &lt;code&gt;main&lt;/code&gt; and &lt;code&gt;neptune&lt;/code&gt; are directly defined in &lt;code&gt;main.yaml&lt;/code&gt;, but why did Hydra now choose the MySQL database?
This is due to fact that in &lt;code&gt;main.yaml&lt;/code&gt;, we defined some&amp;nbsp;defaults:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;# hydra section to build up the config hierarchy with defaults&lt;/span&gt;
&lt;span class="nt"&gt;defaults&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
  &lt;span class="p p-Indicator"&gt;-&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;_self_&lt;/span&gt;
  &lt;span class="p p-Indicator"&gt;-&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;base_config&lt;/span&gt;
  &lt;span class="p p-Indicator"&gt;-&lt;/span&gt; &lt;span class="nt"&gt;db&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;mysql.yaml&lt;/span&gt;
  &lt;span class="c1"&gt;# experiment: is not mentioned here but in config.py to have a mandatory setting&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;We can override this default behavior by adding &lt;code&gt;db=postgresql&lt;/code&gt; and this time without &lt;code&gt;+&lt;/code&gt; as we override a&amp;nbsp;default:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;❯ hydra-test -cd configs +experiment=exp1 db=postgresql
Error executing job with overrides: [&amp;#39;+experiment=exp1&amp;#39;, &amp;#39;db=postgresql&amp;#39;]
Traceback (most recent call last):
  File &amp;quot;.../hydra-example-project/src/my_pkg/cli.py&amp;quot;, line 11, in main
    OmegaConf.to_object(cfg)
pydantic.error_wrappers.ValidationError: 1 validation error for DataBase
port
  Choose a non-privileged port! (type=value_error)

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Nice this worked as expected by telling us that our port configuration is actually wrong as we chose a privileged port!
This is the magic of &lt;a href="https://pydantic-docs.helpmanual.io/"&gt;pydantic&lt;/a&gt; doing its validation work. Taking a look into &lt;code&gt;config.py&lt;/code&gt;, we see the check that assures
a port greater than&amp;nbsp;1023.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nd"&gt;@dataclass&lt;/span&gt;
&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;DataBase&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;driver&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt;
    &lt;span class="n"&gt;host&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt;

    &lt;span class="nd"&gt;@validator&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;port&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;validate_some_var&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;cls&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;port&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;port&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="mi"&gt;1024&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="k"&gt;raise&lt;/span&gt; &lt;span class="ne"&gt;ValueError&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Choose a non-privileged port!&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;port&lt;/span&gt;

    &lt;span class="n"&gt;port&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;
    &lt;span class="n"&gt;username&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt;
    &lt;span class="n"&gt;password&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Good, so we could fix our configuration file or pass an extra parameter if we are in a hurry,&amp;nbsp;i.e.:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;❯ hydra-test -cd configs +experiment=exp1 db=postgresql db.port=1832
[2022-01-27 08:13:52,148][my_pkg.cli][INFO] -
main:
  sleep: 3
neptune:
  project: florian.wilhelm/my_expriments
  api_token: ~/.neptune_api_token
  tags:
  - run-1
  description: Experiment run on GCP
  mode: async
db:
  driver: postgreqsql
  host: server_string
  port: 1832
  username: me
  password: birthday
experiment:
  model: XGBoost
  l2: 0.01
  n_steps: 1000
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;And this works! So much flexibility and robustness in just 70 lines of code, awesome! While you are at it, you can also
run &lt;code&gt;hydra-test -cd configs +experiment=missing_key&lt;/code&gt; and &lt;code&gt;hydra-test -cd configs +experiment=wrong_type&lt;/code&gt; to see some
nice errors from pydantic telling you about a missing key and wrong type of the configuration value, respectively.
By the way, also passing the port parameter wrong, e.g. with &lt;code&gt;db.port=72&lt;/code&gt;, would have triggered the same exception, so 
&lt;span class="caps"&gt;CLI&lt;/span&gt; and the &lt;span class="caps"&gt;YAML&lt;/span&gt; configurations share the same checks and validations. &lt;a href="https://hydra.cc/"&gt;Hydra&lt;/a&gt; and &lt;a href="https://pydantic-docs.helpmanual.io/"&gt;pydantic&lt;/a&gt; work nicely together here
to make this possible. Just remember to use the &lt;code&gt;dataclass&lt;/code&gt; from &lt;a href="https://pydantic-docs.helpmanual.io/"&gt;pydantic&lt;/a&gt;, not the standard library
and call &lt;code&gt;OmegaConf.to_object(cfg)&lt;/code&gt; at the start of your application to fail as early as&amp;nbsp;possible.&lt;/p&gt;
&lt;p&gt;Hydra has many more, really nice features. Imagine you want to run now the experiments &lt;code&gt;exp1&lt;/code&gt; and &lt;code&gt;exp2&lt;/code&gt; consecutively,
you can just use the &lt;code&gt;--multirun&lt;/code&gt; feature, or &lt;code&gt;-m&lt;/code&gt; for&amp;nbsp;short:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;hydra-test -m -cd configs +experiment&lt;span class="o"&gt;=&lt;/span&gt;exp1,exp2
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Or in case you have hundreds of experiments, you can also use globbing&amp;nbsp;like:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;hydra-test -m -cd configs &lt;span class="s2"&gt;&amp;quot;+experiment=glob(exp*)&amp;quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;There&amp;#8217;s so much more to Hydra and several plugins even for hyperparameter optimization exist. Also note that with the
flag &lt;code&gt;--hydra-help&lt;/code&gt;, you can see the hydra-specific parameters of your application. So now go and check it&amp;nbsp;out!&lt;/p&gt;</content><category term="post"></category><category term="python"></category><category term="configuration"></category><category term="production"></category></entry><entry><title>Matrix Factorization for Collaborative Filtering Is Just Solving an Adjoint Latent Dirichlet Allocation Model After All</title><link href="https://florianwilhelm.info/2021/09/mf_for_collaborative_filtering_is_just_solving_an_adjoint_lda_model/" rel="alternate"></link><published>2021-09-24T17:00:00+02:00</published><updated>2021-09-24T17:00:00+02:00</updated><author><name>Florian Wilhelm</name></author><id>tag:florianwilhelm.info,2021-09-24:/2021/09/mf_for_collaborative_filtering_is_just_solving_an_adjoint_lda_model/</id><summary type="html">&lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/Matrix_factorization_(recommender_systems)"&gt;Matrix factorization-based methods&lt;/a&gt; are among the most popular methods for collaborative filtering tasks with &lt;a href="https://en.wikipedia.org/wiki/Relevance_feedback"&gt;implicit feedback&lt;/a&gt;. 
The most effective of these methods do not apply sign constraints, such as non-negativity, to their factors. 
Despite their simplicity, the latent factors for users and items lack interpretability, which is becoming an increasingly …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/Matrix_factorization_(recommender_systems)"&gt;Matrix factorization-based methods&lt;/a&gt; are among the most popular methods for collaborative filtering tasks with &lt;a href="https://en.wikipedia.org/wiki/Relevance_feedback"&gt;implicit feedback&lt;/a&gt;. 
The most effective of these methods do not apply sign constraints, such as non-negativity, to their factors. 
Despite their simplicity, the latent factors for users and items lack interpretability, which is becoming an increasingly important requirement. 
In this work, we provide a theoretical link between unconstrained and the interpretable &lt;a href="https://en.wikipedia.org/wiki/Non-negative_matrix_factorization"&gt;non-negative matrix factorization&lt;/a&gt; in terms of the personalized ranking induced by these methods. 
We also introduce a novel, &lt;a href="https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation"&gt;latent Dirichlet allocation&lt;/a&gt;-inspired model for recommenders and extend our theoretical link to 
also allow the interpretation of an unconstrained matrix factorization as an adjoint formulation of our new model. 
Our experiments indicate that this novel approach represents the unknown processes of implicit user-item interactions in 
the real world much better than unconstrained matrix factorization while being&amp;nbsp;interpretable.&lt;/p&gt;
&lt;p&gt;This paper was presented at the 15th &lt;span class="caps"&gt;ACM&lt;/span&gt; Conference on Recommender Systems (&lt;a href="https://recsys.acm.org/recsys21/"&gt;RecSys 2021&lt;/a&gt;) in Amsterdam. A &lt;a href="https://dl.acm.org/doi/fullHtml/10.1145/3460231.3474266#"&gt;video recording
of my presentation&lt;/a&gt; is generously provided by &lt;span class="caps"&gt;ACM&lt;/span&gt; and also the &lt;a href="https://www.slideshare.net/FlorianWilhelm2/matrix-factorization-for-collaborative-filtering-is-just-solving-an-adjoint-latent-dirichlet-allocation-model-after-all"&gt;slides&lt;/a&gt; are&amp;nbsp;available.&lt;/p&gt;</content><category term="talk"></category><category term="data science"></category><category term="recommender systems"></category></entry><entry><title>Handling Anaconda without getting Constricted</title><link href="https://florianwilhelm.info/2021/09/Handling_Anaconda_without_getting_constricted/" rel="alternate"></link><published>2021-09-01T08:00:00+02:00</published><updated>2021-09-01T08:00:00+02:00</updated><author><name>Florian Wilhelm</name></author><id>tag:florianwilhelm.info,2021-09-01:/2021/09/Handling_Anaconda_without_getting_constricted/</id><summary type="html">&lt;p&gt;Miniforge is a Miniconda alternative that allows you to engage in commercial activities without an Anaconda&amp;nbsp;licence.&lt;/p&gt;</summary><content type="html">&lt;h2&gt;Commercial Activities using the Anaconda&amp;nbsp;Repository&lt;/h2&gt;
&lt;p&gt;In April 2020, &lt;a href="https://www.anaconda.com/"&gt;Anaconda Inc.&lt;/a&gt;, the company behind our beloved Anaconda distribution, started to change their 
&lt;a href="https://www.anaconda.com/terms-of-service"&gt;Terms of Service&lt;/a&gt; (ToS) in a way that most commercial usage is no longer for free. Peter Wang explained the reasons for
this change in a &lt;a href="https://www.anaconda.com/blog/sustaining-our-stewardship-of-the-open-source-data-science-community"&gt;blog post&lt;/a&gt;. There was a bit of turmoil in the Anaconda community, also about the interpretation of the 
ToS at that time. In October 2020, the ToS was updated and some clarifications were given in an official &lt;a href="https://www.anaconda.com/blog/anaconda-commercial-edition-faq"&gt;&lt;span class="caps"&gt;FAQ&lt;/span&gt;&lt;/a&gt; by Stephen&amp;nbsp;Nolan. &lt;/p&gt;
&lt;p&gt;I am no lawyer, so take my words with a grain of salt. My take-way from the ToS is that whenever you are using the Anaconda
repository in commercial activities as a company, you must purchase a licence. This is fair enough since the employees at 
Anaconda Inc. do a hell of a great job, and they need to make a living of course. Thus, if you use Anaconda, and it helps you
as an enterprise, why not just support it? Just do it and stop&amp;nbsp;reading. &lt;/p&gt;
&lt;p&gt;That being said, there might be cases, let&amp;#8217;s say you are working as a freelancer for another company and want to use 
&lt;a href="https://docs.conda.io/en/latest/"&gt;conda&lt;/a&gt;, the package manager of Anaconda, as part of your usual tool chain. Surely, you are not gonna ask the company to
go and buy a license, just because you like to use it in your project. So what legal option do you have besides the obvious
one of not using it and resorting to some &lt;a href="https://virtualenv.pypa.io/"&gt;virtualenv&lt;/a&gt;/&lt;a href="https://pip.pypa.io/"&gt;pip&lt;/a&gt;-based&amp;nbsp;approach?&lt;/p&gt;
&lt;h2&gt;Miniconda Alternative for Commercial&amp;nbsp;Activities&lt;/h2&gt;
&lt;p&gt;As a heavy conda user, you surely know &lt;a href="https://conda-forge.org/"&gt;conda-forge&lt;/a&gt;, a community-led collection of recipes, build infrastructure and 
distributions for the conda package manager. In most cases, you would download &lt;span class="amp"&gt;&amp;amp;&lt;/span&gt; install &lt;a href="https://docs.conda.io/en/latest/miniconda.html"&gt;Miniconda&lt;/a&gt;, which uses the Anaconda
repository as &lt;em&gt;defaults&lt;/em&gt; channel, and add conda-forge as an additional channel in case a package is not in the defaults&amp;nbsp;channel.&lt;/p&gt;
&lt;p&gt;What many users of conda-forge don&amp;#8217;t know, is that it encompasses not only additional packages but also almost all packages
of the Anaconda repository (aka the &lt;em&gt;defaults&lt;/em&gt; channel) itself. With this in mind and the fact that conda-forge has no restrictions for commercial activities, you can just remove the &lt;em&gt;defaults&lt;/em&gt;
channel which uses the Anaconda repository (i.e. anaconda.com) from your conda configuration and use only the conda-forge 
repository (i.e. anaconda.org) instead to be legally on the safe side! More information can be found in 
the &lt;a href="https://conda-forge.org/blog/posts/2020-11-20-anaconda-tos/"&gt;conda-forge statement about Anaconda&amp;#8217;s ToS&lt;/a&gt;. &lt;/p&gt;
&lt;p&gt;To make the aforementioned channel configuration even easier, the &lt;a href="https://github.com/conda-forge/miniforge"&gt;Miniforge&lt;/a&gt; project can be used as an drop-in replacement for Miniconda, 
that automatically has set up everything for you. Nice! And while you are about to make a change to your installation scripts
anyway, why not directly try &lt;a href="https://github.com/conda-forge/miniforge#mambaforge"&gt;Mambaforge&lt;/a&gt; that comes with &lt;a href="https://github.com/mamba-org/mamba"&gt;mamba&lt;/a&gt;, a faster alternative to &lt;a href="https://docs.conda.io/en/latest/"&gt;conda&lt;/a&gt;, by&amp;nbsp;default.&lt;/p&gt;
&lt;p&gt;To sum this up. It&amp;#8217;s really not that hard to legally use the power of the conda package manager in commercial activities for&amp;nbsp;free.&lt;/p&gt;</content><category term="post"></category><category term="production"></category><category term="Python"></category></entry><entry><title>Using Google BigQuery with Programmatic SQL</title><link href="https://florianwilhelm.info/2021/08/using_bigquery_with_programmatic_sql/" rel="alternate"></link><published>2021-08-08T08:00:00+02:00</published><updated>2021-08-08T08:00:00+02:00</updated><author><name>Florian Wilhelm</name></author><id>tag:florianwilhelm.info,2021-08-08:/2021/08/using_bigquery_with_programmatic_sql/</id><summary type="html">&lt;p&gt;An often encountered Antipattern in Python code are complex queries composed with the help of string substitutions and concatenations. SQLAlchemy Core allows you to generate queries in a programmatic way for many &lt;span class="caps"&gt;SQL&lt;/span&gt; databases like&amp;nbsp;BigQuery.&lt;/p&gt;</summary><content type="html">&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Especially in data science projects, Python code is often peppered with functions generating &lt;span class="caps"&gt;SQL&lt;/span&gt; queries as strings&amp;nbsp;like:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;get_sku_ts&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;entity_id&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sku&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;Union&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
&lt;span class="sd"&gt;    Compose query to retrieve timeseries for SKU or list of SKUs&lt;/span&gt;
&lt;span class="sd"&gt;    &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;query&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
&lt;span class="s2"&gt;    SELECT *&lt;/span&gt;
&lt;span class="s2"&gt;    FROM `sku-table`&lt;/span&gt;
&lt;span class="s2"&gt;    WHERE entity_id = &lt;/span&gt;&lt;span class="se"&gt;\&amp;quot;&lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;entity_id&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="se"&gt;\&amp;quot;&lt;/span&gt;&lt;span class="s2"&gt;&lt;/span&gt;
&lt;span class="s2"&gt;    &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="nb"&gt;isinstance&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sku&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;query&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot; AND sku = &lt;/span&gt;&lt;span class="se"&gt;\&amp;quot;&lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;sku&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="se"&gt;\&amp;quot;&lt;/span&gt;&lt;span class="s2"&gt; &amp;quot;&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="nb"&gt;isinstance&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sku&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;list_str&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;, &amp;quot;&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;join&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="se"&gt;\&amp;quot;&lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="se"&gt;\&amp;quot;&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;sku&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
        &lt;span class="n"&gt;query&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot; AND sku IN (&lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;list_str&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;) &amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;query&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
&lt;span class="s2"&gt;    ORDER BY date ASC&lt;/span&gt;
&lt;span class="s2"&gt;    &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;query&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;And this is even only a simple example. Using dynamic joins and sub-queries, things can get even more complicated.
When the Python on-board resources are no longer sufficient, some developers start using templating engines like &lt;a href="https://jinja.palletsprojects.com/"&gt;Jinja&lt;/a&gt;
to handle the complexity of generating a query string&amp;nbsp;dynamically. &lt;/p&gt;
&lt;p&gt;But is string generation really the right way to solve a problem like that? It&amp;#8217;s definitely easy as most developers in
the field of data science know their &lt;span class="caps"&gt;SQL&lt;/span&gt; and dealing with strings, so it&amp;#8217;s a start with low entry hurdle. Let&amp;#8217;s shed some
light on the downsides of this approach. The first thing to note is that we have a language break as we use &lt;span class="caps"&gt;SQL&lt;/span&gt; and Python
side-by-side in the same program. This causes us some trouble as we never know if the queries we generate are even 
syntactically valid. Only at execution time we will see if the &lt;span class="caps"&gt;SQL&lt;/span&gt; parsers swallows what we generated which makes unit testing
such functions quite hard. Without an &lt;span class="caps"&gt;SQL&lt;/span&gt; parser, we can only test in the function above if different kinds of parameters lead
to some string, which is trivial. Another impact of the language break is that structuring and decoupling the code becomes harder.
Imagine you want to write one method that takes another query and adds time-based slicing by specifying two dates. If
the query is a string this will be pretty hard as you need to add some &lt;code&gt;DATE(timestamp) BETWEEN FROM_DATE AND TO_DATE&lt;/code&gt;-clause
and there might be already some other where-clauses specified. A last downside is that specifying &lt;span class="caps"&gt;SQL&lt;/span&gt; queries like
this is quite prone to &lt;a href="https://en.wikipedia.org/wiki/SQL_injection"&gt;&lt;span class="caps"&gt;SQL&lt;/span&gt; injection attacks&lt;/a&gt;. So if you compose your queries based on user-generated data, e.g. user input
from some e-commerce website, a malicious user might inject sub-queries in a smart way to steal your data. In many data science
related projects this might be a rather academic vector of attack, but especially in learning-to-rank use-cases it might not
be that&amp;nbsp;unrealistic.&lt;/p&gt;
&lt;p&gt;To summarize these&amp;nbsp;downsides:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;language gap between Python and &lt;span class="caps"&gt;SQL&lt;/span&gt;, i.e. no &lt;span class="caps"&gt;IDE&lt;/span&gt;-support to find errors before&amp;nbsp;execution, &lt;/li&gt;
&lt;li&gt;hard to write clean, decoupled, well-structured code and meaningful unit&amp;nbsp;tests,&lt;/li&gt;
&lt;li&gt;the possibility of &lt;span class="caps"&gt;SQL&lt;/span&gt; injection&amp;nbsp;attacks.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Programmatic &lt;span class="caps"&gt;SQL&lt;/span&gt;&lt;/h2&gt;
&lt;p&gt;&lt;span class="dquo"&gt;&amp;#8220;&lt;/span&gt;But &lt;span class="caps"&gt;SQL&lt;/span&gt; is the right tool for this task! So what&amp;#8217;s the solution then?&amp;#8221;, I hear you mutter. Welcome to programmatic &lt;span class="caps"&gt;SQL&lt;/span&gt;!
And it might not even be new to you if you have ever used &lt;a href="https://spark.apache.org/"&gt;Apache Spark&lt;/a&gt;. In Spark you have many frontends besides the &lt;span class="caps"&gt;SQL&lt;/span&gt; &lt;span class="caps"&gt;API&lt;/span&gt;, 
like PySpark for Python and others for Java, Scala, R, etc. Most data scientists naturally use a 
programmatic approach in their programs avoiding the downsides of &lt;span class="caps"&gt;SQL&lt;/span&gt; string generation without even&amp;nbsp;noticing. &lt;/p&gt;
&lt;p&gt;But what if we are dealing not with Spark but another database or warehouse like &lt;a href="https://cloud.google.com/bigquery"&gt;BigQuery&lt;/a&gt;? In this case we can
use &lt;a href="https://www.sqlalchemy.org/"&gt;SQLAlchemy&lt;/a&gt;! SQLAlchemy comes with two abstraction layers, a &lt;em&gt;Core&lt;/em&gt; layer and a high-level &lt;em&gt;&lt;span class="caps"&gt;ORM&lt;/span&gt;&lt;/em&gt; layer. &lt;span class="caps"&gt;ORM&lt;/span&gt; stands for
Object-Relational Mapping and is a common technique to define objects that are automatically mapped to a relational database. 
For most data science projects it&amp;#8217;s not so useful as typical use-cases are analytical and are thus not focused on single instances/objects. 
The underrated Core layer of SQLAlchemy, on the other hand, is really useful as it provides us with a way of generating queries programmatically similar to PySpark.
SQLAlchemy is independent of the actual &lt;span class="caps"&gt;SQL&lt;/span&gt; dialect and by installing a corresponding dialect it can deal with all popular
databases and data warehouses, for instance MySQL, PostgreSQL, BigQuery, etc.
So how does this work? Let&amp;#8217;s go through this with a BigQuery example as it is often used in data science&amp;nbsp;projects.&lt;/p&gt;
&lt;h2&gt;SQLAlchemy with&amp;nbsp;BigQuery&lt;/h2&gt;
&lt;p&gt;Let&amp;#8217;s say we want to find out how many monthly downloads some project on &lt;a href="https://pypi.org/"&gt;PyPI&lt;/a&gt; has. If you want to follow along and
try it out yourself, a Jupyter notebook is available on my &lt;a href="https://github.com/FlorianWilhelm/bigquery-programmatic-sql"&gt;Github repository&lt;/a&gt;. &lt;/p&gt;
&lt;p&gt;With plain &lt;span class="caps"&gt;SQL&lt;/span&gt;, we would solve this task the following&amp;nbsp;way:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;google.cloud&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;bigquery&lt;/span&gt;

&lt;span class="n"&gt;bqclient&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;bigquery&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Client&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="n"&gt;query_string&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
&lt;span class="s2"&gt;SELECT &lt;/span&gt;
&lt;span class="s2"&gt;  DATE_TRUNC(DATE(timestamp), MONTH) AS `month`,&lt;/span&gt;
&lt;span class="s2"&gt;  COUNT(*) AS num_downloads,&lt;/span&gt;
&lt;span class="s2"&gt;FROM `bigquery-public-data.pypi.file_downloads`&lt;/span&gt;
&lt;span class="s2"&gt;WHERE file.project = &amp;#39;pyscaffold&amp;#39;&lt;/span&gt;
&lt;span class="s2"&gt;    AND details.installer.name = &amp;#39;pip&amp;#39;&lt;/span&gt;
&lt;span class="s2"&gt;    AND DATE(timestamp) BETWEEN DATE(&amp;#39;2021-01-01&amp;#39;) AND CURRENT_DATE()&lt;/span&gt;
&lt;span class="s2"&gt;GROUP BY `month`&lt;/span&gt;
&lt;span class="s2"&gt;ORDER BY `month`&lt;/span&gt;
&lt;span class="s2"&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;

&lt;span class="n"&gt;df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;bqclient&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;query&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;query_string&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;result&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;to_dataframe&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;create_bqstorage_client&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Quite easy, but note that to keep this example focused, the query string is static, meaning that in order to dynamically
change the time slicing, the project name etc., we would have to introduce string substitutions at least. This would
result in the aforementioned&amp;nbsp;downsides.&lt;/p&gt;
&lt;p&gt;In contrast to that, after having installed &lt;a href="https://www.sqlalchemy.org/"&gt;SQLAlchemy&lt;/a&gt; and the BigQuery dialect &lt;a href="https://pypi.org/project/pybigquery/"&gt;pybigquery&lt;/a&gt;, we can define a 
dynamic &lt;code&gt;query&lt;/code&gt; object instead of &lt;code&gt;query_string&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sqlalchemy&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sqlalchemy.sql&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;func&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;F&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sqlalchemy.engine&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;create_engine&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sqlalchemy.schema&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;

&lt;span class="n"&gt;engine&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;create_engine&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;bigquery://my_gcp_project&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;table&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Table&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;bigquery-public-data.pypi.file_downloads&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;MetaData&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;bind&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;engine&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;autoload&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;query&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;select&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;F&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;date_trunc&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;F&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;date&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;column&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;timestamp&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt; &lt;span class="n"&gt;text&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;month&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;month&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
            &lt;span class="n"&gt;F&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;count&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;*&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;num_downloads&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)],&lt;/span&gt;
           &lt;span class="n"&gt;from_obj&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;table&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;where&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="n"&gt;and_&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
            &lt;span class="n"&gt;F&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;date&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;column&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;timestamp&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;between&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;2021-01-01&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;F&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;current_date&lt;/span&gt;&lt;span class="p"&gt;()),&lt;/span&gt;
            &lt;span class="n"&gt;column&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;file.project&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;pyscaffold&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;column&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;details.installer.name&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;pip&amp;quot;&lt;/span&gt;
        &lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;group_by&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;month&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;order_by&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;month&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;At first sight, it looks somewhat more verbose and unfamiliar, but you will get used to the SQLAlchemy &lt;span class="caps"&gt;API&lt;/span&gt;, even if 
it doesn&amp;#8217;t look as concise as the one of PySpark, I have to admit. More or less, it&amp;#8217;s just &lt;span class="caps"&gt;SQL&lt;/span&gt; but embedded nicely as functions and objects in&amp;nbsp;Python.&lt;/p&gt;
&lt;p&gt;So far, we have only generated the query and SQLAlchemy would have directly informed us if this was invalid. To see how
the prepared &lt;span class="caps"&gt;SQL&lt;/span&gt; statement looks like, maybe for debugging, we can just &lt;code&gt;print(query)&lt;/code&gt; to&amp;nbsp;get:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;SELECT&lt;/span&gt; &lt;span class="n"&gt;date_trunc&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;date&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;`&lt;/span&gt;&lt;span class="k"&gt;timestamp&lt;/span&gt;&lt;span class="o"&gt;`&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="k"&gt;month&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;AS&lt;/span&gt; &lt;span class="o"&gt;`&lt;/span&gt;&lt;span class="k"&gt;month&lt;/span&gt;&lt;span class="o"&gt;`&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="k"&gt;count&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;count_1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;STRING&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;AS&lt;/span&gt; &lt;span class="o"&gt;`&lt;/span&gt;&lt;span class="n"&gt;num_downloads&lt;/span&gt;&lt;span class="o"&gt;`&lt;/span&gt; 
&lt;span class="k"&gt;FROM&lt;/span&gt; &lt;span class="o"&gt;`&lt;/span&gt;&lt;span class="n"&gt;bigquery&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="k"&gt;public&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="k"&gt;data&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pypi&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;file_downloads&lt;/span&gt;&lt;span class="o"&gt;`&lt;/span&gt; 
&lt;span class="k"&gt;WHERE&lt;/span&gt; &lt;span class="nb"&gt;date&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;`&lt;/span&gt;&lt;span class="k"&gt;timestamp&lt;/span&gt;&lt;span class="o"&gt;`&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;BETWEEN&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;date_1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;STRING&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="k"&gt;AND&lt;/span&gt; &lt;span class="k"&gt;CURRENT_DATE&lt;/span&gt; 
  &lt;span class="k"&gt;AND&lt;/span&gt; &lt;span class="o"&gt;`&lt;/span&gt;&lt;span class="n"&gt;file&lt;/span&gt;&lt;span class="o"&gt;`&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="o"&gt;`&lt;/span&gt;&lt;span class="n"&gt;project&lt;/span&gt;&lt;span class="o"&gt;`&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;file&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;project_1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;STRING&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt; 
  &lt;span class="k"&gt;AND&lt;/span&gt; &lt;span class="o"&gt;`&lt;/span&gt;&lt;span class="n"&gt;details&lt;/span&gt;&lt;span class="o"&gt;`&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="o"&gt;`&lt;/span&gt;&lt;span class="n"&gt;installer&lt;/span&gt;&lt;span class="o"&gt;`&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="o"&gt;`&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;`&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;details&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;installer&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;name_1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;STRING&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt; 
&lt;span class="k"&gt;GROUP&lt;/span&gt; &lt;span class="k"&gt;BY&lt;/span&gt; &lt;span class="o"&gt;`&lt;/span&gt;&lt;span class="k"&gt;month&lt;/span&gt;&lt;span class="o"&gt;`&lt;/span&gt; 
&lt;span class="k"&gt;ORDER&lt;/span&gt; &lt;span class="k"&gt;BY&lt;/span&gt; &lt;span class="o"&gt;`&lt;/span&gt;&lt;span class="k"&gt;month&lt;/span&gt;&lt;span class="o"&gt;`&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;To see the actual query with all literals bound to their final values, we need to compile the query first before&amp;nbsp;printing:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;query&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;compile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;engine&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;compile_kwargs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;literal_binds&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;}))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;to&amp;nbsp;get:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;SELECT&lt;/span&gt; &lt;span class="n"&gt;date_trunc&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;date&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;`&lt;/span&gt;&lt;span class="k"&gt;timestamp&lt;/span&gt;&lt;span class="o"&gt;`&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="k"&gt;month&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;AS&lt;/span&gt; &lt;span class="o"&gt;`&lt;/span&gt;&lt;span class="k"&gt;month&lt;/span&gt;&lt;span class="o"&gt;`&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="k"&gt;count&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;*&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;AS&lt;/span&gt; &lt;span class="o"&gt;`&lt;/span&gt;&lt;span class="n"&gt;num_downloads&lt;/span&gt;&lt;span class="o"&gt;`&lt;/span&gt; 
&lt;span class="k"&gt;FROM&lt;/span&gt; &lt;span class="o"&gt;`&lt;/span&gt;&lt;span class="n"&gt;bigquery&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="k"&gt;public&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="k"&gt;data&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pypi&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;file_downloads&lt;/span&gt;&lt;span class="o"&gt;`&lt;/span&gt; 
&lt;span class="k"&gt;WHERE&lt;/span&gt; &lt;span class="nb"&gt;date&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;`&lt;/span&gt;&lt;span class="k"&gt;timestamp&lt;/span&gt;&lt;span class="o"&gt;`&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;BETWEEN&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;2021-01-01&amp;#39;&lt;/span&gt; &lt;span class="k"&gt;AND&lt;/span&gt; &lt;span class="k"&gt;CURRENT_DATE&lt;/span&gt; 
  &lt;span class="k"&gt;AND&lt;/span&gt; &lt;span class="o"&gt;`&lt;/span&gt;&lt;span class="n"&gt;file&lt;/span&gt;&lt;span class="o"&gt;`&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="o"&gt;`&lt;/span&gt;&lt;span class="n"&gt;project&lt;/span&gt;&lt;span class="o"&gt;`&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;pyscaffold&amp;#39;&lt;/span&gt; 
  &lt;span class="k"&gt;AND&lt;/span&gt; &lt;span class="o"&gt;`&lt;/span&gt;&lt;span class="n"&gt;details&lt;/span&gt;&lt;span class="o"&gt;`&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="o"&gt;`&lt;/span&gt;&lt;span class="n"&gt;installer&lt;/span&gt;&lt;span class="o"&gt;`&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="o"&gt;`&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;`&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;pip&amp;#39;&lt;/span&gt; 
&lt;span class="k"&gt;GROUP&lt;/span&gt; &lt;span class="k"&gt;BY&lt;/span&gt; &lt;span class="o"&gt;`&lt;/span&gt;&lt;span class="k"&gt;month&lt;/span&gt;&lt;span class="o"&gt;`&lt;/span&gt; 
&lt;span class="k"&gt;ORDER&lt;/span&gt; &lt;span class="k"&gt;BY&lt;/span&gt; &lt;span class="o"&gt;`&lt;/span&gt;&lt;span class="k"&gt;month&lt;/span&gt;&lt;span class="o"&gt;`&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;But this is only interesting for debugging and also for learning SQLAlchemy. In order to execute the query object and transform
its result set into a dataframe, all we have to do&amp;nbsp;is:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_sql&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;query&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;engine&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Quite easy, right? &lt;a href="https://pandas.pydata.org/"&gt;Pandas&lt;/a&gt; understands and works nicely with SQLAlchemy query objects and&amp;nbsp;engines.&lt;/p&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;We have seen how &lt;span class="caps"&gt;SQL&lt;/span&gt; string generation in Python programs can be replaced by a programmatic approach and that this
approach has several advantages over the usage of strings. But it also comes with some small downsides. First of all, one
has to learn SQLAlchemy, and its &lt;span class="caps"&gt;API&lt;/span&gt; as well as documentation is surely not as nice as PySpark. So when switching, you
will become slower for a while until you get the hang of SQLAlchemy. The second downside is that another level of abstraction,
naturally comes with less control over the final &lt;span class="caps"&gt;SQL&lt;/span&gt; statement, but this is also true for PySpark and a trade-off every developer
always needs to consider. Having mentioned Spark again, my 5 cents are that the PySpark &lt;span class="caps"&gt;API&lt;/span&gt; is much better than SQLAlchemy 
and if you are using Spark anyway, you should rather use Google&amp;#8217;s &lt;a href="https://github.com/GoogleCloudDataproc/spark-bigquery-connector"&gt;Spark Bigquery Connector&lt;/a&gt;. For rather small-data projects,
where your Python program just uses BigQuery as data storage, using SQLAlchemy might up your Python code quite a bit&amp;nbsp;though.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;&lt;span class="caps"&gt;REMARK&lt;/span&gt;: Spark BigQuery Connector and Pushdowns to&amp;nbsp;BigQuery&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Right now the &lt;a href="https://github.com/GoogleCloudDataproc/spark-bigquery-connector"&gt;Spark Bigquery Connector&lt;/a&gt; has no full pushdown capabilities regarding BigQuery. If &lt;code&gt;pushAllFilters&lt;/code&gt; is set
to true in &lt;code&gt;spark.conf&lt;/code&gt;, which is the default, the connector pushes all the filters Spark can delegate to the BigQuery Storage &lt;span class="caps"&gt;API&lt;/span&gt;. 
Since GroupBy is a more complex operation, the following&amp;nbsp;query:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;spark&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;bigquery&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;load&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;bigquery-public-data.pypi.file_downloads&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;select&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;...&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;groupby&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;...&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;will first fetch all data from the table into Spark. As this might be quite inefficient it&amp;#8217;s better to directly provide an &lt;span class="caps"&gt;SQL&lt;/span&gt;
query that filters and groups direclty in BigQuery&amp;nbsp;like:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;spark&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;bigquery&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;load&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sql_string&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;In this case, the programmatic SQLAlchemy approach as described above, can also be applied to generate &lt;code&gt;sql_string&lt;/code&gt; with the
advantages described&amp;nbsp;above.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Feedback is always welcome! Please let me know if this was helpful and also if you think otherwise in the comments&amp;nbsp;below.&lt;/p&gt;</content><category term="post"></category><category term="python"></category><category term="production"></category><category term="data science"></category></entry><entry><title>Finally! Bayesian Hierarchical Modelling at Scale</title><link href="https://florianwilhelm.info/2020/10/bayesian_hierarchical_modelling_at_scale/" rel="alternate"></link><published>2020-10-01T08:00:00+02:00</published><updated>2020-10-01T08:00:00+02:00</updated><author><name>Florian Wilhelm</name></author><id>tag:florianwilhelm.info,2020-10-01:/2020/10/bayesian_hierarchical_modelling_at_scale/</id><summary type="html">&lt;p&gt;For a long time, Bayesian Hierarchical Modelling has been a very powerful tool that sadly could not be applied often due to its high computations costs. With NumPyro and the latest advances in high-performance computations in Python, Bayesian Hierarchical Modelling is now ready for prime&amp;nbsp;time.&lt;/p&gt;</summary><content type="html">&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Since the advent of deep learning, everything is or has to be about &lt;em&gt;Artificial Intelligence&lt;/em&gt;, so it seems. Even software which is applying traditional
techniques from e.g. instrumentation and control engineering, is nowadays considered &lt;em&gt;&lt;span class="caps"&gt;AI&lt;/span&gt;&lt;/em&gt;. For instance, the famous robots
of Boston Dynamics are not based on deep reinforcement learning as many people think but much more traditional engineering
methods. This hype around &lt;span class="caps"&gt;AI&lt;/span&gt;, which is very often equated with deep learning, seems to draw that much attention such that
great advances of more traditional methods seem to go almost completely unnoticed. In this blog post, I want to draw your 
attention to the somewhat dusty &lt;em&gt;Bayesian Hierarchical Modelling&lt;/em&gt;. Modern techniques and frameworks allow you to finally apply this
cool method on datasets with sizes much bigger than what was possible before and thus letting it really&amp;nbsp;shine.&lt;/p&gt;
&lt;p&gt;So for starters, what is &lt;em&gt;Bayesian Hierarchical Modelling&lt;/em&gt; and why should I care? I assume you already have a basic knowledge about
Bayesian inference, otherwise &lt;a href="https://github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers"&gt;Probabilistic Programming and Bayesian Methods for Hackers&lt;/a&gt;
is a really good starting point to explore the Bayesian rabbit hole. In simple words, Bayesian inference allows you to define
a model with the help of probability distributions and also incorporate your prior knowledge about the parameters of your model.
This leads to a &lt;em&gt;directed acyclic graphical model&lt;/em&gt; (aka Bayesian network) which is explainable, visual and easy to reason about.
But that&amp;#8217;s not even everything, you also get &lt;a href="https://en.wikipedia.org/wiki/Uncertainty_quantification"&gt;Uncertainty Quantification&lt;/a&gt; for free, 
meaning that the model&amp;#8217;s parameters are not mere point estimates but whole distributions telling you how certain you are
about their&amp;nbsp;values. &lt;/p&gt;
&lt;p&gt;A classical statistical method that most data scientists learn about early on is &lt;em&gt;linear regression&lt;/em&gt;. It can also
be interpreted in a Bayesian way giving you the possibility to define prior knowledge about the parameters, e.g.
that they have to be close to zero or that they are non-negative. Then again, many of the priors you might come up with could also 
be seen as mere regularizers in a non-Bayesian way, and treated like that, often efficient techniques exist to solve such formulations.
So where the Bayesian framework now really shines is, if you consider the following problem setting I stole from the wonderful 
presentation &lt;a href="https://www.youtube.com/watch?v=WbNmcvxRwow"&gt;A Bayesian Workflow with PyMC and ArviZ&lt;/a&gt; by Corrie&amp;nbsp;Bartelheimer.&lt;/p&gt;
&lt;p&gt;Imagine you want to estimate the price of an apartment in Berlin given its living area in square meters and district. Making a linear
regression with all data points you have neglecting the districts, i.e. a &lt;em&gt;pooled model&lt;/em&gt;, will lead to a robust estimation of the slope and intercept
but a wide residual distribution. This is due to the fact that the price of an apartment also heavily depends on the district it is
located in. Now grouping your data with respect to the respective districts and making a linear regression for each,
i.e. an &lt;em&gt;unpooled model&lt;/em&gt;, will lead to a much more narrow residual distribution but also a high uncertainty in your parameters since
some district might only have three data points. To combine the advantages of a pooled and unpooled model, one
would intuitively demand that for each district the prior knowledge of the parameter from the pooled model should be used
and updated according to the data we have about a certain district. If we have only a few data points we would only allow to
deviate a bit from our prior knowledge about the parameter. In case we have lots of data points, the parameter for the
respective district should be allowed to have a huge difference compared to the parameter of the pooled model. Thus the pooled model
acts as an informed prior for the parameters within the unpooled model leading altogether to an &lt;em&gt;hierarchical model&lt;/em&gt;,
which is sometimes also referred to as &lt;em&gt;partially pooled model&lt;/em&gt;. Figure 1 illustrates our thoughts so&amp;nbsp;far.&lt;/p&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;
&lt;figure&gt;
&lt;img class="noZoom" src="/images/hierarchical_model.png" alt="pooled, unpooled, hierarchical model"&gt;
&lt;figcaption align="center"&gt;&lt;strong&gt;Figure 1:&lt;/strong&gt; Hierarchical model as a combination of a pooled and an unpooled model 
from &lt;a href="https://widdowquinn.github.io/Teaching-Stan-Hierarchical-Modelling/07-partial_pooling_intro.html"&gt;Bayesian Multilevel Modelling using PyStan&lt;/a&gt;.
&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;
&lt;h2&gt;Recent&amp;nbsp;Advances&lt;/h2&gt;
&lt;p&gt;So far I mostly used &lt;a href="https://docs.pymc.io/"&gt;PyMC3&lt;/a&gt; for Bayesian inference or &lt;em&gt;probabilistic programming&lt;/em&gt; as the authors
of PyMC3 like to call it. I love it for it&amp;#8217;s elegant design and consequently its expressiveness. The documentation is great
and thus you can pretty much hack away with your model ideas. The only problem I always had with it is that for me it never
scaled so well with somewhat larger datasets, i.e. more than 100k data points, and a larger number of parameters. There is a technical and 
methodical reason for it. Regarding the former, PyMC3 uses &lt;a href="https://github.com/Theano/Theano"&gt;Theano&lt;/a&gt; to speed
up its computations by transpiling your Python code to C. Theano inspired many frameworks like &lt;a href="https://www.tensorflow.org/"&gt;Tensorflow&lt;/a&gt; 
and &lt;a href="https://pytorch.org/"&gt;PyTorch&lt;/a&gt; but is considered deprecated today and cannot rival the speed of modern frameworks
anymore. For the latter, I used PyMC3 mostly with &lt;a href="https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo"&gt;Markov chain Monte Carlo&lt;/a&gt; (&lt;span class="caps"&gt;MCMC&lt;/span&gt;) based methods, 
which are sampling algorithms and thus computationally quite demanding, while &lt;a href="https://en.wikipedia.org/wiki/Variational_Bayesian_methods"&gt;variational inference (&lt;span class="caps"&gt;VI&lt;/span&gt;)&lt;/a&gt;
methods are much faster. But also when using &lt;span class="caps"&gt;VI&lt;/span&gt;, which PyMC3 also supports, it never really allowed me to deal with larger datasets rendering
Bayesian Hierarchical Modelling (&lt;span class="caps"&gt;BHM&lt;/span&gt;) a wonderful tool that sadly could not be applied in many suitable projects due to its computational&amp;nbsp;costs.&lt;/p&gt;
&lt;p&gt;Luckily, the world of data science moves on with an incredible speed, and some time ago I had a nice project at my hand that
could make good use of &lt;span class="caps"&gt;BHM&lt;/span&gt;. Thus, I gave it another shot and also looked beyond PyMC3. My first candidate to evaluate was &lt;a href="http://pyro.ai/"&gt;Pyro&lt;/a&gt;,
which uses Stochastic Variational Inference (&lt;span class="caps"&gt;SVI&lt;/span&gt;) by default, and calls itself a &lt;em&gt;deep universal probabilistic programming&lt;/em&gt; framework.
Instead of Theano it is based on PyTorch and thus allows for &lt;a href="https://en.wikipedia.org/wiki/Just-in-time_compilation"&gt;just-in-time (&lt;span class="caps"&gt;JIT&lt;/span&gt;) compilation&lt;/a&gt;,
which sped up my test case already quite a bit. Pyro also emphasizes vectorization, thus allowing for fast parallel computation, e.g. &lt;a href="https://en.wikipedia.org/wiki/SIMD"&gt;&lt;span class="caps"&gt;SIMD&lt;/span&gt;&lt;/a&gt; operations.
In total the speed-up compared to PyMC3 was amazing in my test-case letting me almost forget the two downsides of Pyro compared
to PyMC3. Firstly, the documentation of Pyro is not as polished and secondly, it&amp;#8217;s just so much more complicated to use and understand 
but your mileage may vary on that&amp;nbsp;one. &lt;/p&gt;
&lt;p&gt;Digging through the website of Pyro I then stumbled over &lt;a href="https://github.com/pyro-ppl/numpyro"&gt;NumPyro&lt;/a&gt; that has a similar
interface compared to Pyro but uses &lt;a href="https://github.com/google/jax"&gt;&lt;span class="caps"&gt;JAX&lt;/span&gt;&lt;/a&gt; instead of PyTorch as its backend. &lt;span class="caps"&gt;JAX&lt;/span&gt; is like &lt;a href="https://numpy.org/"&gt;NumPy&lt;/a&gt; on 
steroids. It&amp;#8217;s crazy fast as it uses &lt;a href="https://www.tensorflow.org/xla"&gt;&lt;span class="caps"&gt;XLA&lt;/span&gt;&lt;/a&gt;, which is a domain-specific compiler for linear algebra
operations. Additionally, it allows for automatic differentiation like &lt;a href="https://github.com/hips/autograd"&gt;Autograd&lt;/a&gt;,
whose maintainers moved over to develop &lt;span class="caps"&gt;JAX&lt;/span&gt; further. Long story short, NumPyro even blew the benchmark results of Pyro out of the water.
For the first time (at least for what I know), NumPyro allows you do Bayesian inference with lots of parameters like in
&lt;span class="caps"&gt;BHM&lt;/span&gt; on large data! In the rest of this post, I want to show you how NumPyro can be applied in a typical demand prediction
use-case on some public dataset. The dataset in my actual use-case was much bigger, my model had more parameters and NumPyro could still handle it but you just have to trust me on this one ;-)
Hopefully some readers will find this post useful and maybe it mitigates a bit the pain coming from the lack of NumPyro&amp;#8217;s documentation and&amp;nbsp;examples.&lt;/p&gt;
&lt;h2&gt;Use-Case &lt;span class="amp"&gt;&amp;amp;&lt;/span&gt;&amp;nbsp;Modelling&lt;/h2&gt;
&lt;p&gt;Imagine you have many retail stores and want to make individual demand predictions for them. For stores that were opened
a long time ago, this should be no problem but how do you deal with stores that first opened a week ago or even will open soon? Like in the example
of apartment prices in different districts, &lt;span class="caps"&gt;BHM&lt;/span&gt; helps you to deal exactly with this &lt;a href="https://en.wikipedia.org/wiki/Cold_start_(recommender_systems)"&gt;cold start problem&lt;/a&gt;. We take the 
&lt;a href="https://www.kaggle.com/c/rossmann-store-sales"&gt;Rossmann dataset from Kaggle&lt;/a&gt; to simulate this problem by removing the data
of some of the stores. The data consists of a &lt;em&gt;train&lt;/em&gt; dataset with information about the sales and daily features of the stores,
e.g. if a promotion happened (&lt;code&gt;promo&lt;/code&gt;), as well as a &lt;em&gt;store&lt;/em&gt; dataset with time-independent store features.
Here&amp;#8217;s what we wanna do in our little experiment and study&amp;nbsp;protocol:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;join the data from Kaggle&amp;#8217;s &lt;code&gt;train.csv&lt;/code&gt; dataset with the general store features from the &lt;code&gt;store.csv&lt;/code&gt; dataset,&lt;/li&gt;
&lt;li&gt;perform some really basic feature engineering and encoding of the categorical&amp;nbsp;features,&lt;/li&gt;
&lt;li&gt;split the data into train and test where we treat the stores from train as being opened for a long time and the ones
  from test as newly&amp;nbsp;opened,&lt;/li&gt;
&lt;li&gt;fit our hierarchical model on the train dataset to infer the &amp;#8220;global&amp;#8221; parameters of the upper model&amp;nbsp;hierarchy,&lt;/li&gt;
&lt;li&gt;take only the first 7 days for each store in the test data, which we assume to know, and fit our model only inferring
 the local, i.e. store-specific, parameters of the lower hierarchy while keeping the global ones&amp;nbsp;fixed,&lt;/li&gt;
&lt;li&gt;compare the inferred parameters of a test store to:&lt;ol&gt;
&lt;li&gt;the inferred local parameters of a simple Poisson model. We expect them to be completely different due to the lack
   of data and thus overfitting of the Poisson&amp;nbsp;model,&lt;/li&gt;
&lt;li&gt;the inferred local parameters of our model if we had given it the whole time series from test, i.e. not only the first 7 days.
   In this case, we assume that we are already pretty close since the priors given by the global parameters nudge them
   in the right direction even with only little&amp;nbsp;data.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;All code of this little experiment can be found under my &lt;a href="https://github.com/FlorianWilhelm/bhm-at-scale"&gt;bhm-at-scale repository&lt;/a&gt;
so that you can follow along easily.
The steps 1-3 are performed in the &lt;a href="https://github.com/FlorianWilhelm/bhm-at-scale/blob/master/notebooks/01-preprocessing.ipynb"&gt;preprocessing notebook&lt;/a&gt;
and are actually not that interesting, thus we will skip it here. Steps 4-6 are performed in the
&lt;a href="https://github.com/FlorianWilhelm/bhm-at-scale/blob/master/notebooks/02-model.ipynb"&gt;model notebook&lt;/a&gt; while some visualisations are presented in the &lt;a href="https://github.com/FlorianWilhelm/bhm-at-scale/blob/master/notebooks/03-evaluation.ipynb"&gt;evaluation notebook&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;But before we start to get technical, let&amp;#8217;s take a minute and frame again the forecasting problem from a more mathematical side.
The data of each store is a time-series of feature vectors and target scalars. We want to find a mapping such that the
feature vector of each time-step is mapped to a value close to the target scalar of the respective time-step. Since our target
value, i.e. the number of sales, is a non-negative integer we could assume a &lt;a href="https://en.wikipedia.org/wiki/Poisson_distribution"&gt;Poisson distribution&lt;/a&gt; and consequently
perform a &lt;a href="https://en.wikipedia.org/wiki/Poisson_regression"&gt;Poisson regression&lt;/a&gt; in a hierarchical way. This would be kind of okay
if we were only interested in a point estimation and thus would not care about the variance of the predictive posterior distribution. 
The Poisson distribution only has one parameter &lt;span class="math"&gt;\(\lambda\)&lt;/span&gt; that allows you to define the mean &lt;span class="math"&gt;\(\mu\)&lt;/span&gt; while the variance &lt;span class="math"&gt;\(\sigma^2\)&lt;/span&gt; then just equals the mean as there is no way to adjust the variance independently. &lt;br&gt;
In many practical use-cases, there is &lt;a href="https://en.wikipedia.org/wiki/Overdispersion"&gt;overdispersion&lt;/a&gt; though, meaning that the variance is larger than the mean and we have to make up for it.
We can define a so called &lt;em&gt;dispersion parameter&lt;/em&gt; &lt;span class="math"&gt;\(r\in(0,\infty)\)&lt;/span&gt; by reparametrization in the &lt;a href="https://en.wikipedia.org/wiki/Negative_binomial_distribution"&gt;negative binomial distribution&lt;/a&gt;,&amp;nbsp;i.e.&lt;/p&gt;
&lt;div class="math"&gt;$$\mathrm{NB}(y;\mu,r) = \frac{\Gamma(r+y)}{y!\cdot\Gamma(r)}\cdot\left(\frac{r}{r+\mu}\right)^r\cdot\left(\frac{\mu}{r+\mu}\right)^y,$$&lt;/div&gt;
&lt;p&gt;where &lt;span class="math"&gt;\(\Gamma\)&lt;/span&gt; is the &lt;a href="https://en.wikipedia.org/wiki/Gamma_function"&gt;Gamma function&lt;/a&gt;. Now we&amp;nbsp;have&lt;/p&gt;
&lt;div class="math"&gt;$$\sigma^2=\mu + \frac{1}{r}\mu^2$$&lt;/div&gt;
&lt;p&gt;and using &lt;span class="math"&gt;\(r\)&lt;/span&gt; we are thus able to adjust the variance from &lt;span class="math"&gt;\(\mu\)&lt;/span&gt; to &lt;span class="math"&gt;\(\infty\)&lt;/span&gt;. 
Another name for the negative binomial distribution is Gamma-Poisson distribution and this is the name under which we find it also in NumPyro. I find this name 
much more catchy since you can imagine a Poisson distribution with its only parameter drawn from a Gamma distribution that
has two parameters &lt;span class="math"&gt;\(\alpha\)&lt;/span&gt; and &lt;span class="math"&gt;\(\beta\)&lt;/span&gt;. This also intuitively explains why the variance of &lt;span class="caps"&gt;NB&lt;/span&gt; is bounded below by its mean.
Just think of &lt;span class="caps"&gt;NB&lt;/span&gt; as a generalization of the Poisson distribution with one more parameter that allows adjusting the&amp;nbsp;variance.&lt;/p&gt;
&lt;p&gt;Uncertainty Quantification is a crucial requirement for demand forecasts in retail although peculiarly, no one really cares about forecasts in retail anyway.
What retailers really care about is optimal replenishment, meaning that they want to have a system telling them how much
to order so that there is an optimal amount of stocks available in their store. In order to provide optimal replenishment
suggestions you need demand forecasts that provide probability distributions, not only point estimations. With the help
of those distributions the replenishment system basically runs an optimization with respect to some cost function, e.g.
cost of a missed sale is weighted 3 times the cost of a written-off product, and further constraints, e.g. if products can only be ordered in bundles of 10. 
For these reasons we will use the &lt;span class="caps"&gt;NB&lt;/span&gt; distribution that allows us the quantify the uncertainties in our sales predictions&amp;nbsp;adequately.&lt;/p&gt;
&lt;p&gt;So now that we settled with &lt;span class="caps"&gt;NB&lt;/span&gt; as the distribution that we want to fit to the daily sales of our stores &lt;span class="math"&gt;\(\mathbf{y}\)&lt;/span&gt;, we can think
about incorporating our features &lt;span class="math"&gt;\(\mathbf{x}\)&lt;/span&gt;. We want to use a linear model to map &lt;span class="math"&gt;\(\mathbf{x}\)&lt;/span&gt; to &lt;span class="math"&gt;\(\mathbf{\mu}\)&lt;/span&gt; such
that we can use it later to calculate &lt;span class="math"&gt;\(\alpha\)&lt;/span&gt; and &lt;span class="math"&gt;\(\beta\)&lt;/span&gt; of &lt;span class="caps"&gt;NB&lt;/span&gt;. Using again the fact that we are dealing with non-negative
numbers and also considering that we expect effects to be multiplicative, e.g. 10% more during a promotion, our ansatz&amp;nbsp;is&lt;/p&gt;
&lt;div class="math"&gt;$$\mu = \exp(\mathbf{\theta}^\top\mathbf{x}),$$&lt;/div&gt;
&lt;p&gt;where &lt;span class="math"&gt;\(\mathbf{\theta}\)&lt;/span&gt; is a vector of coefficients. For each store &lt;span class="math"&gt;\(i\)&lt;/span&gt; and each feature &lt;span class="math"&gt;\(j\)&lt;/span&gt; we will have a separate
coefficient &lt;span class="math"&gt;\(\theta_{ij}\)&lt;/span&gt;. The &lt;span class="math"&gt;\(\theta_{ij}\)&lt;/span&gt; are regularized by parameters &lt;span class="math"&gt;\(\theta^\mu_j\)&lt;/span&gt; and &lt;span class="math"&gt;\(\theta^{\sigma^2}_j\)&lt;/span&gt; on 
the global level, which helps us in case a store has only little historical data. For the dispersion parameters we
infer individual &lt;span class="math"&gt;\(r_i\)&lt;/span&gt; for each store &lt;span class="math"&gt;\(i\)&lt;/span&gt; as well as global parameters &lt;span class="math"&gt;\(r^\mu\)&lt;/span&gt; and &lt;span class="math"&gt;\(r^{\sigma^2}\)&lt;/span&gt; over all stores. 
And that&amp;#8217;s already most of it. Figure 2 depicts the graphical model outlined so&amp;nbsp;far.&lt;/p&gt;
&lt;figure&gt;
&lt;p align="center"&gt;
&lt;img class="noZoom" src="/images/bhm_model.png" alt="centered hierarchical model" width="60%"&gt;
&lt;/p&gt;
&lt;figcaption align="center"&gt;
&lt;strong&gt;Figure 2:&lt;/strong&gt; Graphical representation of a hierarchical model (centered version) as defined above.
&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;
&lt;p&gt;Those boxes in Figure 2, which are called &lt;a href="https://en.wikipedia.org/wiki/Plate_notation"&gt;plates&lt;/a&gt;, tell you how many times a parameter is repeated.
Nested plates are multiplied by the number given by outer plates, which can also be seen by looking at the number of indices.
The concept of plates was also taken up by the authors of NumPyro to express that certain dimensions are conditionally independent.
This also helps them to increase performance by taking optimizations into account that are just not possible in the general case.
Shaded circles are observed values, which in our case are the number of sales on a given day &lt;span class="math"&gt;\(k\)&lt;/span&gt; and store &lt;span class="math"&gt;\(i\)&lt;/span&gt;.&lt;/p&gt;
&lt;h2&gt;Implementation&lt;/h2&gt;
&lt;p&gt;Let&amp;#8217;s take a quick look into our model code which is just a normal Python function. It&amp;#8217;s good to keep in mind, that we call this a &lt;em&gt;model&lt;/em&gt; since we assume that given
the right parameters it would be able to generate sales for some given stores and days resembling the observed sales for these stores and days.
The model function only defines the model parameters, how they interact and their&amp;nbsp;priors.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpyro&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpyro.distributions&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;dist&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;numpyro.infer.reparam&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;TransformReparam&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;jax.numpy&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;DeviceArray&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;jax.numpy&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;jnp&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;jax&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;random&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;DeviceArray&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;DeviceArray&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;Gamma-Poisson hierarchical model for daily sales forecasting&lt;/span&gt;

&lt;span class="sd"&gt;    Args:&lt;/span&gt;
&lt;span class="sd"&gt;        X: input data&lt;/span&gt;

&lt;span class="sd"&gt;    Returns:&lt;/span&gt;
&lt;span class="sd"&gt;        output data&lt;/span&gt;
&lt;span class="sd"&gt;    &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;n_stores&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_days&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_features&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;
    &lt;span class="n"&gt;n_features&lt;/span&gt; &lt;span class="o"&gt;-=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;  &lt;span class="c1"&gt;# remove one dim for target&lt;/span&gt;
    &lt;span class="n"&gt;eps&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;1e-12&lt;/span&gt;  &lt;span class="c1"&gt;# epsilon&lt;/span&gt;

    &lt;span class="n"&gt;plate_features&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;numpyro&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;features&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_features&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dim&lt;/span&gt;&lt;span class="o"&gt;=-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;plate_stores&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;numpyro&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;stores&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_stores&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dim&lt;/span&gt;&lt;span class="o"&gt;=-&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;plate_days&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;numpyro&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;timesteps&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_days&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dim&lt;/span&gt;&lt;span class="o"&gt;=-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;disp_param_mu&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;numpyro&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="s2"&gt;&amp;quot;disp_param_mu&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;dist&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;loc&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;4.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;scale&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;1.&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;disp_param_sigma&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;numpyro&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="s2"&gt;&amp;quot;disp_param_sigma&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;dist&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;HalfNormal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;scale&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;1.&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;plate_stores&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;numpyro&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;handlers&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reparam&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;disp_params&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;TransformReparam&lt;/span&gt;&lt;span class="p"&gt;()}):&lt;/span&gt;
            &lt;span class="n"&gt;disp_params&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;numpyro&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
                &lt;span class="s2"&gt;&amp;quot;disp_params&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                &lt;span class="n"&gt;dist&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;TransformedDistribution&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
                    &lt;span class="n"&gt;dist&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;loc&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;jnp&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;n_stores&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt; &lt;span class="n"&gt;scale&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
                    &lt;span class="n"&gt;dist&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;transforms&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;AffineTransform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;disp_param_mu&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;disp_param_sigma&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
                &lt;span class="p"&gt;),&lt;/span&gt;
            &lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;plate_features&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;coef_mus&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;numpyro&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
            &lt;span class="s2"&gt;&amp;quot;coef_mus&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;dist&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;loc&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;jnp&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_features&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
                        &lt;span class="n"&gt;scale&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;jnp&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ones&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_features&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
        &lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;coef_sigmas&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;numpyro&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
            &lt;span class="s2"&gt;&amp;quot;coef_sigmas&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;dist&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;HalfNormal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;scale&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;2.&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;jnp&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ones&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_features&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
        &lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;plate_stores&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;numpyro&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;handlers&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reparam&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;coefs&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;TransformReparam&lt;/span&gt;&lt;span class="p"&gt;()}):&lt;/span&gt;
                &lt;span class="n"&gt;coefs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;numpyro&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
                    &lt;span class="s2"&gt;&amp;quot;coefs&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                    &lt;span class="n"&gt;dist&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;TransformedDistribution&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
                        &lt;span class="n"&gt;dist&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;loc&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;jnp&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;n_stores&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_features&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt; &lt;span class="n"&gt;scale&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;1.0&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
                        &lt;span class="n"&gt;dist&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;transforms&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;AffineTransform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;coef_mus&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;coef_sigmas&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
                    &lt;span class="p"&gt;),&lt;/span&gt;
                &lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;plate_days&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;plate_stores&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;targets&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;...&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="n"&gt;features&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;jnp&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nan_to_num&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;...&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;  &lt;span class="c1"&gt;# padded features to 0&lt;/span&gt;
        &lt;span class="n"&gt;is_observed&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;jnp&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;where&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;jnp&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;isnan&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;targets&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;jnp&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros_like&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;targets&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;jnp&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ones_like&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;targets&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
        &lt;span class="n"&gt;not_observed&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;is_observed&lt;/span&gt;
        &lt;span class="n"&gt;means&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;is_observed&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;jnp&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;jnp&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;jnp&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;expand_dims&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;coefs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
                 &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;not_observed&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;eps&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="n"&gt;betas&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;is_observed&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;jnp&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;disp_params&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;not_observed&lt;/span&gt;
        &lt;span class="n"&gt;alphas&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;means&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;betas&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;numpyro&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;days&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dist&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;GammaPoisson&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;alphas&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;betas&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;obs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;jnp&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nan_to_num&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;targets&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Note that &lt;code&gt;disp_param&lt;/code&gt; is &lt;span class="math"&gt;\(r\)&lt;/span&gt; and &lt;code&gt;coef&lt;/code&gt; is &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; in the source code above for better readability. You will
recognize a lot of what we have talked about and I don&amp;#8217;t want to go into the syntactical details of NumPyro. My suggestion
would be to first read the documentation of Pyro, as it is way more comprehensive, and then look up the differences in the
NumPyro&amp;nbsp;reference. &lt;/p&gt;
&lt;p&gt;Reading the source code more thoroughly, you might wonder about the definition of the coefficients&amp;nbsp;as:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;numpyro&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;handlers&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reparam&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;coefs&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;TransformReparam&lt;/span&gt;&lt;span class="p"&gt;()}):&lt;/span&gt;
    &lt;span class="n"&gt;coefs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;numpyro&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="s2"&gt;&amp;quot;coefs&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;dist&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;TransformedDistribution&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
            &lt;span class="n"&gt;dist&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;loc&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;jnp&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;n_stores&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_features&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt; &lt;span class="n"&gt;scale&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;1.0&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
            &lt;span class="n"&gt;dist&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;transforms&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;AffineTransform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;coef_mus&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;coef_sigmas&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
        &lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The explanations of the model I have given and also the plot, actually shows the &lt;em&gt;centered version&lt;/em&gt; of a
hierarchical model. For me the centered version feels much more intuitive and is easier to explain. The downside is that
the direct dependency of the local parameters on the global ones make it hard for many &lt;span class="caps"&gt;MCMC&lt;/span&gt; sampling methods but also &lt;span class="caps"&gt;SVI&lt;/span&gt; methods to explore
certain regions of the local parameter space. This effect is called &lt;em&gt;funnel&lt;/em&gt; and can be imagined as walking with the the same step length
on a bridge that gets narrower and narrower. From the point on where the bridge is about as wide as your step length, you might
become a bit hesitant to explore more of it. As very often the case, a reparameterization overcomes this problem resulting
in the &lt;em&gt;non-centered&lt;/em&gt; version of a hierarchical model. This is the version used in the implementation. If you want to know more about this, a really great 
&lt;a href="https://twiecki.io/blog/2017/02/08/bayesian-hierchical-non-centered/"&gt;blog post by Thomas Wiecki&lt;/a&gt; gives you all the details about&amp;nbsp;it.&lt;/p&gt;
&lt;p&gt;Another thing that wasn&amp;#8217;t mentioned yet are the &lt;code&gt;is_observed&lt;/code&gt; and &lt;code&gt;not_observed&lt;/code&gt; variables which are just a nice gimmick.
Instead of using up degrees of freedom to learn that the number of sales is 0 on days where the store is closed, I set
the target variable &lt;span class="math"&gt;\(y\)&lt;/span&gt; to &lt;em&gt;not observed&lt;/em&gt; instead of 0. During training, these target values are just ignored and later
allows the model to answer a store manager&amp;#8217;s potential question: &amp;#8220;How many sales would I have had if I had opened my store on that&amp;nbsp;day?&amp;#8221;&lt;/p&gt;
&lt;p&gt;Until now we have talked about the model and if you are a PyMC3 user, you might think that this should be enough to actually solve it.
Pyro and NumPyro have a curious difference with respect to that. To actually fit the parameters of the model, distributions for the 
parameters have to be defined since its &lt;span class="caps"&gt;SVI&lt;/span&gt;. This is done in a separate function called &lt;em&gt;guide&lt;/em&gt;. &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;guide&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;DeviceArray&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;Guide with parameters of the posterior&lt;/span&gt;

&lt;span class="sd"&gt;    Args:&lt;/span&gt;
&lt;span class="sd"&gt;        X: input data&lt;/span&gt;
&lt;span class="sd"&gt;    &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;n_stores&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_days&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_features&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;
    &lt;span class="n"&gt;n_features&lt;/span&gt; &lt;span class="o"&gt;-=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;  &lt;span class="c1"&gt;# remove one dim for target&lt;/span&gt;

    &lt;span class="n"&gt;plate_features&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;numpyro&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;features&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_features&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dim&lt;/span&gt;&lt;span class="o"&gt;=-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;plate_stores&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;numpyro&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;stores&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_stores&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dim&lt;/span&gt;&lt;span class="o"&gt;=-&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;disp_param_mu&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;numpyro&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="s2"&gt;&amp;quot;disp_param_mu&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;dist&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;loc&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;numpyro&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;param&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;loc_disp_param_mu&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;4.&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;jnp&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ones&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt;
                    &lt;span class="n"&gt;scale&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;numpyro&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;param&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;scale_disp_param_mu&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;1.&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;jnp&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ones&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
                                        &lt;span class="n"&gt;constraint&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;dist&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;constraints&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;positive&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;disp_param_sigma&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;numpyro&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="s2"&gt;&amp;quot;disp_param_sigma&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;dist&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;TransformedDistribution&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
            &lt;span class="n"&gt;dist&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;loc&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;numpyro&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;param&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;loc_disp_param_logsigma&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;jnp&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ones&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt;
                        &lt;span class="n"&gt;scale&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;numpyro&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;param&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;scale_disp_param_logsigma&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;jnp&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ones&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
                                            &lt;span class="n"&gt;constraint&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;dist&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;constraints&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;positive&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt;
            &lt;span class="n"&gt;transforms&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;dist&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;transforms&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ExpTransform&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
    &lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;plate_stores&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;numpyro&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;handlers&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reparam&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;disp_params&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;TransformReparam&lt;/span&gt;&lt;span class="p"&gt;()}):&lt;/span&gt;
            &lt;span class="n"&gt;numpyro&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
                &lt;span class="s2"&gt;&amp;quot;disp_params&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                &lt;span class="n"&gt;dist&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;TransformedDistribution&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
                    &lt;span class="n"&gt;dist&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
                        &lt;span class="n"&gt;loc&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;numpyro&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;param&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;loc_disp_params&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;jnp&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;n_stores&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))),&lt;/span&gt;
                        &lt;span class="n"&gt;scale&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;numpyro&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;param&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;scale_disp_params&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.1&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;jnp&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ones&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;n_stores&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt;
                                            &lt;span class="n"&gt;constraint&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;dist&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;constraints&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;positive&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                        &lt;span class="p"&gt;),&lt;/span&gt;
                    &lt;span class="p"&gt;),&lt;/span&gt;
                    &lt;span class="n"&gt;dist&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;transforms&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;AffineTransform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;disp_param_mu&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;disp_param_sigma&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
                &lt;span class="p"&gt;),&lt;/span&gt;
            &lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;plate_features&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;numpyro&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
            &lt;span class="s2"&gt;&amp;quot;coef_mus&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;dist&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;loc&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;numpyro&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;param&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;loc_coef_mus&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;jnp&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ones&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_features&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt;
                        &lt;span class="n"&gt;scale&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;numpyro&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;param&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;scale_coef_mus&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;jnp&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ones&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_features&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
                                            &lt;span class="n"&gt;constraint&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;dist&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;constraints&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;positive&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
        &lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;numpyro&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
            &lt;span class="s2"&gt;&amp;quot;coef_sigmas&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;dist&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;TransformedDistribution&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
                &lt;span class="n"&gt;dist&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;loc&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;numpyro&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;param&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;loc_coef_logsigmas&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;jnp&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_features&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt;
                            &lt;span class="n"&gt;scale&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;numpyro&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;param&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;scale_coef_logsigmas&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;jnp&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ones&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_features&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
                                                &lt;span class="n"&gt;constraint&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;dist&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;constraints&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;positive&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt;
                &lt;span class="n"&gt;transforms&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;dist&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;transforms&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ExpTransform&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
        &lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;plate_stores&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;numpyro&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;handlers&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reparam&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;coefs&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;TransformReparam&lt;/span&gt;&lt;span class="p"&gt;()}):&lt;/span&gt;
                &lt;span class="n"&gt;numpyro&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
                    &lt;span class="s2"&gt;&amp;quot;coefs&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                    &lt;span class="n"&gt;dist&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;TransformedDistribution&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
                        &lt;span class="n"&gt;dist&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
                            &lt;span class="n"&gt;loc&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;numpyro&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;param&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;loc_coefs&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;jnp&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;n_stores&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_features&lt;/span&gt;&lt;span class="p"&gt;))),&lt;/span&gt;
                            &lt;span class="n"&gt;scale&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;numpyro&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;param&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;scale_coefs&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.5&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;jnp&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ones&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;n_stores&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_features&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt;
                                                &lt;span class="n"&gt;constraint&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;dist&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;constraints&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;positive&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                            &lt;span class="p"&gt;),&lt;/span&gt;
                        &lt;span class="p"&gt;),&lt;/span&gt;
                        &lt;span class="n"&gt;dist&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;transforms&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;AffineTransform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;coef_mus&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;coef_sigmas&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
                    &lt;span class="p"&gt;),&lt;/span&gt;
                &lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;As you can see, the structure of the guide reflects the structure of the model and there we are just defining for each model parameter
a distribution that again has parameters that need to be determined. The link between model and guide is given by the names of the &lt;em&gt;sample sites&lt;/em&gt;
like &amp;#8220;coef_offsets&amp;#8221;. This is a bit dangerous as a single typo in the model or guide may break this link leading to unexpected
behaviour. I spent more than a day of debugging a model once until I realized that some sample site in the guide had a typo.
You can see in the actual implementation, i.e. &lt;a href="https://github.com/FlorianWilhelm/bhm-at-scale/blob/master/src/bhm_at_scale/model.py"&gt;model.py&lt;/a&gt;, that I learnt from my mistakes as this source of error can be completely eliminated by simply defining class variables&amp;nbsp;like:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Site&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;disp_param_mu&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;disp_param_mu&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;disp_param_sigma&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;disp_param_sigma&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;disp_param_offsets&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;disp_param_offsets&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;disp_params&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;disp_params&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;coef_mus&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;coef_mus&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;coef_sigmas&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;coef_sigmas&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;coef_offsets&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;coef_offsets&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;coefs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;coefs&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;days&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;days&amp;quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Then using variables like &lt;code&gt;Site.coef_offsets&lt;/code&gt; instead of strings like &lt;code&gt;"coef_offsets"&lt;/code&gt; as identifiers of sample sites, allows your &lt;span class="caps"&gt;IDE&lt;/span&gt; to inform you about any typo as you go. Problem&amp;nbsp;solved.&lt;/p&gt;
&lt;p&gt;Besides the model and guide, we also have to define a local guide and a predictive model. The local guide assumes
that we have already fitted the global parameters but want to only determine the local parameters of new stores with little data.
The predictive model assumes global and local parameters to be already inferred so that we can use it to predict the number
of sales on days beyond our training interval. As these functions are only slide variations, I spare you the details and refer you to the implementation in &lt;a href="https://github.com/FlorianWilhelm/bhm-at-scale/blob/master/src/bhm_at_scale/model.py"&gt;model.py&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Most parts of the &lt;a href="https://github.com/FlorianWilhelm/bhm-at-scale/blob/master/notebooks/02-model.ipynb"&gt;model notebook&lt;/a&gt; actually deal with training the model on stores from the training data with a long sales history,
then fixing the global parameters and fitting the local guide on the stores from the test set with a really short history of 7 days.
This works really well although the number of features, which is 23, is much higher than 7! Let&amp;#8217;s take a look at the coefficients 
of a store from the test set as detailed in the &lt;a href="https://github.com/FlorianWilhelm/bhm-at-scale/blob/master/notebooks/02-model.ipynb"&gt;model notebook&lt;/a&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;[ 2.7901888   2.6591218   2.6881874   2.6126788   2.656775    2.554397
  2.4938385   0.33227605  3.115691    2.9264386   2.692092    2.9548435
  0.05613964  0.06542112  2.8379264   2.9023972   3.5701406   3.2074358
  4.0569873   2.9304545   2.7463424   2.823191    2.959007  ]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The notebook also shows that the traditional Poisson regression using Scikit-Learn overfits the training set and yields implausible coefficients. 
Comparing the coefficients from above to the ones of the Poisson regression for the same store,&amp;nbsp;i.e.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;[ 1.1747563  -2.2386415   2.1442924   1.9889396   1.9385103   1.8024149
 -6.8102717   1.1747563   2.2386413  -2.2386413   0.          0.
 -0.09434899  0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.        ]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;we see that they are highly different and zero for features in the Poisson regression that weren&amp;#8217;t encountered in the data of just 7 days.
Now comparing the coefficients of our &lt;span class="caps"&gt;BHM&lt;/span&gt; model trained on just 7 days with the coefficients of a cheating model trained on the whole test set,&amp;nbsp;i.e.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;[ 2.814997    2.7551868   2.6182172   2.6453698   2.672692    2.5201027
  2.593036    0.2265296   3.184446    3.1163387   2.5429602   2.9477317
 -0.03218627  0.06836608  2.8726482   2.925492    3.56679     3.215817
  4.0523443   2.9164758   2.7241356   2.8247747   2.9598234 ]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;we see that they are quite similar. That&amp;#8217;s the magic of a hierarchical model! We started with plausible defaults from a 
global perspective and adapted locally depending on the amount of available&amp;nbsp;data.&lt;/p&gt;
&lt;p&gt;Running the code from the &lt;a href="https://github.com/FlorianWilhelm/bhm-at-scale/blob/master/notebooks/02-model.ipynb"&gt;model notebook&lt;/a&gt; on your laptop will be matter of a few minutes for the training on 1000 stores
each having 942 days of sales and 23 features for each store and day. In total this leads to roughly one million data points
and &lt;span class="math"&gt;\(23\cdot 1000+1000+2\cdot 23+2=24048\)&lt;/span&gt; parameters in our graphical model. Since each parameter is fitted with the help of a 
parameterized distribution in the guide, as we are doing &lt;span class="caps"&gt;SVI&lt;/span&gt;, the number of actual variables is twice as much leading to roughly 50,000
variables that need to be fitted. While 50k parameters and about 1 Million data points surely is not big data, 
it&amp;#8217;s still impressive that using NumPyro you can fit a model like that within a few minutes on your
laptop and the implementation is not even using batching that would speed it up even further. In one of my customer
projects we used a way larger model on much more data and our workstation was still able to handle it smoothly. 
NumPyro really scales well even beyond this little&amp;nbsp;demonstration. &lt;/p&gt;
&lt;h2&gt;Visualization&lt;/h2&gt;
&lt;p&gt;There is now tons of things one could do with the results of our hierarchical model. One could check out the
actual prediction results, look at how certain we are about the parameters like the coefficients and so on. Most of that
I will leave to the interested reader and give only a few tidbits here from the &lt;a href="https://github.com/FlorianWilhelm/bhm-at-scale/blob/master/notebooks/03-evaluation.ipynb"&gt;evaluation notebook&lt;/a&gt;.
We start with taking a look at the sales prediction for one of the stores from the test set as depicted in Figure&amp;nbsp;3.&lt;/p&gt;
&lt;figure&gt;
&lt;p align="center"&gt;
&lt;img class="noZoom" src="/images/bhm_sales_forecast.png" alt="plot of sales forecast"&gt;
&lt;/p&gt;
&lt;figcaption align="center"&gt;
&lt;strong&gt;Figure 3:&lt;/strong&gt; Sales forecast of one store from the test set. The blue dashed line is the the mean predicted mean.
&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;
&lt;p&gt;Only judging by the eye, we see that predicted mean (blue dashed line) follows the number of sales (bluish bars) quite good.
The background is shaded according to some important features like promotions and holidays which explain some of the variations
in our predictions. Just for information, also the number of customers are displayed but not used in the prediction of course.
Also, we see the 50% and 90% &lt;a href="https://en.wikipedia.org/wiki/Credible_interval"&gt;credible intervals&lt;/a&gt; as shaded blue areas
around our mean, which tell us how certain we are about our predictions. We can also see that on Sundays, when the store
was closed, we predict not 0 but what would have likely happened if it wasn&amp;#8217;t closed, which was part of our&amp;nbsp;model.&lt;/p&gt;
&lt;p&gt;We could then also start looking into the effects of certain features like the weekdays. Figure 4 shows for each
weekday starting with Monday a density over the means of all&amp;nbsp;stores. &lt;/p&gt;
&lt;figure&gt;
&lt;p align="center"&gt;
&lt;img class="noZoom" src="/images/bhm_weekday_effect.png" alt="plot weekday effect"&gt;
&lt;/p&gt;
&lt;figcaption align="center"&gt;
&lt;strong&gt;Figure 4:&lt;/strong&gt; Density plot of the means of the weekday coefficients over all stores. 
&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;
&lt;p&gt;We can see that on average there seem to be a higher sales uplift on Mondays and also a high variance for the means on Saturdays and
Sundays when many stores are closed. If we are more interested in things we can change, like when to do a promotion,
we could be interested in analyzing the distribution of the promotion effect over all stores as shown in Figure&amp;nbsp;5.  &lt;/p&gt;
&lt;figure&gt;
&lt;p align="center"&gt;
&lt;img class="noZoom" src="/images/bhm_promo_effect.png" alt="plot of promotion effect"&gt;
&lt;/p&gt;
&lt;figcaption align="center"&gt;
&lt;strong&gt;Figure 5:&lt;/strong&gt; Density plot of the promotion effect over all stores with the red line showing the median.
&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;
&lt;p&gt;These are just some analysis one could start to look into to derive useful insights for store management. 
The rest is up to your imagination and also depending on your use-case. Imagine we had a dataset that features not 
only the aggregated sales but also the ones of individual products, i.e. &lt;a href="https://en.wikipedia.org/wiki/Stock_keeping_unit"&gt;&lt;span class="caps"&gt;SKU&lt;/span&gt;-level&lt;/a&gt;. We could then build hierarchies over the product hierarchies
and thus addressing cannibalization effects, e.g. when we introduce a new type of wine within our current offering.
We could also use &lt;span class="caps"&gt;BHM&lt;/span&gt; to address &lt;a href="https://en.wikipedia.org/wiki/Censoring_(statistics)"&gt;censored data&lt;/a&gt;, which is also an important task when doing demand forecasts. So far we have
used the words sales forecast and demand forecast interchangeably but bear in mind that we are actually interested in the demand.
Canonically, one assumes that the demand for a product equals its sales but this only holds true if there was no out-of-stock situation
in which we only know that demand ≥ sales. Right-censored data like that provides us with information about the &lt;a href="https://en.wikipedia.org/wiki/Cumulative_distribution_function"&gt;cumulative
distribution function&lt;/a&gt; in contrast to the &lt;a href="https://en.wikipedia.org/wiki/Probability_mass_function"&gt;probability mass function&lt;/a&gt;
in case of no out-of-stock situation. There are ways to include both types of information into a &lt;span class="caps"&gt;BHM&lt;/span&gt;.
Those are just some of many possible improvements and extensions to this model. I am looking forward to your ideas and&amp;nbsp;use-cases!&lt;/p&gt;
&lt;h2&gt;Final&amp;nbsp;Remarks&lt;/h2&gt;
&lt;p&gt;We have seen that &lt;span class="caps"&gt;BHM&lt;/span&gt; allows us to combine the advantages of a pooled and unpooled model. Using some retailer&amp;#8217;s data,
we implemented a simple &lt;span class="caps"&gt;BHM&lt;/span&gt; thereby also outlining the advantages of a Bayesian approach like uncertainty quantification and
explainability. From a practical perspective, we have seen that &lt;span class="caps"&gt;BHM&lt;/span&gt; even scales really well with the help of NumPyro.
On the theoretical side, we have talked about the Poisson distribution and why we preferred the Gamma-Poisson distribution.
Finally, I hope to have conveyed the most important point of this post well, being that these models can now be applied to
practical dataset sizes with the help of NumPyro! Cheers to that and let&amp;#8217;s follow a famous saying in the French world of mathematics &lt;em&gt;Poisson sans boisson est poison&lt;/em&gt;! &lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js','color.js'], equationNumbers: { autoNumber: 'auto' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="post"></category><category term="data science"></category><category term="mathematics"></category><category term="production"></category></entry></feed>