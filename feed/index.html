<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>Florian Wilhelm</title><link>http://www.florianwilhelm.info/</link><description>Data Scientist</description><lastBuildDate>Sun, 23 Oct 2016 11:00:00 +0200</lastBuildDate><item><title>Python UDFs and UDAFs in Hive</title><link>http://www.florianwilhelm.info/2016/10/python_udf_in_hive/</link><description>&lt;p&gt;Sometimes the analytical power of &lt;a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF"&gt;built-in Hive functions&lt;/a&gt; is just not enough.
In this case it is possible to write hand-tailored User-Defined Functions (UDFs)
for transformations and even aggregations which are therefore called User-Defined
Aggregation Functions (UDAFs). In this post we focus on how to write sophisticated
UDFs and UDAFs in Python. By sophisticated we mean that our &lt;span class="caps"&gt;UD&lt;/span&gt;(A)Fs should
also be able to leverage external libraries like Numpy, Scipy, Pandas etc.
This makes things a lot more complicated since we have to provide not only some
Python script but also a full-blown virtual environment including the external
libraries. Therefore, we require only from the actual Hive setup that
a basic installation of Python is available on the data&amp;nbsp;nodes.&lt;/p&gt;
&lt;h2&gt;General&amp;nbsp;information&lt;/h2&gt;
&lt;p&gt;To keep the idea behind &lt;span class="caps"&gt;UD&lt;/span&gt;(A)Fs short, only some general notes are mentioned here.
With the help of the &lt;a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Transform"&gt;Transform/Map-Reduce syntax&lt;/a&gt;, i.e. &lt;code&gt;TRANSFORM&lt;/code&gt;, it is
possible to plug in own custom mappers and reducers. This is where we gonna hook
in our Python script. An &lt;span class="caps"&gt;UDF&lt;/span&gt; is basically only a transformation done by a mapper
meaning that each row should be mapped to exactly one row. A &lt;span class="caps"&gt;UDAF&lt;/span&gt; on the
other hand allows us to transform a group of rows into one or more rows so we
can reduce the number of input rows to a single output row by some custom
aggregation. We can control if the script is run in a mapper or reducer step
by the way we formulate our HiveQL query. The statements &lt;code&gt;DISTRIBUTE BY&lt;/code&gt; and
&lt;code&gt;CLUSTER BY&lt;/code&gt; allow us to indicate that we want to actually perform an aggregation.
HiveQL feeds its data to the Python script or any other custom script by using
the standard input and reads the result from its standard out. All messages from
standard error are ignored and can therefore be used for debugging.
Since a &lt;span class="caps"&gt;UDAF&lt;/span&gt; is more complex than a &lt;span class="caps"&gt;UDF&lt;/span&gt; and actually can be seen as a generalization
of it, the development of an &lt;span class="caps"&gt;UDAF&lt;/span&gt; is demonstrated&amp;nbsp;here.   &lt;/p&gt;
&lt;h2&gt;Overview and our little&amp;nbsp;task&lt;/h2&gt;
&lt;p&gt;In order to not get lost in the details, here is what we want to achieve from
a high-level&amp;nbsp;perspective.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Set up small example Hive table within some&amp;nbsp;database.&lt;/li&gt;
&lt;li&gt;Create a virtual environment and upload it to Hive&amp;#8217;s distributed&amp;nbsp;cache.&lt;/li&gt;
&lt;li&gt;Write the actual &lt;span class="caps"&gt;UDAF&lt;/span&gt; as Python script and a little helper shell&amp;nbsp;script.&lt;/li&gt;
&lt;li&gt;Write a HiveQL query that feeds our example table into the Python&amp;nbsp;script.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Our dummy data consists of different types of vehicles (car or bike) and a price. For
each category we want to calculate mean and the standard deviation with the help
of Pandas to keep things simple. It should not be necessary to mention that this
task can be handled in HiveQL directly, so this is really only for&amp;nbsp;demonstration.&lt;/p&gt;
&lt;h2&gt;1. Setting up our dummy&amp;nbsp;table&lt;/h2&gt;
&lt;p&gt;With the following query we generate our sample&amp;nbsp;data:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;create&lt;/span&gt; &lt;span class="k"&gt;database&lt;/span&gt; &lt;span class="n"&gt;tmp&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="n"&gt;use&lt;/span&gt; &lt;span class="n"&gt;tmp&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="k"&gt;create&lt;/span&gt; &lt;span class="k"&gt;table&lt;/span&gt; &lt;span class="n"&gt;foo&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;id&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="k"&gt;type&lt;/span&gt; &lt;span class="n"&gt;string&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;price&lt;/span&gt; &lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;span class="k"&gt;insert&lt;/span&gt; &lt;span class="k"&gt;into&lt;/span&gt; &lt;span class="k"&gt;table&lt;/span&gt; &lt;span class="n"&gt;foo&lt;/span&gt; &lt;span class="k"&gt;VALUES&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="ss"&gt;&amp;quot;car&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1000&lt;/span&gt;&lt;span class="p"&gt;.);&lt;/span&gt;
&lt;span class="k"&gt;insert&lt;/span&gt; &lt;span class="k"&gt;into&lt;/span&gt; &lt;span class="k"&gt;table&lt;/span&gt; &lt;span class="n"&gt;foo&lt;/span&gt; &lt;span class="k"&gt;VALUES&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="ss"&gt;&amp;quot;car&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;42&lt;/span&gt;&lt;span class="p"&gt;.);&lt;/span&gt;
&lt;span class="k"&gt;insert&lt;/span&gt; &lt;span class="k"&gt;into&lt;/span&gt; &lt;span class="k"&gt;table&lt;/span&gt; &lt;span class="n"&gt;foo&lt;/span&gt; &lt;span class="k"&gt;VALUES&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="ss"&gt;&amp;quot;car&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10000&lt;/span&gt;&lt;span class="p"&gt;.);&lt;/span&gt;
&lt;span class="k"&gt;insert&lt;/span&gt; &lt;span class="k"&gt;into&lt;/span&gt; &lt;span class="k"&gt;table&lt;/span&gt; &lt;span class="n"&gt;foo&lt;/span&gt; &lt;span class="k"&gt;VALUES&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="ss"&gt;&amp;quot;car&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;69&lt;/span&gt;&lt;span class="p"&gt;.);&lt;/span&gt;
&lt;span class="k"&gt;insert&lt;/span&gt; &lt;span class="k"&gt;into&lt;/span&gt; &lt;span class="k"&gt;table&lt;/span&gt; &lt;span class="n"&gt;foo&lt;/span&gt; &lt;span class="k"&gt;VALUES&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="ss"&gt;&amp;quot;bike&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1426&lt;/span&gt;&lt;span class="p"&gt;.);&lt;/span&gt;
&lt;span class="k"&gt;insert&lt;/span&gt; &lt;span class="k"&gt;into&lt;/span&gt; &lt;span class="k"&gt;table&lt;/span&gt; &lt;span class="n"&gt;foo&lt;/span&gt; &lt;span class="k"&gt;VALUES&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="ss"&gt;&amp;quot;bike&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;32&lt;/span&gt;&lt;span class="p"&gt;.);&lt;/span&gt;
&lt;span class="k"&gt;insert&lt;/span&gt; &lt;span class="k"&gt;into&lt;/span&gt; &lt;span class="k"&gt;table&lt;/span&gt; &lt;span class="n"&gt;foo&lt;/span&gt; &lt;span class="k"&gt;VALUES&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="ss"&gt;&amp;quot;bike&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1234&lt;/span&gt;&lt;span class="p"&gt;.);&lt;/span&gt;
&lt;span class="k"&gt;insert&lt;/span&gt; &lt;span class="k"&gt;into&lt;/span&gt; &lt;span class="k"&gt;table&lt;/span&gt; &lt;span class="n"&gt;foo&lt;/span&gt; &lt;span class="k"&gt;VALUES&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="ss"&gt;&amp;quot;bike&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="k"&gt;null&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Note that the last row even contains a null value we need to handle&amp;nbsp;later.&lt;/p&gt;
&lt;h2&gt;2. Creating and uploading a virtual&amp;nbsp;environment&lt;/h2&gt;
&lt;p&gt;We start by creating an empty virtual environment&amp;nbsp;with:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;virtualenv &amp;#8212;no-site-packages -p /usr/bin/python3&amp;nbsp;venv&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;assuming that &lt;code&gt;virtualenv&lt;/code&gt; was already installed with the help of pip. Note that
we explicitly ask for Python 3. Who uses Python 2 these days anyhow?
We activate the virtual environment and install Pandas in&amp;nbsp;it.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;source&amp;nbsp;venv/bin/activate&lt;/p&gt;
&lt;p&gt;pip install numpy&amp;nbsp;pandas&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This should install Pandas and all its dependencies into our virtual environment.
No we package the virtual environment for later deployment in the distributed&amp;nbsp;cache:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;cd&amp;nbsp;venv&lt;/p&gt;
&lt;p&gt;tar cvfhz ../venv.tgz&amp;nbsp;./&lt;/p&gt;
&lt;p&gt;cd&amp;nbsp;..&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Be aware that the archive was created with the actual content at its root so
when unpacking there will be no directory holding the actual content. We also
used the parameter &lt;code&gt;h&lt;/code&gt; to package linked&amp;nbsp;files.&lt;/p&gt;
&lt;p&gt;Now we push the archive to &lt;span class="caps"&gt;HDFS&lt;/span&gt; so that later Hive&amp;#8217;s data nodes will be able to
find&amp;nbsp;it:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;hdfs dfs -put venv.tgz&amp;nbsp;/tmp&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The directory &lt;code&gt;/tmp&lt;/code&gt; should be changed accordingly. One should also note that
in principle the same procedure should also be possible with conda environments. In
practice though, it might be a bit more involved since the activation of a conda
environment (what we need to do later) assumes an installation of at least
miniconda which might not be available on the data&amp;nbsp;nodes.&lt;/p&gt;
&lt;h2&gt;3. Writing and uploading the&amp;nbsp;scripts&lt;/h2&gt;
&lt;p&gt;We start by writing a simple Python script &lt;code&gt;udaf.py&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;sys&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;logging&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;itertools&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;groupby&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;operator&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;itemgetter&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pandas&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pd&lt;/span&gt;

&lt;span class="n"&gt;SEP&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="se"&gt;\t&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;
&lt;span class="n"&gt;NULL&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="se"&gt;\\&lt;/span&gt;&lt;span class="s1"&gt;N&amp;#39;&lt;/span&gt;

&lt;span class="n"&gt;_logger&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;logging&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;getLogger&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;__name__&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;read_input&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;input_data&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;line&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;input_data&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;yield&lt;/span&gt; &lt;span class="n"&gt;line&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;strip&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;split&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;SEP&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;main&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="n"&gt;logging&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;basicConfig&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;level&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;logging&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;INFO&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;stream&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;sys&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;stderr&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;read_input&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sys&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;stdin&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;vtype&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;group&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;groupby&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;itemgetter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)):&lt;/span&gt;
        &lt;span class="n"&gt;_logger&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;info&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Reading group {}...&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;vtype&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
        &lt;span class="n"&gt;group&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[(&lt;/span&gt;&lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;rowid&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;vtype&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nan&lt;/span&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;price&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;NULL&lt;/span&gt; &lt;span class="k"&gt;else&lt;/span&gt; &lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;price&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
                 &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;rowid&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;vtype&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;price&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;group&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="n"&gt;df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataFrame&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;group&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;columns&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;id&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;type&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;price&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
        &lt;span class="n"&gt;output&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;vtype&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;price&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;price&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="p"&gt;()]&lt;/span&gt;
        &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;SEP&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;join&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;o&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;o&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;output&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;


&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;__name__&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;__main__&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;main&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The script should be pretty much self-explanatory. We read from the standard
input with the help of a generator that strips and splits the lines by the
separator &amp;#8216;\t&amp;#8217;. At any point we want to avoid to have more data in memory as
needed to perform the actual computation. We use the &lt;code&gt;groupby&lt;/code&gt; function that
is shipped with Python to iterate over our two types of vehicles. For each group
we convert the read values to their respective data types and at that point
also take care of &lt;code&gt;null&lt;/code&gt; values which are encoded as &lt;code&gt;\N&lt;/code&gt;. After this preprocessing
we finally feed everything into a Pandas dataframe, do our little mean and standard
deviation calculations and print everything as a tabular separated list.
It should also be noted that we set up a logger at the beginning which writes
everything to standard error. This really helps a lot with debugging and should
be used. For demonstration purposes the vehicle type of the group currently
processed is&amp;nbsp;printed.&lt;/p&gt;
&lt;h2&gt;Finally&lt;/h2&gt;
&lt;p&gt;At this point we would actually be done if it wasn&amp;#8217;t for the fact that we are
importing external libraries like Pandas. So if we ran this Python script directly
as &lt;span class="caps"&gt;UDAF&lt;/span&gt; we would see import errors if Pandas is not installed on all cluster nodes.
But in the spirit of David Wheeler&amp;#8217;s &amp;#8220;All problems in computer science can be
solved by another level of indirection.&amp;#8221; we just write a little helper script
called &lt;code&gt;udaf.sh&lt;/code&gt; that does this job for us and calls the Python script&amp;nbsp;afterwards.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="ch"&gt;#!/bin/bash&lt;/span&gt;
&lt;span class="nb"&gt;set&lt;/span&gt; -e
&lt;span class="o"&gt;(&lt;/span&gt;&amp;gt;&lt;span class="p"&gt;&amp;amp;&lt;/span&gt;&lt;span class="m"&gt;2&lt;/span&gt; &lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Begin of script&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
&lt;span class="nb"&gt;source&lt;/span&gt; ./venv.tgz/bin/activate
&lt;span class="o"&gt;(&lt;/span&gt;&amp;gt;&lt;span class="p"&gt;&amp;amp;&lt;/span&gt;&lt;span class="m"&gt;2&lt;/span&gt; &lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Activated venv&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
./venv.tgz/bin/python3 udaf.py
&lt;span class="o"&gt;(&lt;/span&gt;&amp;gt;&lt;span class="p"&gt;&amp;amp;&lt;/span&gt;&lt;span class="m"&gt;2&lt;/span&gt; &lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;End of script&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;With the help of &lt;code&gt;chmod 755&lt;/code&gt; we make sure that it is executable and now all that&amp;#8217;s
left is to push both files somewhere on &lt;span class="caps"&gt;HDFS&lt;/span&gt; for the cluster to&amp;nbsp;find:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;hdfs dfs -put udaf.py&amp;nbsp;/tmp&lt;/p&gt;
&lt;p&gt;hdfs dfs -put udaf.sh&amp;nbsp;/tmp&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;4. Writing the actual HiveQL&amp;nbsp;query&lt;/h2&gt;
&lt;p&gt;After we are all prepared and set we can write the actual HiveQL&amp;nbsp;query:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;DELETE&lt;/span&gt; &lt;span class="n"&gt;ARCHIVE&lt;/span&gt; &lt;span class="n"&gt;hdfs&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="o"&gt;///&lt;/span&gt;&lt;span class="n"&gt;tmp&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;venv&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;tgz&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="k"&gt;ADD&lt;/span&gt; &lt;span class="n"&gt;ARCHIVE&lt;/span&gt; &lt;span class="n"&gt;hdfs&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="o"&gt;///&lt;/span&gt;&lt;span class="n"&gt;tmp&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;venv&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;tgz&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="k"&gt;DELETE&lt;/span&gt; &lt;span class="n"&gt;FILE&lt;/span&gt; &lt;span class="n"&gt;hdfs&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="o"&gt;///&lt;/span&gt;&lt;span class="n"&gt;tmp&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;udaf&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;py&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="k"&gt;ADD&lt;/span&gt; &lt;span class="n"&gt;FILE&lt;/span&gt; &lt;span class="n"&gt;hdfs&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="o"&gt;///&lt;/span&gt;&lt;span class="n"&gt;tmp&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;udaf&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;py&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="k"&gt;DELETE&lt;/span&gt; &lt;span class="n"&gt;FILE&lt;/span&gt; &lt;span class="n"&gt;hdfs&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="o"&gt;///&lt;/span&gt;&lt;span class="n"&gt;tmp&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;udaf&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sh&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="k"&gt;ADD&lt;/span&gt; &lt;span class="n"&gt;FILE&lt;/span&gt; &lt;span class="n"&gt;hdfs&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="o"&gt;///&lt;/span&gt;&lt;span class="n"&gt;tmp&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;udaf&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sh&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;

&lt;span class="n"&gt;USE&lt;/span&gt; &lt;span class="n"&gt;tmp&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="k"&gt;SELECT&lt;/span&gt; &lt;span class="k"&gt;TRANSFORM&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;id&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="k"&gt;type&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;price&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;USING&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;udaf.sh&amp;#39;&lt;/span&gt; &lt;span class="k"&gt;AS&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;type&lt;/span&gt; &lt;span class="n"&gt;STRING&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;mean&lt;/span&gt; &lt;span class="nb"&gt;FLOAT&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;var&lt;/span&gt; &lt;span class="nb"&gt;FLOAT&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="k"&gt;FROM&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;SELECT&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="k"&gt;FROM&lt;/span&gt; &lt;span class="n"&gt;foo&lt;/span&gt; &lt;span class="k"&gt;CLUSTER&lt;/span&gt; &lt;span class="k"&gt;BY&lt;/span&gt; &lt;span class="k"&gt;type&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;AS&lt;/span&gt; &lt;span class="n"&gt;TEMP_TABLE&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;At first we add the zipped virtual environment to the distributed cache that
will be automatically unpacked for us due to the &lt;code&gt;ADD ARCHIVE&lt;/code&gt; command.
Then we upload the Python and helper script. To make sure the current version
in the cache is actually the latest, so in case changes are made, we
prepended &lt;code&gt;DELETE&lt;/code&gt; statements before each &lt;code&gt;ADD&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The actual query now calls &lt;code&gt;TRANSFORM&lt;/code&gt; with the three input column we expect
in our Python script. After the &lt;code&gt;USING&lt;/code&gt; statement our helper script is provided
as the actual &lt;span class="caps"&gt;UDAF&lt;/span&gt; seen by HiveQL. This is followed by &lt;code&gt;AS&lt;/code&gt; defining the names
and types of the output&amp;nbsp;columns.&lt;/p&gt;
&lt;p&gt;At this point we need to make sure that the script is executed in a reducer step.
We assure this by defining a subselect that reads from our &lt;code&gt;foo&lt;/code&gt; table and clusters
by the &lt;code&gt;type&lt;/code&gt;. &lt;code&gt;CLUSTER BY&lt;/code&gt; which is a shortcut for &lt;code&gt;DISTRIBUTE BY&lt;/code&gt; followed by
&lt;code&gt;SORT BY&lt;/code&gt; asserts that rows having the same &lt;code&gt;type&lt;/code&gt; column are also located on
the same reducer. Furthermore, the implicit &lt;code&gt;SORT BY&lt;/code&gt; orders within a reducer
the rows with respect to the &lt;code&gt;type&lt;/code&gt; column. The overall result are consecutive
partitions of a given type (car and bike in our case) whereas each partition resides
on a single reducer. Finally, our script is fed the whole data on a single reducer
and needs to figure out itself where one partition ends and another one starts
(what we did with &lt;code&gt;itertools.groupby&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;Since our little task is now accomplished, it should also be noted that there
are some more Python libraries one should know when working with Hive.
To actually execute the HiveQL query we have written with the help of Python, there
is &lt;a href="https://github.com/cloudera/impyla"&gt;impyla&lt;/a&gt; by Cloudera with supports Python 3 in contrast to &lt;a href="https://github.com/dropbox/PyHive"&gt;PyHive&lt;/a&gt; by Dropbox.
In order to work with &lt;span class="caps"&gt;HDFS&lt;/span&gt; the best library around is &lt;a href="https://hdfs3.readthedocs.io/"&gt;hdfs3&lt;/a&gt;. That would
for instance allow us to push changes in &lt;code&gt;udaf.py&lt;/code&gt; automatically with a Python&amp;nbsp;script.&lt;/p&gt;
&lt;p&gt;Have fun hacking Hive with the power of&amp;nbsp;Python!&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Florian Wilhelm</dc:creator><pubDate>Sun, 23 Oct 2016 11:00:00 +0200</pubDate><guid isPermaLink="false">tag:www.florianwilhelm.info,2016-10-23:2016/10/python_udf_in_hive/</guid><category>python</category><category>hadoop</category><category>hive</category><category>big data</category></item><item><title>Handling GPS Data with Python</title><link>http://www.florianwilhelm.info/2016/07/handling_gps_data_with_python/</link><description>&lt;p&gt;This talk presented at the &lt;a href="https://ep2016.europython.eu/conference/talks/handling-gps-data-with-python"&gt;EuroPython 2016&lt;/a&gt; introduces several Python libraries
related to the handling of &lt;span class="caps"&gt;GPS&lt;/span&gt; data. The slides of this talk are available on
&lt;a href="https://github.com/FlorianWilhelm/gps_data_with_python"&gt;Github&lt;/a&gt; or on &lt;a href="http://nbviewer.jupyter.org/format/slides/github/FlorianWilhelm/gps_data_with_python/blob/master/talk.ipynb#/"&gt;nbviewer&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If you have ever happened to need to deal with &lt;span class="caps"&gt;GPS&lt;/span&gt; data in Python you may have
felt a bit lost. There are many libraries at various states of maturity and scope.
Finding a place to start and to actually work with the &lt;span class="caps"&gt;GPS&lt;/span&gt; data might not be as
easy and obvious as you might expect from other Python&amp;nbsp;domains.&lt;/p&gt;
&lt;p&gt;Inspired from my own experiences of dealing with &lt;span class="caps"&gt;GPS&lt;/span&gt; data in Python, I want to
give an overview of some useful libraries. From basic reading and writing &lt;span class="caps"&gt;GPS&lt;/span&gt;
tracks in the &lt;span class="caps"&gt;GPS&lt;/span&gt; Exchange Format with the help of gpxpy to adding missing
elevation information with srtm.py. Additionally, I will cover mapping and
visualising tracks on OpenStreetmap with mplleaflet that even supports
interactive plots in a Jupyter&amp;nbsp;notebook.&lt;/p&gt;
&lt;p&gt;Besides the tooling, I will also demonstrate and explain common algorithms like
Douglas-Peucker to simplify a track and the famous Kalman filters for smoothing.
For both algorithms I will give an intuition about how they work as well as their
basic mathematical concepts. Especially the Kalman filter that is used for all
kinds of sensor, not only &lt;span class="caps"&gt;GPS&lt;/span&gt;, has the reputation of being hard to understand.
Still, its concept is really easy and quite comprehensible as I will also
demonstrate by presenting an implementation in Python with the help of Numpy and
Scipy. My presentation will make heavy use of the Jupyter notebook which is a
wonderful tool perfectly suited for experimenting and&amp;nbsp;learning.&lt;/p&gt;
&lt;p&gt;&lt;div class="videobox"&gt;
                &lt;iframe width="800" height="500"
                    src='https://www.youtube.com/embed/9Q8nEA_0ccg'
                    frameborder='0' webkitAllowFullScreen mozallowfullscreen
                    allowFullScreen&gt;
                &lt;/iframe&gt;
            &lt;/div&gt;&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Florian Wilhelm</dc:creator><pubDate>Fri, 22 Jul 2016 11:15:00 +0200</pubDate><guid isPermaLink="false">tag:www.florianwilhelm.info,2016-07-22:2016/07/handling_gps_data_with_python/</guid><category>python</category><category>jupyter</category><category>kalman</category><category>gps</category></item><item><title>Leveraging the Value of Big Data with Automated Decision Making</title><link>http://www.florianwilhelm.info/2016/04/leveraging_the_value_of-big_data_with_automated_decision_making/</link><description>&lt;p&gt;It is a widely accepted fact that we are living in the era of Big Data. Many
traditional companies are looking for ways to improve their business through
the virtues of Big Data and Data Science. While matured startups born in this
era like Facebook and Twitter seem to naturally exploit the value of their data,
many traditional companies struggle to find new ways of utilizing their data to
leverage its value for their classical&amp;nbsp;businesses.&lt;/p&gt;
&lt;p&gt;In this post I elaborate on the specific domain of decision making where Big Data
and Data Science can help to improve the efficiency of conventional businesses.
Proving the benefits of Big Data in a lighthouse project is of utmost importance
in long-established companies with regard to overcoming initial resistance in
the digital transformation of business processes. We will see that the
automatization of operational decisions, i.e. routine decisions related to the
day-to-day running of the business, are especially suitable candidates for
lighthouse projects to prove the value of Big Data in a&amp;nbsp;company.&lt;/p&gt;
&lt;p&gt;The notion of automating data-driven decisions with the help of Data Science is
often denoted with the term Prescriptive Analytics, which can be regarded as the
conclusive step after Predictive Analytics. In other words, the predictions
generated with the help of Predictive Analytics are used to optimize a predefined
metric under consideration of side conditions, strategic direction, business
processes etc. to derive excellent business decisions. The
&lt;a href="http://www.gartner.com/it-glossary/predictive-analytics/"&gt;predictive analytics diagram from Gartner&lt;/a&gt; illustrates the business value
compared to the difficulty of different analytical&amp;nbsp;approaches.&lt;/p&gt;
&lt;p&gt;In many businesses repetitive operational decisions consume lots of working time.
For instance pricing of articles and services, replenishment of stores or stocks,
demand forecasts and customer services involve operational decisions which are
often conducted in a manual process supported by traditional, rule based decision
support systems. Automating these decisions with the help of data-driven decision
systems has several&amp;nbsp;benefits:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Labor costs are reduced and scarce expertise can be leveraged for non-routine,
exceptional decisions. Less routine decisions means having more time for decisions
in extraordinary circumstances as well as decisions that are of a more tactical
or strategical nature. This encompasses also decisions in situations where data
is lacking as well as decisions about creative and visionary solutions. For
instance, no machine learning algorithm could have ever predicted the success of
the first iPhone since it was something completely new and its success was a
consequence of not only that but also many other soft&amp;nbsp;factors.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The quality of decisions is improved given that all information sources used in
the manual decision process are available as machine readable data. Modern machine
learning algorithm are able to quickly analyze huge amounts of data that a human
being could never even read in a lifetime. This plethora of data allows the
inference of patterns that lead to fast, consistent, high quality decisions which
are resistant to the &lt;a href="https://en.wikipedia.org/wiki/List_of_cognitive_biases"&gt;long list of cognitive biases&lt;/a&gt; or &lt;a href="https://en.wikipedia.org/wiki/Decision_fatigue"&gt;decision fatigue&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Prescriptive Analytics allows to scale the number of decisions. Too often in
traditional businesses, decisions are made by not actually taking a decision which
consequently leads to idleness and thus unexploited potential. Being able to scale
the number of decisions, enables this untapped potential to be fully realized and
can also generate new services. Imagine for instance that one marketing tool of a
company is to give special offerings and product recommendations based on different
market segments, not single customers. Being able to scale the number of decisions
due to automation would allow special offerings and recommendations for individual
customers, just like Amazon’s recommendation&amp;nbsp;system.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;To justify our statement that the automation of routine decisions with Prescriptive
Analytics is exceptionally well suited as a pioneer project in a traditional company,
it is necessary to elaborate on certain characteristics that many operational decisions&amp;nbsp;hold.&lt;/p&gt;
&lt;p&gt;While a single operational decision, e.g. a small change in the price of a single
article, may have an insignificantly small but direct impact on the revenue of
the whole business, the sum of all decisions quite often has great economic impact.
This is due to the fact that the frequency of operational decisions is often huge,
meaning that a small overall improvement in decision quality is highly profitable.
Obviously, candidates for a Prescriptive Analytics project should have exactly
these properties of high and direct economic impact. The ability to measure such
an impact requires that a performance metric or key performance indicator (&lt;span class="caps"&gt;KPI&lt;/span&gt;)
is already established. This is another important prerequisite for a successful
Prescriptive Analytics&amp;nbsp;project.&lt;/p&gt;
&lt;p&gt;Since operational decisions are often related to the core of the business even in
traditional companies huge amounts of data are already collected and available.
Often, routine decisions that are taken by analyzing spreadsheets and personal
experience are based on data with high predictive power. A quote, often attributed
to Mark Twain, says that “history doesn’t repeat itself, but it does rhyme” which
captures the essence of what automated decision making is about. Having lots of
data about past events allows us to find patterns and relationships which can
predict future events to some extent. The goal is to develop a model that describes
what happened in the past without being bound to the past and thus allowing to
apply the model to the future. In just the same way as our brain learns from
experiences and infers future outcomes in similar&amp;nbsp;situations.&lt;/p&gt;
&lt;p&gt;Consequently, the high frequency of routine decision with a direct economic impact
combined with an abundance of data and a metric to measure performance are
favorable characteristic of a business process that can be successfully automated.
In order to quantify the added value of Prescriptive Analytics an estimation of
the gain in decision quality and its impact on revenue is needed with the help
of the predefined metric or &lt;span class="caps"&gt;KPI&lt;/span&gt;. For this complex estimation it is recommended
for traditional companies to have an experienced partner alongside and optionally
a proof of concept to evaluate the predictive power of the data and the business
case as a&amp;nbsp;whole.&lt;/p&gt;
&lt;p&gt;We should not ignore the fact that automation also includes costs encompassing the
maintenance, licence &lt;span class="amp"&gt;&amp;amp;&lt;/span&gt; support etc. of an established automated decision system. The
automation costs depend mainly on the order of magnitude of performed decisions as well
as the time frame. For instance the &lt;span class="caps"&gt;IT&lt;/span&gt; setup of a decision system for one million decisions
per day will be much smaller than the setup for one billion that need to be determined in real-
time. The initial costs for the implementation of an automated decision system varies largely
depending on the domain of application, the necessary changes in the business processes
and other factors. An estimation of these costs is needed to determine the time-to-value.
Since the added value of an automated decision system quite often heavily surpasses
automation costs by at least one order of magnitude time-to-value is often&amp;nbsp;low.&lt;/p&gt;
&lt;p&gt;We conclude that the scaling in the number of decisions and the improved effectiveness of
the decisions are the main drivers of the added value in automated decision making.
Subsequently, operational decisions that are ubiquitous and directly influence the business
value are well suited for a Predictive Analytics light-house&amp;nbsp;project.&lt;/p&gt;
&lt;p&gt;Following mnemonic recaps the main qualities of a successful Prescriptive Analytics project.
It consists of the following questions that should be answered&amp;nbsp;positively:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Are decisions taken &lt;strong&gt;F&lt;/strong&gt;requently?&lt;/li&gt;
&lt;li&gt;Does the business process allow the &lt;strong&gt;A&lt;/strong&gt;utomation of&amp;nbsp;decisions?&lt;/li&gt;
&lt;li&gt;Are &lt;strong&gt;M&lt;/strong&gt;etrics defined to determine the quality of a&amp;nbsp;decision?&lt;/li&gt;
&lt;li&gt;Do decisions have a direct &lt;strong&gt;E&lt;/strong&gt;conomic&amp;nbsp;impact?&lt;/li&gt;
&lt;li&gt;Is enough and suitable &lt;strong&gt;D&lt;/strong&gt;ata available to base decisions&amp;nbsp;on?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Prescriptive Analytics projects with these properties are very likely to become &lt;strong&gt;famed&lt;/strong&gt;
in your company. The successful implementation of a lighthouse project in the business process
generates momentum for new projects. This drives the digital transformation of a classical
business in an iterative&amp;nbsp;way.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;This article was originally posted on the &lt;a href="http://www.21stcenturyit.de/leveraging-the-value-of-big-data-with-automated-decision-making/"&gt;&lt;span class="caps"&gt;CSC&lt;/span&gt; 21st Century &lt;span class="caps"&gt;IT&lt;/span&gt; blog&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Florian Wilhelm</dc:creator><pubDate>Thu, 07 Apr 2016 12:00:00 +0200</pubDate><guid isPermaLink="false">tag:www.florianwilhelm.info,2016-04-07:2016/04/leveraging_the_value_of-big_data_with_automated_decision_making/</guid><category>big data</category><category>data science</category><category>predictive analytics</category></item><item><title>Interactively visualizing distributions in a Jupyter notebook with Bokeh</title><link>http://www.florianwilhelm.info/2016/03/jupyter_distribution_visualizer/</link><description>&lt;p&gt;If you are doing probabilistic programming you are dealing with all kinds of
different distributions. That means choosing an ensemble of right distributions
which describe the underlying real-world process in a suitable way but also
choosing the right parameters for prior distributions. At that point I often
start visualizing the distributions with the help of &lt;a href="http://jupyter.org/"&gt;Jupyter&lt;/a&gt; notebooks,
&lt;a href="http://matplotlib.org/"&gt;matplotlib&lt;/a&gt; and &lt;a href="http://www.scipy.org/"&gt;SciPy&lt;/a&gt; to get a feeling how the distribution behaves when
changing its parameters. And please don&amp;#8217;t tell me you are able to visualize all the
distributions &lt;a href="http://docs.scipy.org/doc/scipy/reference/stats.html"&gt;scipy.stats&lt;/a&gt; has to offer just in your&amp;nbsp;head.&lt;/p&gt;
&lt;p&gt;For me, this surely is a repetitive task that every good and lazy programmer tries
to avoid. Additionally, I was never quite satisfied with the interactivity of
matplotlib in a notebook. Granted, the &lt;code&gt;%matplotlib notebook&lt;/code&gt; magic was a huge
step into the right direction but there is still much room for improvement.
The new and shiny kid on the block is &lt;a href="http://bokeh.pydata.org/"&gt;Bokeh&lt;/a&gt; and so far I have not really done
much with it, meaning it is a good candidate for a test ride. The same goes
actually for Jupyter&amp;#8217;s &lt;a href="http://ipywidgets.readthedocs.org/"&gt;ipywidgets&lt;/a&gt; and you see where this going. No evaluation
of a tool without a proper goal and that is now set to developing an interactive
visualization widget for Jupyter based on Bokeh and ipywidgets. So here we&amp;nbsp;go!&lt;/p&gt;
&lt;p&gt;It turned out that this task is easier than expected due the good documentation
and examples of ipywidgets and especially Bokeh. You can read all about the
implementation inside this &lt;a href="https://github.com/FlorianWilhelm/distvis/blob/master/index.ipynb"&gt;notebook&lt;/a&gt; which is hosted in a separate
&lt;a href="https://github.com/FlorianWilhelm/distvis"&gt;Github repository&lt;/a&gt;. This also always me to make use of a new service that I
just recently learned about, &lt;a href="http://mybinder.org/"&gt;binder&lt;/a&gt;. This totally rad service takes any
Github repository with a Jupyter notebook in it, fires up a container with Kubernetes,
installs necessary requirements and finally runs your notebook! By just clicking
on a link! Amazing to see how the ecosystem around Jupyter develops these&amp;nbsp;days.&lt;/p&gt;
&lt;p&gt;And of course to wet your appetite, here are the screenshots of the final tool
that you will experience interactively by &lt;a href="http://mybinder.org/repo/FlorianWilhelm/distvis"&gt;starting the notebook with binder&lt;/a&gt;.&lt;/p&gt;
&lt;figure&gt;
&lt;img class="noZoom" src="/images/alpha_dist.png" alt="Alpha distribution"&gt;
&lt;figcaption&gt;The probability density function of a continuous alpha distribution with shape parameter a=1.3&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;
&lt;figure&gt;
&lt;img class="noZoom" src="/images/binom_dist.png" alt="Binomial distribution"&gt;
&lt;figcaption&gt;The probability mass function of a discrete binomial distribution with shape parameters n=10 and p=0.7&lt;/figcaption&gt;
&lt;/figure&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Florian Wilhelm</dc:creator><pubDate>Sat, 26 Mar 2016 09:00:00 +0100</pubDate><guid isPermaLink="false">tag:www.florianwilhelm.info,2016-03-26:2016/03/jupyter_distribution_visualizer/</guid><category>jupyter</category><category>python</category><category>scipy</category><category>bokeh</category></item><item><title>Explaining the Idea behind ARD and Bayesian Interpolation</title><link>http://www.florianwilhelm.info/2016/03/explaining_the_idea_behind_ard/</link><description>&lt;p&gt;This talk presented at the &lt;a href="http://pydata.org/amsterdam2016/schedule/presentation/17/"&gt;PyData Amsterdam 2016&lt;/a&gt; explains the idea of Bayesian
model selection techniques, especially the Automatic Relevance Determination.
The slides of this talk are available on &lt;a href="http://www.slideshare.net/FlorianWilhelm2/explaining-the-idea-behind-automatic-relevance-determination-and-bayesian-interpolation-59498957"&gt;SlideShare&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Even in the era of Big Data there are many real-world problems where the number
of input features has about the some order of magnitude than the number of samples.
Often many of those input features are irrelevant and thus inferring the relevant
ones is an important problem in order to prevent over-fitting. Automatic Relevance
Determination solves this problem by applying Bayesian&amp;nbsp;techniques.&lt;/p&gt;
&lt;p&gt;In order to motivate Automatic Relevance Determination (&lt;span class="caps"&gt;ARD&lt;/span&gt;) an intuition for
the problem of choosing a complex model that fits the data well vs a simple model
that generalizes well is established. Thereby the idea behind Occam&amp;#8217;s razor is
presented as a way of balancing bias and variance. This leads us to the mathematical
framework of Bayesian interpolation and model selection to choose between different
models based on the&amp;nbsp;data.&lt;/p&gt;
&lt;p&gt;To derive &lt;span class="caps"&gt;ARD&lt;/span&gt; as gently as possible the mathematical basics of a simple linear model
are repeated as well as the idea of regularization to prevent over-fitting.
Based on that, the Bayesian Ridge Regression (BayesianRidge in Scikit-Learn) is
introduced. Generalizing the concept of Bayesian Ridge Regression even more gets
us eventually to the the idea behind &lt;span class="caps"&gt;ARD&lt;/span&gt; (ARDRegression in&amp;nbsp;Scikit-Learn).&lt;/p&gt;
&lt;p&gt;With the help of a practical example, we consolidate what has been learned so far
and compare &lt;span class="caps"&gt;ARD&lt;/span&gt; to an ordinary least square model. Now we dive deep into the
mathematics of &lt;span class="caps"&gt;ARD&lt;/span&gt; and present the algorithm that solves the minimization problem
of &lt;span class="caps"&gt;ARD&lt;/span&gt;. Finally, some details of Scikit-Learn&amp;#8217;s &lt;span class="caps"&gt;ARD&lt;/span&gt; implementation are&amp;nbsp;discussed.&lt;/p&gt;
&lt;p&gt;&lt;div class="videobox"&gt;
                &lt;iframe width="800" height="500"
                    src='https://www.youtube.com/embed/2gT-Q0NZzoE'
                    frameborder='0' webkitAllowFullScreen mozallowfullscreen
                    allowFullScreen&gt;
                &lt;/iframe&gt;
            &lt;/div&gt;&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Florian Wilhelm</dc:creator><pubDate>Sun, 13 Mar 2016 22:00:00 +0100</pubDate><guid isPermaLink="false">tag:www.florianwilhelm.info,2016-03-13:2016/03/explaining_the_idea_behind_ard/</guid><category>scikit-learn</category><category>machine-learning</category><category>bayesian</category></item></channel></rss>