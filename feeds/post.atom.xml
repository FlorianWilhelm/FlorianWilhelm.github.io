<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Florian Wilhelm - post</title><link href="https://florianwilhelm.info/" rel="alternate"></link><link href="https://florianwilhelm.info/feeds/post.atom.xml" rel="self"></link><id>https://florianwilhelm.info/</id><updated>2020-10-01T08:00:00+02:00</updated><entry><title>Finally! Bayesian Hierarchical Modelling atÂ Scale</title><link href="https://florianwilhelm.info/2020/10/bayesian_hierarchical_modelling_at_scale/" rel="alternate"></link><published>2020-10-01T08:00:00+02:00</published><updated>2020-10-01T08:00:00+02:00</updated><author><name>Florian Wilhelm</name></author><id>tag:florianwilhelm.info,2020-10-01:/2020/10/bayesian_hierarchical_modelling_at_scale/</id><summary type="html">&lt;p&gt;For a long time, Bayesian Hierarchical Modelling has been a very powerful tool that sadly could not be applied often due to its high computations costs. With NumPyro and the latest advances in high-performance computations in Python, Bayesian Hierarchical Modelling is now ready for prime&amp;nbsp;time.&lt;/p&gt;</summary><content type="html">&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Since the advent of deep learning, everything is or has to be about &lt;em&gt;Artificial Intelligence&lt;/em&gt;, so it seems. Even software which is applying traditional
techniques from e.g. instrumentation and control engineering, is nowadays considered &lt;em&gt;&lt;span class="caps"&gt;AI&lt;/span&gt;&lt;/em&gt;. For instance, the famous robots
of Boston Dynamics are not based on deep reinforcement learning as many people think but much more traditional engineering
methods. This hype around &lt;span class="caps"&gt;AI&lt;/span&gt;, which is very often equated with deep learning, seems to draw that much attention such that
great advances of more traditional methods seem to go almost completely unnoticed. In this blog post, I want to draw your 
attention to the somewhat dusty &lt;em&gt;Bayesian Hierarchical Modelling&lt;/em&gt;. Modern techniques and frameworks allow you to finally apply this
cool method on datasets with sizes much bigger than what was possible before and thus letting it really&amp;nbsp;shine.&lt;/p&gt;
&lt;p&gt;So for starters, what is &lt;em&gt;Bayesian Hierarchical Modelling&lt;/em&gt; and why should I care? I assume you already have a basic knowledge about
Bayesian inference, otherwise &lt;a href="https://github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers"&gt;Probabilistic Programming and Bayesian Methods for Hackers&lt;/a&gt;
is a really good starting point to explore the Bayesian rabbit hole. In simple words, Bayesian inference allows you to define
a model with the help of probability distributions and also incorporate your prior knowledge about the parameters of your model.
This leads to a &lt;em&gt;directed acyclic graphical model&lt;/em&gt; (aka Bayesian network) which is explainable, visual and easy to reason about.
But that&amp;#8217;s not even everything, you also get &lt;a href="https://en.wikipedia.org/wiki/Uncertainty_quantification"&gt;Uncertainty Quantification&lt;/a&gt; for free, 
meaning that the model&amp;#8217;s parameters are not mere point estimates but whole distributions telling you how certain you are
about their&amp;nbsp;values. &lt;/p&gt;
&lt;p&gt;A classical statistical method that most data scientists learn about early on is &lt;em&gt;linear regression&lt;/em&gt;. It can also
be interpreted in a Bayesian way giving you the possibility to define prior knowledge about the parameters, e.g.
that they have to be close to zero or that they are non-negative. Then again, many of the priors you might come up with could also 
be seen as mere regularizers in a non-Bayesian way, and treated like that, often efficient techniques exist to solve such formulations.
So where the Bayesian framework now really shines is, if you consider the following problem setting I stole from the wonderful 
presentation &lt;a href="https://www.youtube.com/watch?v=WbNmcvxRwow"&gt;A Bayesian Workflow with PyMC and ArviZ&lt;/a&gt; by Corrie&amp;nbsp;Bartelheimer.&lt;/p&gt;
&lt;p&gt;Imagine you want to estimate the price of an apartment in Berlin given its living area in square meters and district. Making a linear
regression with all data points you have neglecting the districts, i.e. a &lt;em&gt;pooled model&lt;/em&gt;, will lead to a robust estimation of the slope and intercept
but a wide residual distribution. This is due to the fact that the price of an apartment also heavily depends on the district it is
located in. Now grouping your data with respect to the respective districts and making a linear regression for each,
i.e. an &lt;em&gt;unpooled model&lt;/em&gt;, will lead to a much more narrow residual distribution but also a high uncertainty in your parameters since
some district might only have three data points. To combine the advantages of a pooled and unpooled model, one
would intuitively demand that for each district the prior knowledge of the parameter from the pooled model should be used
and updated according to the data we have about a certain district. If we have only a few data points we would only allow to
deviate a bit from our prior knowledge about the parameter. In case we have lots of data points, the parameter for the
respective district should be allowed to have a huge difference compared to the parameter of the pooled model. Thus the pooled model
acts as an informed prior for the parameters within the unpooled model leading altogether to an &lt;em&gt;hierarchical model&lt;/em&gt;,
which is sometimes also referred to as &lt;em&gt;partially pooled model&lt;/em&gt;. Figure 1 illustrates our thoughts so&amp;nbsp;far.&lt;/p&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;
&lt;figure&gt;
&lt;img class="noZoom" src="/images/hierarchical_model.png" alt="pooled, unpooled, hierarchical model"&gt;
&lt;figcaption align="center"&gt;&lt;strong&gt;Figure 1:&lt;/strong&gt; Hierarchical model as a combination of a pooled and an unpooled model 
from &lt;a href="https://widdowquinn.github.io/Teaching-Stan-Hierarchical-Modelling/07-partial_pooling_intro.html"&gt;Bayesian Multilevel Modelling using PyStan&lt;/a&gt;.
&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;
&lt;h2&gt;Recent&amp;nbsp;Advances&lt;/h2&gt;
&lt;p&gt;So far I mostly used &lt;a href="https://docs.pymc.io/"&gt;PyMC3&lt;/a&gt; for Bayesian inference or &lt;em&gt;probabilistic programming&lt;/em&gt; as the authors
of PyMC3 like to call it. I love it for it&amp;#8217;s elegant design and consequently its expressiveness. The documentation is great
and thus you can pretty much hack away with your model ideas. The only problem I always had with it is that for me it never
scaled so well with somewhat larger datasets, i.e. more than 100k data points, and a larger number of parameters. There is a technical and 
methodical reason for it. Regarding the former, PyMC3 uses &lt;a href="http://deeplearning.net/software/theano/"&gt;Theano&lt;/a&gt; to speed
up its computations by transpiling your Python code to C. Theano inspired many frameworks like &lt;a href="https://www.tensorflow.org/"&gt;Tensorflow&lt;/a&gt; 
and &lt;a href="https://pytorch.org/"&gt;PyTorch&lt;/a&gt; but is considered deprecated today and cannot rival the speed of modern frameworks
anymore. For the latter, I used PyMC3 mostly with &lt;a href="https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo"&gt;Markov chain Monte Carlo&lt;/a&gt; (&lt;span class="caps"&gt;MCMC&lt;/span&gt;) based methods, 
which are sampling algorithms and thus computationally quite demanding, while &lt;a href="https://en.wikipedia.org/wiki/Variational_Bayesian_methods"&gt;variational inference (&lt;span class="caps"&gt;VI&lt;/span&gt;)&lt;/a&gt;
methods are much faster. But also when using &lt;span class="caps"&gt;VI&lt;/span&gt;, which PyMC3 also supports, it never really allowed me to deal with larger datasets rendering
Bayesian Hierarchical Modelling (&lt;span class="caps"&gt;BHM&lt;/span&gt;) a wonderful tool that sadly could not be applied in many suitable projects due to its computational&amp;nbsp;costs.&lt;/p&gt;
&lt;p&gt;Luckily, the world of data science moves on with an incredible speed, and some time ago I had a nice project at my hand that
could make good use of &lt;span class="caps"&gt;BHM&lt;/span&gt;. Thus, I gave it another shot and also looked beyond PyMC3. My first candidate to evaluate was &lt;a href="http://pyro.ai/"&gt;Pyro&lt;/a&gt;,
which uses Stochastic Variational Inference (&lt;span class="caps"&gt;SVI&lt;/span&gt;) by default, and calls itself a &lt;em&gt;deep universal probabilistic programming&lt;/em&gt; framework.
Instead of Theano it is based on PyTorch and thus allows for &lt;a href="https://en.wikipedia.org/wiki/Just-in-time_compilation"&gt;just-in-time (&lt;span class="caps"&gt;JIT&lt;/span&gt;) compilation&lt;/a&gt;,
which sped up my test case already quite a bit. Pyro also emphasizes vectorization, thus allowing for fast parallel computation, e.g. &lt;a href="https://en.wikipedia.org/wiki/SIMD"&gt;&lt;span class="caps"&gt;SIMD&lt;/span&gt;&lt;/a&gt; operations.
In total the speed-up compared to PyMC3 was amazing in my test-case letting me almost forget the two downsides of Pyro compared
to PyMC3. Firstly, the documentation of Pyro is not as polished and secondly, it&amp;#8217;s just so much more complicated to use and understand 
but your mileage may vary on that&amp;nbsp;one. &lt;/p&gt;
&lt;p&gt;Digging through the website of Pyro I then stumbled over &lt;a href="https://github.com/pyro-ppl/numpyro"&gt;NumPyro&lt;/a&gt; that has a similar
interface compared to Pyro but uses &lt;a href="https://github.com/google/jax"&gt;&lt;span class="caps"&gt;JAX&lt;/span&gt;&lt;/a&gt; instead of PyTorch as its backend. &lt;span class="caps"&gt;JAX&lt;/span&gt; is like &lt;a href="https://numpy.org/"&gt;NumPy&lt;/a&gt; on 
steroids. It&amp;#8217;s crazy fast as it uses &lt;a href="https://www.tensorflow.org/xla"&gt;&lt;span class="caps"&gt;XLA&lt;/span&gt;&lt;/a&gt;, which is a domain-specific compiler for linear algebra
operations. Additionally, it allows for automatic differentiation like &lt;a href="https://github.com/hips/autograd"&gt;Autograd&lt;/a&gt;,
whose maintainers moved over to develop &lt;span class="caps"&gt;JAX&lt;/span&gt; further. Long story short, NumPyro even blew the benchmark results of Pyro out of the water.
For the first time (at least for what I know), NumPyro allows you do Bayesian inference with lots of parameters like in
&lt;span class="caps"&gt;BHM&lt;/span&gt; on large data! In the rest of this post, I want to show you how NumPyro can be applied in a typical demand prediction
use-case on some public dataset. The dataset in my actual use-case was much bigger, my model had more parameters and NumPyro could still handle it but you just have to trust me on this one ;-)
Hopefully some readers will find this post useful and maybe it mitigates a bit the pain coming from the lack of NumPyro&amp;#8217;s documentation and&amp;nbsp;examples.&lt;/p&gt;
&lt;h2&gt;Use-Case &lt;span class="amp"&gt;&amp;amp;&lt;/span&gt;&amp;nbsp;Modelling&lt;/h2&gt;
&lt;p&gt;Imagine you have many retail stores and want to make individual demand predictions for them. For stores that were opened
a long time ago, this should be no problem but how do you deal with stores that first opened a week ago or even will open soon? Like in the example
of apartment prices in different districts, &lt;span class="caps"&gt;BHM&lt;/span&gt; helps you to deal exactly with this &lt;a href="https://en.wikipedia.org/wiki/Cold_start_(recommender_systems)"&gt;cold start problem&lt;/a&gt;. We take the 
&lt;a href="https://www.kaggle.com/c/rossmann-store-sales"&gt;Rossmann dataset from Kaggle&lt;/a&gt; to simulate this problem by removing the data
of some of the stores. The data consists of a &lt;em&gt;train&lt;/em&gt; dataset with information about the sales and daily features of the stores,
e.g. if a promotion happened (&lt;code&gt;promo&lt;/code&gt;), as well as a &lt;em&gt;store&lt;/em&gt; dataset with time-independent store features.
Here&amp;#8217;s what we wanna do in our little experiment and study&amp;nbsp;protocol:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;join the data from Kaggle&amp;#8217;s &lt;code&gt;train.csv&lt;/code&gt; dataset with the general store features from the &lt;code&gt;store.csv&lt;/code&gt; dataset,&lt;/li&gt;
&lt;li&gt;perform some really basic feature engineering and encoding of the categorical&amp;nbsp;features,&lt;/li&gt;
&lt;li&gt;split the data into train and test where we treat the stores from train as being opened for a long time and the ones
  from test as newly&amp;nbsp;opened,&lt;/li&gt;
&lt;li&gt;fit our hierarchical model on the train dataset to infer the &amp;#8220;global&amp;#8221; parameters of the upper model&amp;nbsp;hierarchy,&lt;/li&gt;
&lt;li&gt;take only the first 7 days for each store in the test data, which we assume to know, and fit our model only inferring
 the local, i.e. store-specific, parameters of the lower hierarchy while keeping the global ones&amp;nbsp;fixed,&lt;/li&gt;
&lt;li&gt;compare the inferred parameters of a test store to:&lt;ol&gt;
&lt;li&gt;the inferred local parameters of a simple Poisson model. We expect them to be completely different due to the lack
   of data and thus overfitting of the Poisson&amp;nbsp;model,&lt;/li&gt;
&lt;li&gt;the inferred local parameters of our model if we had given it the whole time series from test, i.e. not only the first 7 days.
   In this case, we assume that we are already pretty close since the priors given by the global parameters nudge them
   in the right direction even with only little&amp;nbsp;data.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;All code of this little experiment can be found under my &lt;a href="https://github.com/FlorianWilhelm/bhm-at-scale"&gt;bhm-at-scale repository&lt;/a&gt;
so that you can follow along easily.
The steps 1-3 are performed in the &lt;a href="https://github.com/FlorianWilhelm/bhm-at-scale/blob/master/notebooks/01-preprocessing.ipynb"&gt;preprocessing notebook&lt;/a&gt;
and are actually not that interesting, thus we will skip it here. Steps 4-6 are performed in the
&lt;a href="https://github.com/FlorianWilhelm/bhm-at-scale/blob/master/notebooks/02-model.ipynb"&gt;model notebook&lt;/a&gt; while some visualisations are presented in the &lt;a href="https://github.com/FlorianWilhelm/bhm-at-scale/blob/master/notebooks/03-evaluation.ipynb"&gt;evaluation notebook&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;But before we start to get technical, let&amp;#8217;s take a minute and frame again the forecasting problem from a more mathematical side.
The data of each store is a time-series of feature vectors and target scalars. We want to find a mapping such that the
feature vector of each time-step is mapped to a value close to the target scalar of the respective time-step. Since our target
value, i.e. the number of sales, is a non-negative integer we could assume a &lt;a href="https://en.wikipedia.org/wiki/Poisson_distribution"&gt;Poisson distribution&lt;/a&gt; and consequently
perform a &lt;a href="https://en.wikipedia.org/wiki/Poisson_regression"&gt;Poisson regression&lt;/a&gt; in a hierarchical way. This would be kind of okay
if we were only interested in a point estimation and thus would not care about the variance of the predictive posterior distribution. 
The Poisson distribution only has one parameter &lt;span class="math"&gt;\(\lambda\)&lt;/span&gt; that allows you to define the mean &lt;span class="math"&gt;\(\mu\)&lt;/span&gt; while the variance &lt;span class="math"&gt;\(\sigma^2\)&lt;/span&gt; then just equals the mean as there is no way to adjust the variance independently. &lt;br&gt;
In many practical use-cases, there is &lt;a href="https://en.wikipedia.org/wiki/Overdispersion"&gt;overdispersion&lt;/a&gt; though, meaning that the variance is larger than the mean and we have to make up for it.
We can define a so called &lt;em&gt;dispersion parameter&lt;/em&gt; &lt;span class="math"&gt;\(r\in(0,\infty)\)&lt;/span&gt; by reparametrization in the &lt;a href="https://en.wikipedia.org/wiki/Negative_binomial_distribution"&gt;negative binomial distribution&lt;/a&gt;,&amp;nbsp;i.e.&lt;/p&gt;
&lt;div class="math"&gt;$$\mathrm{NB}(y;\mu,r) = \frac{\Gamma(r+y)}{y!\cdot\Gamma(r)}\cdot\left(\frac{r}{r+\mu}\right)^r\cdot\left(\frac{\mu}{r+\mu}\right)^y,$$&lt;/div&gt;
&lt;p&gt;where &lt;span class="math"&gt;\(\Gamma\)&lt;/span&gt; is the &lt;a href="https://en.wikipedia.org/wiki/Gamma_function"&gt;Gamma function&lt;/a&gt;. Now we&amp;nbsp;have&lt;/p&gt;
&lt;div class="math"&gt;$$\sigma^2=\mu + \frac{1}{r}\mu^2$$&lt;/div&gt;
&lt;p&gt;and using &lt;span class="math"&gt;\(r\)&lt;/span&gt; we are thus able to adjust the variance from &lt;span class="math"&gt;\(\mu\)&lt;/span&gt; to &lt;span class="math"&gt;\(\infty\)&lt;/span&gt;. 
Another name for the negative binomial distribution is Gamma-Poisson distribution and this is the name under which we find it also in NumPyro. I find this name 
much more catchy since you can imagine a Poisson distribution with its only parameter drawn from a Gamma distribution that
has two parameters &lt;span class="math"&gt;\(\alpha\)&lt;/span&gt; and &lt;span class="math"&gt;\(\beta\)&lt;/span&gt;. This also intuitively explains why the variance of &lt;span class="caps"&gt;NB&lt;/span&gt; is bounded below by its mean.
Just think of &lt;span class="caps"&gt;NB&lt;/span&gt; as a generalization of the Poisson distribution with one more parameter that allows adjusting the&amp;nbsp;variance.&lt;/p&gt;
&lt;p&gt;Uncertainty Quantification is a crucial requirement for demand forecasts in retail although peculiarly, no one really cares about forecasts in retail anyway.
What retailers really care about is optimal replenishment, meaning that they want to have a system telling them how much
to order so that there is an optimal amount of stocks available in their store. In order to provide optimal replenishment
suggestions you need demand forecasts that provide probability distributions, not only point estimations. With the help
of those distributions the replenishment system basically runs an optimization with respect to some cost function, e.g.
cost of a missed sale is weighted 3 times the cost of a written-off product, and further constraints, e.g. if products can only be ordered in bundles of 10. 
For these reasons we will use the &lt;span class="caps"&gt;NB&lt;/span&gt; distribution that allows us the quantify the uncertainties in our sales predictions&amp;nbsp;adequately.&lt;/p&gt;
&lt;p&gt;So now that we settled with &lt;span class="caps"&gt;NB&lt;/span&gt; as the distribution that we want to fit to the daily sales of our stores &lt;span class="math"&gt;\(\mathbf{y}\)&lt;/span&gt;, we can think
about incorporating our features &lt;span class="math"&gt;\(\mathbf{x}\)&lt;/span&gt;. We want to use a linear model to map &lt;span class="math"&gt;\(\mathbf{x}\)&lt;/span&gt; to &lt;span class="math"&gt;\(\mathbf{\mu}\)&lt;/span&gt; such
that we can use it later to calculate &lt;span class="math"&gt;\(\alpha\)&lt;/span&gt; and &lt;span class="math"&gt;\(\beta\)&lt;/span&gt; of &lt;span class="caps"&gt;NB&lt;/span&gt;. Using again the fact that we are dealing with non-negative
numbers and also considering that we expect effects to be multiplicative, e.g. 10% more during a promotion, our ansatz&amp;nbsp;is&lt;/p&gt;
&lt;div class="math"&gt;$$\mu = \exp(\mathbf{\theta}^\top\mathbf{x}),$$&lt;/div&gt;
&lt;p&gt;where &lt;span class="math"&gt;\(\mathbf{\theta}\)&lt;/span&gt; is a vector of coefficients. For each store &lt;span class="math"&gt;\(i\)&lt;/span&gt; and each feature &lt;span class="math"&gt;\(j\)&lt;/span&gt; we will have a separate
coefficient &lt;span class="math"&gt;\(\theta_{ij}\)&lt;/span&gt;. The &lt;span class="math"&gt;\(\theta_{ij}\)&lt;/span&gt; are regularized by parameters &lt;span class="math"&gt;\(\theta^\mu_j\)&lt;/span&gt; and &lt;span class="math"&gt;\(\theta^{\sigma^2}_j\)&lt;/span&gt; on 
the global level, which helps us in case a store has only little historical data. For the dispersion parameters we
infer individual &lt;span class="math"&gt;\(r_i\)&lt;/span&gt; for each store &lt;span class="math"&gt;\(i\)&lt;/span&gt; as well as global parameters &lt;span class="math"&gt;\(r^\mu\)&lt;/span&gt; and &lt;span class="math"&gt;\(r^{\sigma^2}\)&lt;/span&gt; over all stores. 
And that&amp;#8217;s already most of it. Figure 2 depicts the graphical model outlined so&amp;nbsp;far.&lt;/p&gt;
&lt;figure&gt;
&lt;p align="center"&gt;
&lt;img class="noZoom" src="/images/bhm_model.png" alt="centered hierarchical model" width="60%"&gt;
&lt;/p&gt;
&lt;figcaption align="center"&gt;
&lt;strong&gt;Figure 2:&lt;/strong&gt; Graphical representation of a hierarchical model (centered version) as defined above.
&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;
&lt;p&gt;Those boxes in Figure 2, which are called &lt;a href="https://en.wikipedia.org/wiki/Plate_notation"&gt;plates&lt;/a&gt;, tell you how many times a parameter is repeated.
Nested plates are multiplied by the number given by outer plates, which can also be seen by looking at the number of indices.
The concept of plates was also taken up by the authors of NumPyro to express that certain dimensions are conditionally independent.
This also helps them to increase performance by taking optimizations into account that are just not possible in the general case.
Shaded circles are observed values, which in our case are the number of sales on a given day &lt;span class="math"&gt;\(k\)&lt;/span&gt; and store &lt;span class="math"&gt;\(i\)&lt;/span&gt;.&lt;/p&gt;
&lt;h2&gt;Implementation&lt;/h2&gt;
&lt;p&gt;Let&amp;#8217;s take a quick look into our model code which is just a normal Python function. It&amp;#8217;s good to keep in mind, that we call this a &lt;em&gt;model&lt;/em&gt; since we assume that given
the right parameters it would be able to generate sales for some given stores and days resembling the observed sales for these stores and days.
The model function only defines the model parameters, how they interact and their&amp;nbsp;priors.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpyro&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpyro.distributions&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;dist&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;jax.numpy&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;DeviceArray&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;jax.numpy&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;jnp&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;jax&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;random&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;DeviceArray&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;DeviceArray&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;Gamma-Poisson hierarchical model for daily sales forecasting&lt;/span&gt;

&lt;span class="sd"&gt;    Args:&lt;/span&gt;
&lt;span class="sd"&gt;        X: input data&lt;/span&gt;

&lt;span class="sd"&gt;    Returns:&lt;/span&gt;
&lt;span class="sd"&gt;        output data&lt;/span&gt;
&lt;span class="sd"&gt;    &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;n_stores&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_days&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_features&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;
    &lt;span class="n"&gt;n_features&lt;/span&gt; &lt;span class="o"&gt;-=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;  &lt;span class="c1"&gt;# remove one dim for target&lt;/span&gt;
    &lt;span class="n"&gt;eps&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;1e-12&lt;/span&gt;  &lt;span class="c1"&gt;# epsilon&lt;/span&gt;

    &lt;span class="n"&gt;plate_features&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;numpyro&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;features&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_features&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dim&lt;/span&gt;&lt;span class="o"&gt;=-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;plate_stores&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;numpyro&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;stores&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_stores&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dim&lt;/span&gt;&lt;span class="o"&gt;=-&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;plate_days&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;numpyro&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;timesteps&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_days&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dim&lt;/span&gt;&lt;span class="o"&gt;=-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;disp_param_mu&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;numpyro&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="s2"&gt;&amp;quot;disp_param_mu&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;dist&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;loc&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;4.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;scale&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;1.&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;disp_param_sigma&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;numpyro&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="s2"&gt;&amp;quot;disp_param_sigma&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;dist&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;HalfNormal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;scale&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;1.&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;plate_stores&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;disp_param_offsets&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;numpyro&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
            &lt;span class="s2"&gt;&amp;quot;disp_param_offsets&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;dist&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
                &lt;span class="n"&gt;loc&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;jnp&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;n_stores&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt;
                &lt;span class="n"&gt;scale&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;disp_params&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;disp_param_mu&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;disp_param_offsets&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;disp_param_sigma&lt;/span&gt;
        &lt;span class="n"&gt;disp_params&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;numpyro&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
            &lt;span class="s2"&gt;&amp;quot;disp_params&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;dist&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Delta&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;disp_params&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
            &lt;span class="n"&gt;obs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;disp_params&lt;/span&gt;
        &lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;plate_features&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;coef_mus&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;numpyro&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
            &lt;span class="s2"&gt;&amp;quot;coef_mus&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;dist&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;loc&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;jnp&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_features&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
                        &lt;span class="n"&gt;scale&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;jnp&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ones&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_features&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
        &lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;coef_sigmas&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;numpyro&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
            &lt;span class="s2"&gt;&amp;quot;coef_sigmas&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;dist&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;HalfNormal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;scale&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;2.&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;jnp&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ones&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_features&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
        &lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;plate_stores&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;coef_offsets&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;numpyro&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
                &lt;span class="s2"&gt;&amp;quot;coef_offsets&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                &lt;span class="n"&gt;dist&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
                    &lt;span class="n"&gt;loc&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;jnp&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;n_stores&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_features&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt;
                    &lt;span class="n"&gt;scale&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;1.&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;coefs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;coef_mus&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;coef_offsets&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;coef_sigmas&lt;/span&gt;
            &lt;span class="n"&gt;coefs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;numpyro&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
                &lt;span class="s2"&gt;&amp;quot;coefs&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                &lt;span class="n"&gt;dist&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Delta&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;coefs&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
                &lt;span class="n"&gt;obs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;coefs&lt;/span&gt;
            &lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;plate_days&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;plate_stores&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;targets&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;...&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="n"&gt;features&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;jnp&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nan_to_num&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;...&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;  &lt;span class="c1"&gt;# padded features to 0&lt;/span&gt;
        &lt;span class="n"&gt;is_observed&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;jnp&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;where&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;jnp&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;isnan&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;targets&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;jnp&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros_like&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;targets&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;jnp&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ones_like&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;targets&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
        &lt;span class="n"&gt;not_observed&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;is_observed&lt;/span&gt;
        &lt;span class="n"&gt;means&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;is_observed&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;jnp&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;jnp&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;jnp&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;expand_dims&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;coefs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
                 &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;not_observed&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;eps&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="n"&gt;betas&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;is_observed&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;jnp&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;disp_params&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;not_observed&lt;/span&gt;
        &lt;span class="n"&gt;alphas&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;means&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;betas&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;numpyro&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;days&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dist&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;GammaPoisson&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;alphas&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;betas&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;obs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;jnp&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nan_to_num&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;targets&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Note that &lt;code&gt;disp_param&lt;/code&gt; is &lt;span class="math"&gt;\(r\)&lt;/span&gt; and &lt;code&gt;coef&lt;/code&gt; is &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; in the source code above for better readability. You will
recognize a lot of what we have talked about and I don&amp;#8217;t want to go into the syntactical details of NumPyro. My suggestion
would be to first read the documentation of Pyro, as it is way more comprehensive, and then look up the differences in the
NumPyro&amp;nbsp;reference. &lt;/p&gt;
&lt;p&gt;Reading the source code more thoroughly, you might wonder about the definition of the coefficients as &lt;code&gt;coefs = coef_mus + coef_offsets * coef_sigmas&lt;/code&gt;.
The explanations of the model I have given and also the plot, actually shows the &lt;em&gt;centered version&lt;/em&gt; of a
hierarchical model. For me the centered version feels much more intuitive and is easier to explain. The downside is that
the direct dependency of the local parameters on the global ones make it hard for many &lt;span class="caps"&gt;MCMC&lt;/span&gt; sampling methods but also &lt;span class="caps"&gt;SVI&lt;/span&gt; methods to explore
certain regions of the local parameter space. This effect is called &lt;em&gt;funnel&lt;/em&gt; and can be imagined as walking with the the same step length
on a bridge that gets narrower and narrower. From the point on where the bridge is about as wide as your step length, you might
become a bit hesitant to explore more of it. As very often the case, a reparameterization overcomes this problem resulting
in the &lt;em&gt;non-centered&lt;/em&gt; version of a hierarchical model. This is the version used in the implementation. If you want to know more about this, a really great 
&lt;a href="https://twiecki.io/blog/2017/02/08/bayesian-hierchical-non-centered/"&gt;blog post by Thomas Wiecki&lt;/a&gt; gives you all the details about&amp;nbsp;it.&lt;/p&gt;
&lt;p&gt;Another thing that wasn&amp;#8217;t mentioned yet are the &lt;code&gt;is_observed&lt;/code&gt; and &lt;code&gt;not_observed&lt;/code&gt; variables which are just a nice gimmick.
Instead of using up degrees of freedom to learn that the number of sales is 0 on days where the store is closed, I set
the target variable &lt;span class="math"&gt;\(y\)&lt;/span&gt; to &lt;em&gt;not observed&lt;/em&gt; instead of 0. During training, these target values are just ignored and later
allows the model to answer a store manager&amp;#8217;s potential question: &amp;#8220;How many sales would I have had if I had opened my store on that&amp;nbsp;day?&amp;#8221;&lt;/p&gt;
&lt;p&gt;Until now we have talked about the model and if you are a PyMC3 user, you might think that this should be enough to actually solve it.
Pyro and NumPyro have a curious difference with respect to that. To actually fit the parameters of the model, distributions for the 
parameters have to be defined since its &lt;span class="caps"&gt;SVI&lt;/span&gt;. This is done in a separate function called &lt;em&gt;guide&lt;/em&gt;. &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;guide&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;DeviceArray&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;Guide with parameters of the posterior&lt;/span&gt;

&lt;span class="sd"&gt;    Args:&lt;/span&gt;
&lt;span class="sd"&gt;        X: input data&lt;/span&gt;
&lt;span class="sd"&gt;    &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;n_stores&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_days&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_features&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;
    &lt;span class="n"&gt;n_features&lt;/span&gt; &lt;span class="o"&gt;-=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;  &lt;span class="c1"&gt;# remove one dim for target&lt;/span&gt;

    &lt;span class="n"&gt;plate_features&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;numpyro&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;features&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_features&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dim&lt;/span&gt;&lt;span class="o"&gt;=-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;plate_stores&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;numpyro&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;stores&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_stores&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dim&lt;/span&gt;&lt;span class="o"&gt;=-&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;numpyro&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="s2"&gt;&amp;quot;disp_param_mu&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;dist&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;loc&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;numpyro&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;param&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;loc_disp_param_mu&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;4.&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;jnp&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ones&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt;
                    &lt;span class="n"&gt;scale&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;numpyro&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;param&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;scale_disp_param_mu&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;1.&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;jnp&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ones&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
                                        &lt;span class="n"&gt;constraint&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;dist&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;constraints&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;positive&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;numpyro&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="s2"&gt;&amp;quot;disp_param_sigma&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;dist&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;TransformedDistribution&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
            &lt;span class="n"&gt;dist&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;loc&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;numpyro&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;param&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;loc_disp_param_logsigma&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;jnp&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ones&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt;
                        &lt;span class="n"&gt;scale&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;numpyro&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;param&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;scale_disp_param_logsigma&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;jnp&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ones&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
                                            &lt;span class="n"&gt;constraint&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;dist&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;constraints&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;positive&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt;
            &lt;span class="n"&gt;transforms&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;dist&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;transforms&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ExpTransform&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
    &lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;plate_stores&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;numpyro&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
            &lt;span class="s2"&gt;&amp;quot;disp_param_offsets&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;dist&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;loc&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;numpyro&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;param&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;loc_disp_param_offsets&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;jnp&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;n_stores&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))),&lt;/span&gt;
                        &lt;span class="n"&gt;scale&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;numpyro&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;param&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;scale_disp_param_offsets&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;jnp&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ones&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;n_stores&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt;
                                            &lt;span class="n"&gt;constraint&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;dist&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;constraints&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;positive&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
        &lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;plate_features&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;numpyro&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
            &lt;span class="s2"&gt;&amp;quot;coef_mus&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;dist&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;loc&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;numpyro&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;param&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;loc_coef_mus&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;jnp&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ones&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_features&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt;
                        &lt;span class="n"&gt;scale&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;numpyro&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;param&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;scale_coef_mus&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;jnp&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ones&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_features&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
                                            &lt;span class="n"&gt;constraint&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;dist&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;constraints&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;positive&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
        &lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;numpyro&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
            &lt;span class="s2"&gt;&amp;quot;coef_sigmas&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;dist&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;TransformedDistribution&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
                &lt;span class="n"&gt;dist&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;loc&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;numpyro&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;param&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;loc_coef_logsigmas&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;jnp&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_features&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt;
                            &lt;span class="n"&gt;scale&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;numpyro&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;param&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;scale_coef_logsigmas&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;jnp&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ones&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_features&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
                                                &lt;span class="n"&gt;constraint&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;dist&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;constraints&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;positive&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt;
                &lt;span class="n"&gt;transforms&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;dist&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;transforms&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ExpTransform&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
        &lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;plate_stores&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;numpyro&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
                &lt;span class="s2"&gt;&amp;quot;coef_offsets&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                &lt;span class="n"&gt;dist&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
                    &lt;span class="n"&gt;loc&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;numpyro&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;param&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;loc_coef_offsets&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;jnp&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;n_stores&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_features&lt;/span&gt;&lt;span class="p"&gt;))),&lt;/span&gt;
                    &lt;span class="n"&gt;scale&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;numpyro&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;param&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;scale_coef_offsets&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;jnp&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ones&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;n_stores&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_features&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt;
                                        &lt;span class="n"&gt;constraint&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;dist&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;constraints&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;positive&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
                    &lt;span class="p"&gt;)&lt;/span&gt;
                &lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;As you can see, the structure of the guide reflects the structure of the model and there we are just defining for each model parameter
a distribution that again has parameters that need to be determined. The link between model and guide is given by the names of the &lt;em&gt;sample sites&lt;/em&gt;
like &amp;#8220;coef_offsets&amp;#8221;. This is a bit dangerous as a single typo in the model or guide may break this link leading to unexpected
behaviour. I spent more than a day of debugging a model once until I realized that some sample site in the guide had a typo.
You can see in the actual implementation, i.e. &lt;a href="https://github.com/FlorianWilhelm/bhm-at-scale/blob/master/src/bhm_at_scale/model.py"&gt;model.py&lt;/a&gt;, that I learnt from my mistakes as this source of error can be completely eliminated by simply defining class variables&amp;nbsp;like:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Site&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;disp_param_mu&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;disp_param_mu&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;disp_param_sigma&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;disp_param_sigma&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;disp_param_offsets&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;disp_param_offsets&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;disp_params&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;disp_params&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;coef_mus&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;coef_mus&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;coef_sigmas&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;coef_sigmas&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;coef_offsets&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;coef_offsets&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;coefs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;coefs&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;days&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;days&amp;quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Then using variables like &lt;code&gt;Site.coef_offsets&lt;/code&gt; instead of strings like &lt;code&gt;"coef_offsets"&lt;/code&gt; as identifiers of sample sites, allows your &lt;span class="caps"&gt;IDE&lt;/span&gt; to inform you about any typo as you go. Problem&amp;nbsp;solved.&lt;/p&gt;
&lt;p&gt;Besides the model and guide, we also have to define a local guide and a predictive model. The local guide assumes
that we have already fitted the global parameters but want to only determine the local parameters of new stores with little data.
The predictive model assumes global and local parameters to be already inferred so that we can use it to predict the number
of sales on days beyond our training interval. As these functions are only slide variations, I spare you the details and refer you to the implementation in &lt;a href="https://github.com/FlorianWilhelm/bhm-at-scale/blob/master/src/bhm_at_scale/model.py"&gt;model.py&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Most parts of the &lt;a href="https://github.com/FlorianWilhelm/bhm-at-scale/blob/master/notebooks/02-model.ipynb"&gt;model notebook&lt;/a&gt; actually deal with training the model on stores from the training data with a long sales history,
then fixing the global parameters and fitting the local guide on the stores from the test set with a really short history of 7 days.
This works really well although the number of features, which is 23, is much higher than 7! Let&amp;#8217;s take a look at the coefficients 
of a store from the test set as detailed in the &lt;a href="https://github.com/FlorianWilhelm/bhm-at-scale/blob/master/notebooks/02-model.ipynb"&gt;model notebook&lt;/a&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;[ 2.7901888   2.6591218   2.6881874   2.6126788   2.656775    2.554397
  2.4938385   0.33227605  3.115691    2.9264386   2.692092    2.9548435
  0.05613964  0.06542112  2.8379264   2.9023972   3.5701406   3.2074358
  4.0569873   2.9304545   2.7463424   2.823191    2.959007  ]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The notebook also shows that the traditional Poisson regression using Scikit-Learn overfits the training set and yields implausible coefficients. 
Comparing the coefficients from above to the ones of the Poisson regression for the same store,&amp;nbsp;i.e.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;[ 1.1747563  -2.2386415   2.1442924   1.9889396   1.9385103   1.8024149
 -6.8102717   1.1747563   2.2386413  -2.2386413   0.          0.
 -0.09434899  0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.        ]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;we see that they are highly different and zero for features in the Poisson regression that weren&amp;#8217;t encountered in the data of just 7 days.
Now comparing the coefficients of our &lt;span class="caps"&gt;BHM&lt;/span&gt; model trained on just 7 days with the coefficients of a cheating model trained on the whole test set,&amp;nbsp;i.e.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;[ 2.814997    2.7551868   2.6182172   2.6453698   2.672692    2.5201027
  2.593036    0.2265296   3.184446    3.1163387   2.5429602   2.9477317
 -0.03218627  0.06836608  2.8726482   2.925492    3.56679     3.215817
  4.0523443   2.9164758   2.7241356   2.8247747   2.9598234 ]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;we see that they are quite similar. That&amp;#8217;s the magic of a hierarchical model! We started with plausible defaults from a 
global perspective and adapted locally depending on the amount of available&amp;nbsp;data.&lt;/p&gt;
&lt;p&gt;Running the code from the &lt;a href="https://github.com/FlorianWilhelm/bhm-at-scale/blob/master/notebooks/02-model.ipynb"&gt;model notebook&lt;/a&gt; on your laptop will be matter of a few minutes for the training on 1000 stores
each having 942 days of sales and 23 features for each store and day. In total this leads to roughly one million data points
and &lt;span class="math"&gt;\(23\cdot 1000+1000+2\cdot 23+2=24048\)&lt;/span&gt; parameters in our graphical model. Since each parameter is fitted with the help of a 
parameterized distribution in the guide, as we are doing &lt;span class="caps"&gt;SVI&lt;/span&gt;, the number of actual variables is twice as much leading to roughly 50,000
variables that need to be fitted. While 50k parameters and about 1 Million data points surely is not big data, 
it&amp;#8217;s still impressive that using NumPyro you can fit a model like that within a few minutes on your
laptop and the implementation is not even using batching that would speed it up even further. In one of my customer
projects we used a way larger model on much more data and our workstation was still able to handle it smoothly. 
NumPyro really scales well even beyond this little&amp;nbsp;demonstration. &lt;/p&gt;
&lt;h2&gt;Visualization&lt;/h2&gt;
&lt;p&gt;There is now tons of things one could do with the results of our hierarchical model. One could check out the
actual prediction results, look at how certain we are about the parameters like the coefficients and so on. Most of that
I will leave to the interested reader and give only a few tidbits here from the &lt;a href="https://github.com/FlorianWilhelm/bhm-at-scale/blob/master/notebooks/03-evaluation.ipynb"&gt;evaluation notebook&lt;/a&gt;.
We start with taking a look at the sales prediction for one of the stores from the test set as depicted in Figure&amp;nbsp;3.&lt;/p&gt;
&lt;figure&gt;
&lt;p align="center"&gt;
&lt;img class="noZoom" src="/images/bhm_sales_forecast.png" alt="plot of sales forecast"&gt;
&lt;/p&gt;
&lt;figcaption align="center"&gt;
&lt;strong&gt;Figure 3:&lt;/strong&gt; Sales forecast of one store from the test set. The blue dashed line is the the mean predicted mean.
&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;
&lt;p&gt;Only judging by the eye, we see that predicted mean (blue dashed line) follows the number of sales (bluish bars) quite good.
The background is shaded according to some important features like promotions and holidays which explain some of the variations
in our predictions. Just for information, also the number of customers are displayed but not used in the prediction of course.
Also, we see the 50% and 90% &lt;a href="https://en.wikipedia.org/wiki/Credible_interval"&gt;credible intervals&lt;/a&gt; as shaded blue areas
around our mean, which tell us how certain we are about our predictions. We can also see that on Sundays, when the store
was closed, we predict not 0 but what would have likely happened if it wasn&amp;#8217;t closed, which was part of our&amp;nbsp;model.&lt;/p&gt;
&lt;p&gt;We could then also start looking into the effects of certain features like the weekdays. Figure 4 shows for each
weekday starting with Monday a density over the means of all&amp;nbsp;stores. &lt;/p&gt;
&lt;figure&gt;
&lt;p align="center"&gt;
&lt;img class="noZoom" src="/images/bhm_weekday_effect.png" alt="plot weekday effect"&gt;
&lt;/p&gt;
&lt;figcaption align="center"&gt;
&lt;strong&gt;Figure 4:&lt;/strong&gt; Density plot of the means of the weekday coefficients over all stores. 
&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;
&lt;p&gt;We can see that on average there seem to be a higher sales uplift on Mondays and also a high variance for the means on Saturdays and
Sundays when many stores are closed. If we are more interested in things we can change, like when to do a promotion,
we could be interested in analyzing the distribution of the promotion effect over all stores as shown in Figure&amp;nbsp;5.  &lt;/p&gt;
&lt;figure&gt;
&lt;p align="center"&gt;
&lt;img class="noZoom" src="/images/bhm_promo_effect.png" alt="plot of promotion effect"&gt;
&lt;/p&gt;
&lt;figcaption align="center"&gt;
&lt;strong&gt;Figure 5:&lt;/strong&gt; Density plot of the promotion effect over all stores with the red line showing the median.
&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;
&lt;p&gt;These are just some analysis one could start to look into to derive useful insights for store management. 
The rest is up to your imagination and also depending on your use-case. Imagine we had a dataset that features not 
only the aggregated sales but also the ones of individual products, i.e. &lt;a href="https://en.wikipedia.org/wiki/Stock_keeping_unit"&gt;&lt;span class="caps"&gt;SKU&lt;/span&gt;-level&lt;/a&gt;. We could then build hierarchies over the product hierarchies
and thus addressing cannibalization effects, e.g. when we introduce a new type of wine within our current offering.
We could also use &lt;span class="caps"&gt;BHM&lt;/span&gt; to address &lt;a href="https://en.wikipedia.org/wiki/Censoring_(statistics)"&gt;censored data&lt;/a&gt;, which is also an important task when doing demand forecasts. So far we have
used the words sales forecast and demand forecast interchangeably but bear in mind that we are actually interested in the demand.
Canonically, one assumes that the demand for a product equals its sales but this only holds true if there was no out-of-stock situation
in which we only know that demand â¥ sales. Right-censored data like that provides us with information about the &lt;a href="https://en.wikipedia.org/wiki/Cumulative_distribution_function"&gt;cumulative
distribution function&lt;/a&gt; in contrast to the &lt;a href="https://en.wikipedia.org/wiki/Probability_mass_function"&gt;probability mass function&lt;/a&gt;
in case of no out-of-stock situation. There are ways to include both types of information into a &lt;span class="caps"&gt;BHM&lt;/span&gt;.
Those are just some of many possible improvements and extensions to this model. I am looking forward to your ideas and&amp;nbsp;use-cases!&lt;/p&gt;
&lt;h2&gt;Final&amp;nbsp;Remarks&lt;/h2&gt;
&lt;p&gt;We have seen that &lt;span class="caps"&gt;BHM&lt;/span&gt; allows us to combine the advantages of a pooled and unpooled model. Using some retailer&amp;#8217;s data,
we implemented a simple &lt;span class="caps"&gt;BHM&lt;/span&gt; thereby also outlining the advantages of a Bayesian approach like uncertainty quantification and
explainability. From a practical perspective, we have seen that &lt;span class="caps"&gt;BHM&lt;/span&gt; even scales really well with the help of NumPyro.
On the theoretical side, we have talked about the Poisson distribution and why we preferred the Gamma-Poisson distribution.
Finally, I hope to have conveyed the most important point of this post well, being that these models can now be applied to
practical dataset sizes with the help of NumPyro! Cheers to that and let&amp;#8217;s follow a famous saying in the French world of mathematics &lt;em&gt;Poisson sans boisson est poison&lt;/em&gt;! &lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js','color.js'], equationNumbers: { autoNumber: 'auto' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="post"></category><category term="data science"></category><category term="mathematics"></category><category term="production"></category></entry><entry><title>Honey, I shrunk the targetÂ variable</title><link href="https://florianwilhelm.info/2020/05/honey_i_shrunk_the_target_variable/" rel="alternate"></link><published>2020-05-04T12:00:00+02:00</published><updated>2020-05-04T12:00:00+02:00</updated><author><name>Florian Wilhelm</name></author><id>tag:florianwilhelm.info,2020-05-04:/2020/05/honey_i_shrunk_the_target_variable/</id><summary type="html">&lt;p&gt;Feature engineering takes up a huge part in the work-life of a data scientist. Sometimes this doesn&amp;#8217;t stop at features but also the target variable itself is transformed leading to all kinds of unexpected consequences. In this post, you will learn about common pitfalls, how a transformation can affect the error measure, the math behind it, and even how all this can be used to your&amp;nbsp;advantage.&lt;/p&gt;</summary><content type="html">&lt;h2&gt;Motivation&lt;/h2&gt;
&lt;p&gt;For me it is often an irritating sight to see how inexperienced, up and coming data scientists jump right into the feature engineering when
facing some new supervised learning problem&amp;#8230; but it also makes me contemplate about my past when I started doing data science. 
So full of vigour and enthusiasm, I was often completely absorbed by the idea of minimizing whatever error measure I was given or maybe some random one 
I chose myself, like the &lt;a href="https://en.wikipedia.org/wiki/Root-mean-square_deviation"&gt;root mean square error&lt;/a&gt;. 
In my drive, I used to construct many derived features using clever transformations and sometimes did not even stop at
the target variable. Why should I? If the target variable is for instance non-negative and quite right-skewed, why not transform it using the
logarithm to make it more normally distributed? Isn&amp;#8217;t this better or even required for simple models like linear regression,
anyways? A little &lt;span class="math"&gt;\(\log\)&lt;/span&gt; never killed a dog, so what could possibly go&amp;nbsp;wrong? &lt;/p&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;
&lt;figure&gt;
&lt;p align="center"&gt;
&lt;img class="noZoom" src="/images/shrunk_meme.jpg" alt="Couple looking at spoon with magnifier"&gt;
&lt;/p&gt;
&lt;/figure&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;
&lt;p&gt;As you might have guessed from these questions, it&amp;#8217;s not that easy, and transforming your target variable puts you
directly into the &lt;a href="https://www.youtube.com/watch?v=siwpn14IE7E"&gt;danger zone&lt;/a&gt;. In this blog post, I want to elaborate on why this is so from a mathematical perspective
but also by demonstrating it in some practical examples.
Without spoiling too much I hope, for the too busy or plain lazy readers, the main take-away&amp;nbsp;is:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;&lt;span class="caps"&gt;TLDR&lt;/span&gt;&lt;/strong&gt;: Applying any non-&lt;a href="https://en.wikipedia.org/wiki/Affine_transformation"&gt;affine transformation&lt;/a&gt; to your target variable might have unwanted effects on the error measure you are minimizing.
            So if you don&amp;#8217;t know exactly what you are doing, just&amp;nbsp;don&amp;#8217;t.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;Let&amp;#8217;s get&amp;nbsp;started&lt;/h2&gt;
&lt;p&gt;Before we start with the gory mathematical details, let&amp;#8217;s first pick and explore a typical use-case where most inexperienced
data scientists might be tempted to transform the target variable without a second thought. In order to demonstrate this, I chose 
the &lt;a href="https://www.kaggle.com/orgesleka/used-cars-database"&gt;used-cars database from Kaggle&lt;/a&gt; and if you want to follow along, you find the code in the notebooks folder of my Github 
&lt;a href="https://github.com/FlorianWilhelm/used-cars-log-trans/"&gt;used-cars-log-trans repository&lt;/a&gt;. As the name suggests, the data set contains used cars having car features like &lt;code&gt;vehicleType&lt;/code&gt;,
&lt;code&gt;yearOfRegistration&lt;/code&gt; &lt;span class="amp"&gt;&amp;amp;&lt;/span&gt; &lt;code&gt;monthOfRegistration&lt;/code&gt;, &lt;code&gt;gearbox&lt;/code&gt;, &lt;code&gt;powerPS&lt;/code&gt;, &lt;code&gt;model&lt;/code&gt;, &lt;code&gt;kilometer&lt;/code&gt; (mileage), &lt;code&gt;fuelType&lt;/code&gt;, &lt;code&gt;brand&lt;/code&gt; and &lt;code&gt;price&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Let&amp;#8217;s say the business unit basically asks us to determine the proper market value of a car given the features above to determine
if its price is actually a good deal, fair deal or a bad deal. The obvious way to approach this problem is to create a model
that predicts the price of a car, which we assume to be its market value, given its features. 
Since we have roughly 370,000 cars in our data set, for most cars 
we will have many similar cars and thus our model will predict a price that is some kind of average of their prices.
Consequently, we can think of this predicted price (let&amp;#8217;s call it &lt;code&gt;pred_price&lt;/code&gt;) as the actual market value. 
To determine if the actual &lt;code&gt;price&lt;/code&gt; of a vehicle is a good, fair or bad deal, we would then calculate for instance the relative&amp;nbsp;error &lt;/p&gt;
&lt;div class="math"&gt;$$\frac{\mathrm{pred\_price} - \mathrm{price}}{\mathrm{price}}$$&lt;/div&gt;
&lt;p&gt;in the simplest case. If the relative error is close to zero we would call it fair, if it is much larger than zero it&amp;#8217;s a 
good deal and a bad deal if it is much smaller than zero. For the actual subject of this blog post, this use-case serves us already
as a good motivation for the development of some regression model that will predict the price given some car features.
The attentive reader has certainly noticed that the prices in our data set will be biased towards a higher price and thus
also our predicted &amp;#8220;market value&amp;#8221;. This is due to the fact that we don&amp;#8217;t know for which price the car was eventually sold.
We only know the amount of money the seller wanted to have which is of course higher or equal than what he or she gets in the end.
For the sake of simplicity, we assume that we have raised this point with the business unit, they noted it duly and we
thus neglect it for our&amp;nbsp;analysis.&lt;/p&gt;
&lt;h2&gt;Choosing the right error&amp;nbsp;measure&lt;/h2&gt;
&lt;p&gt;At this point, a lot of inexperienced data scientists would directly get into business of feature engineering and
build some kind of fancy model. Nowadays most machine learning frameworks like &lt;a href="https://scikit-learn.org/"&gt;Scikit-Learn&lt;/a&gt; are so easy to use
that one might even forget the error measure that is optimized as in most cases it will be the &lt;a href="https://en.wikipedia.org/wiki/Mean_squared_error"&gt;mean square error&lt;/a&gt; (&lt;span class="caps"&gt;MSE&lt;/span&gt;) by default.
But does the &lt;span class="caps"&gt;MSE&lt;/span&gt; really make sense for this use-case? First of all is our target measured in some currency,
so why would we try to minimize some squared difference? Squared Euro? Very clearly, even taking the square root in the 
end, i.e. &lt;a href="https://en.wikipedia.org/wiki/Root-mean-square_deviation"&gt;root mean square error&lt;/a&gt; (&lt;span class="caps"&gt;RMSE&lt;/span&gt;), would not change a thing about this fact. Still, we would weight one large residual
higher than many small residuals which sum up to the exact same value as if 10 times a residual of 10.- â¬ is somehow
less severe than a single residual of 100.- â¬. You see where I am getting at. In our use-case an error measure like the 
&lt;a href="https://en.wikipedia.org/wiki/Mean_absolute_error"&gt;mean absolute error&lt;/a&gt; (&lt;span class="caps"&gt;MAE&lt;/span&gt;) might be the more natural choice compared to the &lt;span class="caps"&gt;MSE&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;On the other hand, is it really that important if a car costs you 1,000.- â¬ more or less? It definitely does if you
are looking at cars at around 10,000.- â¬ but it might be negligible if your luxury vehicle is around 100,000.- â¬ anyway.
Consequently, the &lt;a href="https://en.wikipedia.org/wiki/Mean_absolute_percentage_error"&gt;mean absolute percentage error&lt;/a&gt; (&lt;span class="caps"&gt;MAPE&lt;/span&gt;) might even be a better fit than the &lt;span class="caps"&gt;MAE&lt;/span&gt; for this use-case.
Having said that, we will keep all those error measures in mind but use the default &lt;span class="caps"&gt;MSE&lt;/span&gt; criterion in our machine-learning
algorithm for the sake of simplicity and to help me make the actual point of this blog post&amp;nbsp;;-)&lt;/p&gt;
&lt;p&gt;Nevertheless, one crucial aspect should be kept in mind for the rest of this post. In the end after the fun part of modeling,
we, as data scientists, have to communicate the results to business people and the assessment of the quality of the results is going to
play an important role in this. This assessment will almost always be conducted using the raw, i.e. untransformed, target as well as 
the chosen error measure to answer the question if the results are good enough for the use-case at hand and consequently
if the model can go to production as a first iteration. Practically, that means that even if we decide to train a model
on a transformed target, we have to transform the predictions of the model back for evaluation. Results are always
communicated based on the original&amp;nbsp;target.&lt;/p&gt;
&lt;h2&gt;Distribution of the target&amp;nbsp;variable&lt;/h2&gt;
&lt;p&gt;Our data contains not only cars that are for sale but also cars people are searching for with a certain price. Additionally,
we have people offering damaged cars, wanting to trade their car for another or just hoping to get an insanely enormous amount of money. 
Sometimes you get lucky. For our use-case, we gonna keep only real offerings of undamaged cars with a reasonable price 
between 200.- â¬ and 50,000.- â¬ with a first registration not earlier than 1910.
This is what the distribution of the price looks&amp;nbsp;like.&lt;/p&gt;
&lt;figure&gt;
&lt;img class="noZoom" src="/images/histtv_price_distribution.png" alt="distribution of price"&gt;
&lt;figcaption align="center"&gt;Distribution plot of the price variable using 1,000.- â¬ bins.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;
&lt;p&gt;It surely does look like a &lt;a href="https://en.wikipedia.org/wiki/Log-normal_distribution"&gt;log-normal distribution&lt;/a&gt; and just to have visual check, fitting a log-normal distribution
with the help of the wonderful &lt;a href="https://www.scipy.org/"&gt;SciPy&lt;/a&gt; gets us&amp;nbsp;this.&lt;/p&gt;
&lt;figure&gt;
&lt;img class="noZoom" src="/images/histtv_price_log-normal_fit.png" alt="log-normal fit"&gt;
&lt;figcaption align="center"&gt;Log-normal distribution fitted to the distribution of prices.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;
&lt;p&gt;Seeing this, you might feel the itch to just apply now the logarithm to our target variable, just to make it look
more &lt;em&gt;normal&lt;/em&gt;. And isn&amp;#8217;t this some basic assumption of a linear model&amp;nbsp;anyway? &lt;/p&gt;
&lt;p&gt;Well, this is a common misconception. The dependent variable, i.e. target variable, of a linear model doesn&amp;#8217;t need to
be normally distributed, only the residuals are. This can be seen easily by revisiting the formula of a linear model. 
For the observed outcome &lt;span class="math"&gt;\(y_i\)&lt;/span&gt; and some true latent outcome &lt;span class="math"&gt;\(\mu_i\)&lt;/span&gt; of the &lt;span class="math"&gt;\(i\)&lt;/span&gt;-th sample, we&amp;nbsp;have&lt;/p&gt;
&lt;div class="math"&gt;\begin{equation}
\begin{split}
\mu_i &amp;amp;=  \sum_{j=1}^M w_j \phi_j(\mathbf{x}_i) , \\
y_i &amp;amp;= \mu_i + \epsilon, 
\end{split}\label{eqn:linear-model}
\end{equation}&lt;/div&gt;
&lt;p&gt;
&amp;nbsp;&lt;/p&gt;
&lt;p&gt;where &lt;span class="math"&gt;\(\mathbf{x}_i\)&lt;/span&gt; is the original feature vector, &lt;span class="math"&gt;\(\phi_j\)&lt;/span&gt;, &lt;span class="math"&gt;\(j=1, \ldots, M\)&lt;/span&gt; a set of (potentially non-linear) functions,
&lt;span class="math"&gt;\(w_j\)&lt;/span&gt;, &lt;span class="math"&gt;\(j=1, \ldots, M\)&lt;/span&gt; some scalar weights and &lt;span class="math"&gt;\(\epsilon\)&lt;/span&gt; some random noise that is distributed like the normal distribution
with mean &lt;span class="math"&gt;\(0\)&lt;/span&gt; and variance &lt;span class="math"&gt;\(\sigma^2\)&lt;/span&gt; (or &lt;span class="math"&gt;\(\epsilon\sim\mathcal{N}(0, \sigma^2)\)&lt;/span&gt; for short). If you wonder about the
&lt;span class="math"&gt;\(\phi_j\)&lt;/span&gt;, that&amp;#8217;s where all your feature engineering skills and domain knowledge go into to transform the raw features
into more suitable&amp;nbsp;ones.&lt;/p&gt;
&lt;p&gt;One of the reasons for this common misconception might be that the literature often states that the dependent variable &lt;span class="math"&gt;\(y\)&lt;/span&gt; &lt;em&gt;conditioned&lt;/em&gt;
on the predictor &lt;span class="math"&gt;\(\mathbf{x}\)&lt;/span&gt; is normally distributed in a linear model. So for a fixed &lt;span class="math"&gt;\(\mathbf{x}\)&lt;/span&gt; we have according to
&lt;span class="math"&gt;\(\eqref{eqn:linear-model}\)&lt;/span&gt; also a fixed &lt;span class="math"&gt;\(\mu\)&lt;/span&gt; and thus &lt;span class="math"&gt;\(y\)&lt;/span&gt; can be imagined as a realization of a random variable &lt;span class="math"&gt;\(Y=\mathcal{N}(\mu, \sigma^2)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;To make it even a tad more illustrative, imagine you want to predict the average alcohol level (in some strange log scale)
of a person celebrating Carnival only using a single binary feature, e.g. did the person have a one-night-stand over Carnival or not. 
Under these assumptions we simple generate some data using the linear model from above and plot&amp;nbsp;it:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;seaborn&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;sns&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;matplotlib.pylab&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;plt&lt;/span&gt;

&lt;span class="n"&gt;N&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;10000&lt;/span&gt;  &lt;span class="c1"&gt;# number of people&lt;/span&gt;
&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;randint&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mf"&gt;0.28&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;randn&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;sns&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;distplot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;xlim&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Obviously, this results in a bimodal distribution also known as the notorious &lt;a href="https://en.wikipedia.org/wiki/Cologne_Cathedral"&gt;Cologne Cathedral distribution&lt;/a&gt; as some may call it.
Thus, although using a linear model, we generated a non-normally distributed target variable with residuals that are normally&amp;nbsp;distributed.&lt;/p&gt;
&lt;figure&gt;
&lt;img class="noZoom" src="/images/dom_distribution.png" alt="log-normal fit"&gt;
&lt;figcaption align="center"&gt;Bimodal distribution generated with a linear model, which is obviously resembling the cathedral of Cologne.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;
&lt;p&gt;Based on common mnemonic techniques, and assuming this example was surprising, physical, sexual and humorous enough for you, 
you will never forget that the residuals of a linear model are normally distributed and &lt;em&gt;not&lt;/em&gt; the target variable in general. 
Only in the case that you used a linear model having only an intercept, i.e. &lt;span class="math"&gt;\(M=1\)&lt;/span&gt; and &lt;span class="math"&gt;\(\phi_1(\mathbf{x})\equiv 1\)&lt;/span&gt;,
the target distribution equals the residual distribution (up to some shift) on all data sets. But seriously, who does that in real&amp;nbsp;life?&lt;/p&gt;
&lt;h2&gt;Analysis of the residual&amp;nbsp;distribution&lt;/h2&gt;
&lt;p&gt;Now that we learnt about the distribution of the residual, we want to further analyse it. Especially with respect to
the error measure that we are trying to minimize as well as the transformation we apply to the target variable beforehand.
Let&amp;#8217;s take a look at the definition of the &lt;span class="caps"&gt;MSE&lt;/span&gt; again,&amp;nbsp;i.e.
&lt;/p&gt;
&lt;div class="math"&gt;\begin{equation}
\frac{1}{n}\sum_{i=1}^n (y_i - \hat y_i)^2,\label{eqn:sum_residual}
\end{equation}&lt;/div&gt;
&lt;p&gt;
where &lt;span class="math"&gt;\(\hat y_i = \hat y(\mathbf{x}_i)\)&lt;/span&gt; is our prediction given the feature vector &lt;span class="math"&gt;\(\mathbf{x}_i\)&lt;/span&gt; and &lt;span class="math"&gt;\(y_i\)&lt;/span&gt;
is the observed outcome for the sample &lt;span class="math"&gt;\(i\)&lt;/span&gt;. In reality we might only have a single or maybe a few samples sharing
exactly the same feature vector &lt;span class="math"&gt;\(\mathbf{x}_i\)&lt;/span&gt; and thus also the same model prediction &lt;span class="math"&gt;\(\hat y_i\)&lt;/span&gt;. In order to do some actual analysis, 
we now assume that we have an infinite number of observed outcomes for a given feature vector. Now
assume we keep &lt;span class="math"&gt;\(\mathbf{x}_i\)&lt;/span&gt; fixed and want to compute &lt;span class="math"&gt;\(\eqref{eqn:sum_residual}\)&lt;/span&gt; having all those observed outcomes.
Let&amp;#8217;s drop the index &lt;span class="math"&gt;\(i\)&lt;/span&gt; from &lt;span class="math"&gt;\(\hat y_i\)&lt;/span&gt; as it depends only on our fixed &lt;span class="math"&gt;\(\mathbf{x}_i\)&lt;/span&gt;. Also we can imagine all these outcomes
&lt;span class="math"&gt;\(y\)&lt;/span&gt; to be realizations of some random variable &lt;span class="math"&gt;\(Y\)&lt;/span&gt; conditioned on &lt;span class="math"&gt;\(\mathbf{x}\)&lt;/span&gt;. To now handle an infinite number of possible realizations,
we need to introduce the probability &lt;span class="math"&gt;\(f(y)\)&lt;/span&gt; of some realization &lt;span class="math"&gt;\(y\)&lt;/span&gt;, or more precisely the &lt;a href="https://en.wikipedia.org/wiki/Probability_density_function"&gt;probability density function&lt;/a&gt; (pdf) 
since &lt;span class="math"&gt;\(Y\)&lt;/span&gt; is a &lt;em&gt;continuous&lt;/em&gt; random variable. Consequently, as the summation becomes an integration, the discrete &lt;span class="caps"&gt;MSE&lt;/span&gt; in &lt;span class="math"&gt;\(\eqref{eqn:sum_residual}\)&lt;/span&gt;&amp;nbsp;becomes
&lt;/p&gt;
&lt;div class="math"&gt;\begin{equation}
\int_{-\infty}^\infty (y - \hat y)^2f(y)\, \mathrm{d}y,\label{eqn:int_residual}
\end{equation}&lt;/div&gt;
&lt;p&gt;
as you might have expected. Now this is awesome, as it allows us to apply some good, old-school calculus. By the way, when I am talking about the
&lt;em&gt;residual distribution&lt;/em&gt; I am actually referring to the distribution &lt;span class="math"&gt;\(y - \hat y\)&lt;/span&gt; with &lt;span class="math"&gt;\(y\sim f(y)\)&lt;/span&gt;.
Thus the residual distribution is determined by &lt;span class="math"&gt;\(f(y)\)&lt;/span&gt; except for a shift of &lt;span class="math"&gt;\(\hat y\)&lt;/span&gt;.  So what kind of assumptions can we make about it? 
In case of a linear model as in &lt;span class="math"&gt;\(\eqref{eqn:linear-model}\)&lt;/span&gt;, we assume &lt;span class="math"&gt;\(f(y)\)&lt;/span&gt; to be the pdf of a normal distribution but it could also be anything else.
In our car pricing use-case, we know that &lt;span class="math"&gt;\(y\)&lt;/span&gt; will be non-negative as no one is gonna give you money if you take a working car. Let me know if you have a counter-example ;-)
This rules out the normal distribution and demands a right skewed distribution, thus the pdf of the log-normal distribution might be an obvious assumption for &lt;span class="math"&gt;\(f(y)\)&lt;/span&gt; but we will come back later to&amp;nbsp;that.&lt;/p&gt;
&lt;p&gt;For now, we gonna consider &lt;span class="math"&gt;\(\eqref{eqn:int_residual}\)&lt;/span&gt; again and note that our model, whatever it is, will somehow try to minimize &lt;span class="math"&gt;\(\eqref{eqn:int_residual}\)&lt;/span&gt; by choosing a proper &lt;span class="math"&gt;\(\hat y\)&lt;/span&gt;.
So let&amp;#8217;s do that analytically by deriving &lt;span class="math"&gt;\(\eqref{eqn:int_residual}\)&lt;/span&gt; with respect to &lt;span class="math"&gt;\(\hat y\)&lt;/span&gt; and setting to &lt;span class="math"&gt;\(0\)&lt;/span&gt;, we have&amp;nbsp;that
&lt;/p&gt;
&lt;div class="math"&gt;$$
\frac{d}{d\hat y}\int_{-\infty}^\infty (y - \hat y)^2f(y)\, \mathrm{d}y = -2\int_{-\infty}^\infty yf(y)\, \mathrm{d}y + 2\hat y \stackrel{!}{=} 0,
$$&lt;/div&gt;
&lt;p&gt;
and&amp;nbsp;subsequently
&lt;/p&gt;
&lt;div class="math"&gt;\begin{equation}
\hat y = \int_{-\infty}^\infty yf(y)\, \mathrm{d}y.\label{eqn:expected-value}
\end{equation}&lt;/div&gt;
&lt;p&gt;
Looks familiar? Yes, this is just the definition of the &lt;a href="https://en.wikipedia.org/wiki/Expected_value#Absolutely_continuous_case"&gt;expected value in the continuous case&lt;/a&gt;! So whenever we are 
using the &lt;span class="caps"&gt;RMSE&lt;/span&gt; or &lt;span class="caps"&gt;MSE&lt;/span&gt; as error measure, we are actually calculating the expected value of &lt;span class="math"&gt;\(y\)&lt;/span&gt; at some fixed &lt;span class="math"&gt;\(\mathbf{x}\)&lt;/span&gt;. So what happens if
we do the same for the &lt;span class="caps"&gt;MAE&lt;/span&gt;? In this case, we&amp;nbsp;have
&lt;/p&gt;
&lt;div class="math"&gt;$$
\int_{-\infty}^\infty \vert y-\hat y\vert f(y)\, \mathrm{d}y=\int_{\hat y}^\infty (y-\hat y) f(y)\, \mathrm{d}y-\int_{-\infty}^{\hat y} (y-\hat y)f(y)\, \mathrm{d}y,
$$&lt;/div&gt;
&lt;p&gt; 
and deriving by &lt;span class="math"&gt;\(\hat y\)&lt;/span&gt; again, we&amp;nbsp;have
&lt;/p&gt;
&lt;div class="math"&gt;$$
\int_{-\infty}^{\hat y}  f(y)\, \mathrm{d}y - \int_{\hat y}^\infty  f(y)\, \mathrm{d}y \stackrel{!}{=} 0.
$$&lt;/div&gt;
&lt;p&gt;
We thus have &lt;span class="math"&gt;\(\hat y = P(X\leq \frac{1}{2})\)&lt;/span&gt;, which is, lo and behold, the &lt;a href="https://en.wikipedia.org/wiki/Median#Probability_distributions"&gt;median&lt;/a&gt; of the distribution with pdf &lt;span class="math"&gt;\(f(y)\)&lt;/span&gt;!&lt;/p&gt;
&lt;p&gt;A small recap at this point. We just learnt that minimizing the &lt;span class="caps"&gt;MSE&lt;/span&gt; or &lt;span class="caps"&gt;RMSE&lt;/span&gt; (also &lt;a href="https://en.wikipedia.org/wiki/Sequence_space#%E2%84%93p_spaces"&gt;l2-norm&lt;/a&gt; as a fancier name) leads
to the estimation of the expected value while minimizing &lt;span class="caps"&gt;MAE&lt;/span&gt; (also known as l1-norm) gets us the median of some distribution.
Also remember that our feature vector &lt;span class="math"&gt;\(\mathbf{x}\)&lt;/span&gt; is still fixed, so &lt;span class="math"&gt;\(y\sim f(y)\)&lt;/span&gt; just describes the random fluctuations around
some true value &lt;span class="math"&gt;\(y^\star\)&lt;/span&gt;, which we just don&amp;#8217;t know, and &lt;span class="math"&gt;\(\hat y\)&lt;/span&gt; is our best guess for it. If we assume the normal distribution
there is no reason to abandon all the nice mathematical properties of the l2-norm since the result will be theoretically the same as
minimizing the l1-norm. It may make a huge difference though, if we are dealing with a non-symmetrical distribution like
the log-normal&amp;nbsp;distribution.&lt;/p&gt;
&lt;p&gt;Let&amp;#8217;s just demonstrate this using our used cars example. We have already seen that the distribution of price is rather
log-normally than normally distributed. If we now use the simplest model we can think of, having only a single variable, 
(yeah, here comes the linear model with just an intercept again), the target distribution directly determines the residual
distribution. Now, we find the minimum point using &lt;span class="caps"&gt;RMSE&lt;/span&gt; and &lt;span class="caps"&gt;MAE&lt;/span&gt; to compare the results to the mean and median of 
the price vector &lt;code&gt;y&lt;/code&gt;,&amp;nbsp;respectively.  &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;rmse&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_pred&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_true&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt;     &lt;span class="c1"&gt;# not taking the root, i.e. MSE, would not change the actual result&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt;     &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sqrt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;y_true&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;y_pred&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;mae&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_pred&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_true&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt;     &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;abs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_true&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;y_pred&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;price&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;to_numpy&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;sp&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;optimize&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;minimize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;rmse&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;1.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;args&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,))&lt;/span&gt;
      &lt;span class="n"&gt;fun&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;7174.003600843465&lt;/span&gt;
 &lt;span class="n"&gt;hess_inv&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt;&lt;span class="mf"&gt;7052.74958795&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;
      &lt;span class="n"&gt;jac&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mf"&gt;0.&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
  &lt;span class="n"&gt;message&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;Optimization terminated successfully.&amp;#39;&lt;/span&gt;
     &lt;span class="n"&gt;nfev&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;36&lt;/span&gt;
      &lt;span class="n"&gt;nit&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;
     &lt;span class="n"&gt;njev&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;12&lt;/span&gt;
   &lt;span class="n"&gt;status&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
  &lt;span class="n"&gt;success&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="kc"&gt;True&lt;/span&gt;
        &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mf"&gt;6703.59325181&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="mf"&gt;6704.024314214464&lt;/span&gt;

&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;sp&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;optimize&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;minimize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mae&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;1.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;options&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;dict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;gtol&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;2e-4&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;args&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,))&lt;/span&gt;
      &lt;span class="n"&gt;fun&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;4743.492333474732&lt;/span&gt;
 &lt;span class="n"&gt;hess_inv&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt;&lt;span class="mf"&gt;7862.69627309&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;
      &lt;span class="n"&gt;jac&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;0.00018311&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
  &lt;span class="n"&gt;message&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;Optimization terminated successfully.&amp;#39;&lt;/span&gt;
     &lt;span class="n"&gt;nfev&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;120&lt;/span&gt;
      &lt;span class="n"&gt;nit&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;8&lt;/span&gt;
     &lt;span class="n"&gt;njev&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;40&lt;/span&gt;
   &lt;span class="n"&gt;status&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
  &lt;span class="n"&gt;success&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="kc"&gt;True&lt;/span&gt;
        &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mf"&gt;4099.9946168&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;median&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="mf"&gt;4100.0&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;As expected, by looking at the &lt;code&gt;x&lt;/code&gt; in the output of &lt;code&gt;minimize&lt;/code&gt;, we approximated the mean by minimizing the &lt;span class="caps"&gt;RMSE&lt;/span&gt; and the median by minimizing the &lt;span class="caps"&gt;MAE&lt;/span&gt;.&lt;/p&gt;
&lt;h2&gt;Shrinking the target&amp;nbsp;variable&lt;/h2&gt;
&lt;p&gt;There is still some elephant in the room that we haven&amp;#8217;t talked about yet. What happens now if we shrink our target
variable by applying a log transformation and then minimize the &lt;span class="caps"&gt;MSE&lt;/span&gt;?&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;y_log&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;price&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;to_numpy&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;sp&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;optimize&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;minimize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;rmse&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;1.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;args&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_log&lt;/span&gt;&lt;span class="p"&gt;,),&lt;/span&gt; &lt;span class="n"&gt;tol&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;1e-16&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
      &lt;span class="n"&gt;fun&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;1.0632889349620418&lt;/span&gt;
 &lt;span class="n"&gt;hess_inv&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt;&lt;span class="mf"&gt;1.06895454&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;
      &lt;span class="n"&gt;jac&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mf"&gt;0.&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
  &lt;span class="n"&gt;message&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;Optimization terminated successfully.&amp;#39;&lt;/span&gt;
     &lt;span class="n"&gt;nfev&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;36&lt;/span&gt;
      &lt;span class="n"&gt;nit&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;6&lt;/span&gt;
     &lt;span class="n"&gt;njev&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;12&lt;/span&gt;
   &lt;span class="n"&gt;status&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
  &lt;span class="n"&gt;success&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="kc"&gt;True&lt;/span&gt;
        &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mf"&gt;8.31228458&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;So if we now transform the result &lt;code&gt;x&lt;/code&gt; which is roughly &lt;code&gt;8.31&lt;/code&gt; back using &lt;code&gt;np.exp(8.31)&lt;/code&gt; we get a rounded result of &lt;code&gt;4064&lt;/code&gt;.
&lt;em&gt;Wait a second! What just happened!?&lt;/em&gt; We would have expected the final result to be around &lt;code&gt;6704&lt;/code&gt; because that&amp;#8217;s the
mean value we had before. Somehow, transforming the target variable, minimizing the same error measure as before and applying the inverse
transformation changed the result. Now our result of &lt;code&gt;4064&lt;/code&gt; looks rather like an approximation of the median&amp;#8230; well&amp;#8230;
it actually is assuming a log-normal distribution as we will fully understand soon. 
If we had applied some full-blown machine learning model, the difference would have been much smaller since the variance 
of the residual distribution would have been much smaller. Still, 
we would have missed our actual goal of minimizing the (R)&lt;span class="caps"&gt;MSE&lt;/span&gt; on the raw target. Instead we would have unknowingly minimized the &lt;span class="caps"&gt;MAE&lt;/span&gt;, which
might actually be better suited for our use-case at hand. Nevertheless, being a data &lt;em&gt;scientist&lt;/em&gt;, we should know what we
are doing and a lucky punch without a clue of what happened, just doesn&amp;#8217;t suit a&amp;nbsp;scientist.&lt;/p&gt;
&lt;p&gt;Before, we showed that the distribution of prices, and thus our target, resembles a log-normal distribution. So let&amp;#8217;s assume now that we
have a log-normal distribution, and thus we have &lt;span class="math"&gt;\(\log(\mathrm{price})\sim\mathcal{N}(\mu,\sigma^2)\)&lt;/span&gt;. Consequently,
the pdf of the price&amp;nbsp;is
&lt;/p&gt;
&lt;div class="math"&gt;\begin{equation}
\tilde f(x) = \frac {1}{x}\cdot {\frac {1}{ {\sqrt {2\pi\sigma^2 \,}}}}\exp \left(-{\frac {(\ln(x) -\mu )^{2}}{2\sigma ^{2}}}\right),\label{eqn:log-normal}
\end{equation}&lt;/div&gt;
&lt;p&gt;
where the only difference to the pdf of the normal distribution is &lt;span class="math"&gt;\(ln(x)\)&lt;/span&gt; instead of &lt;span class="math"&gt;\(x\)&lt;/span&gt; and the additional factor &lt;span class="math"&gt;\(\frac{1}{x}\)&lt;/span&gt;.
Also note that parameters &lt;span class="math"&gt;\(\mu\)&lt;/span&gt; and &lt;span class="math"&gt;\(\sigma\)&lt;/span&gt; are the well-known parameters of the normal distribution but for the
log-transformed target.
So when we now minimize the &lt;span class="caps"&gt;RMSE&lt;/span&gt; of the log-transformed prices as we did before, we actually infer the parameter
&lt;span class="math"&gt;\(\mu\)&lt;/span&gt; of the normal distribution, which is the expected value and also the &lt;em&gt;median&lt;/em&gt;, i.e. &lt;span class="math"&gt;\(\operatorname {P} (\log(\mathrm{price})\leq \mu)= 0.5\)&lt;/span&gt;. 
Applying any kind of strictly monotonic increasing transformation &lt;span class="math"&gt;\(\varphi\)&lt;/span&gt; to the price, we see that 
&lt;span class="math"&gt;\(\operatorname {P} (\varphi(\log(\mathrm{price}))\leq \varphi(\mu)) = 0.5\)&lt;/span&gt; and thus the median as well as any other quantile
is equivariant under the transformation &lt;span class="math"&gt;\(\varphi\)&lt;/span&gt;. In our specific case from above, we have &lt;span class="math"&gt;\(\varphi(x) = \exp(x)\)&lt;/span&gt; and
thus the result, that we are approximating the median instead of the mean, is not surprising at all from a mathematical point of&amp;nbsp;view.&lt;/p&gt;
&lt;p&gt;The expected value is not so well-behaved under transformations as the median. Using the definition of the expected
value &lt;span class="math"&gt;\(\eqref{eqn:expected-value}\)&lt;/span&gt;, we can easily show that only transformations &lt;span class="math"&gt;\(\phi\)&lt;/span&gt; of the form &lt;span class="math"&gt;\(\phi(x)=ax + b\)&lt;/span&gt;,
with scalars &lt;span class="math"&gt;\(a\)&lt;/span&gt; and &lt;span class="math"&gt;\(b\)&lt;/span&gt;, allow us to transform the target, determine the expected value and apply the
inverse transformation to get the expected value of the original distribution. In math-speak, a transformation of the form 
&lt;span class="math"&gt;\(\phi(x)=ax + b\)&lt;/span&gt; is also called an &lt;a href="https://en.wikipedia.org/wiki/Affine_transformation"&gt;affine transformation&lt;/a&gt;. For the transformed random variable &lt;span class="math"&gt;\(\phi(X)\)&lt;/span&gt; we have for the expected value&amp;nbsp;that
&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align*}
E[\phi(X)] &amp;amp;= E[aX + b] \\
            &amp;amp;= \int (ax + b)f(x)\, \mathrm{d}x \\
            &amp;amp;= a\int xf(x)\, \mathrm{d}x + b \\
            &amp;amp;=aE[X] + b =\phi(E[X]),
\end{align*}
$$&lt;/div&gt;
&lt;p&gt;
where we used the fact that probability density functions are normalized, i.e. &lt;span class="math"&gt;\(\int f(x)\mathrm{d}x=1\)&lt;/span&gt;. What a relief!
That means at least affine transformations are fine when we minimize the (R)&lt;span class="caps"&gt;MSE&lt;/span&gt;. 
This is especially important if you belong to the illustrious circle of deep learning specialists. In some cases, 
the target variable of a regression problem is standardized or &lt;a href="https://en.wikipedia.org/wiki/Feature_scaling#Rescaling_(min-max_normalization)"&gt;min-max scaled&lt;/a&gt; during training and transformed back afterwards.
Since these normalization techniques are affine transformations we are on the safe side,&amp;nbsp;though.  &lt;/p&gt;
&lt;p&gt;Let&amp;#8217;s come back to our example where we know that the distribution is quite log-normal. Can we 
somehow still receive the mean of the untransformed target variable? Yes we can, actually. Using the parameter &lt;span class="math"&gt;\(\mu\)&lt;/span&gt; that
we already determined above we just calculate the variance &lt;span class="math"&gt;\(\sigma^2\)&lt;/span&gt; and have &lt;span class="math"&gt;\(\exp(\mu + \frac{\sigma^2}{2})\)&lt;/span&gt; for the mean
of the untransformed distribution. More details on how to do this can be found in the &lt;a href="https://github.com/FlorianWilhelm/used-cars-log-trans/blob/master/notebooks/used-cars.ipynb"&gt;notebook&lt;/a&gt;
of the &lt;a href="https://github.com/FlorianWilhelm/used-cars-log-trans/"&gt;used-cars-log-trans repository&lt;/a&gt;. Way more interesting, at least for the mathematically interested reader,
is the question &lt;em&gt;Why does this work?&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;This is easy to see using some calculus. With &lt;span class="math"&gt;\(\tilde y = \log(y)\)&lt;/span&gt; and let &lt;span class="math"&gt;\(f(y)\)&lt;/span&gt; be the pdf of the normal distribution
as well as &lt;span class="math"&gt;\(\tilde f(y)\)&lt;/span&gt; the pdf of the log-normal distribution &lt;span class="math"&gt;\(\eqref{eqn:log-normal}\)&lt;/span&gt;. 
Using &lt;a href="https://en.wikipedia.org/wiki/Integration_by_substitution"&gt;integration by substitution&lt;/a&gt; and noting that &lt;span class="math"&gt;\(\mathrm{d}y = e^{\tilde y}\mathrm{d}\tilde y\)&lt;/span&gt;, we&amp;nbsp;have
&lt;/p&gt;
&lt;div class="math"&gt;\begin{equation}
\int y \tilde f(y)\, \mathrm{d}y = \int e^{\tilde y} \tilde f(e^{\tilde y})e^{\tilde y}\, \mathrm{d}\tilde y = \int e^{\tilde y} f(\tilde y)\, \mathrm{d}\tilde y,\label{eqn:mean-log-normal}
\end{equation}&lt;/div&gt;
&lt;p&gt;
where in the last equation the additional factor of the log-normal distribution was canceled out with &lt;span class="math"&gt;\(e^{\tilde y}\)&lt;/span&gt; and thus
became the pdf of the normal distribution due to our substitution. Writing out the exponent in &lt;span class="math"&gt;\(f(x)\)&lt;/span&gt;, which is &lt;span class="math"&gt;\(-\frac{(\tilde y-\mu)^2}{2\sigma^2}\)&lt;/span&gt; 
and completing the square with &lt;span class="math"&gt;\(\tilde y\)&lt;/span&gt;, we&amp;nbsp;have 
&lt;/p&gt;
&lt;div class="math"&gt;\begin{equation}
\begin{split}
\tilde y - \frac{(\tilde y-\mu)^2}{2\sigma^2} &amp;amp;= -\frac{\mu^2 - 2\mu\tilde y + \tilde{y}^2 - 2\sigma^2\tilde y}{2\sigma^2} \\
            &amp;amp;= -\frac{(\tilde y - (\mu + \sigma^2))^2}{2\sigma^2} + \mu + \frac{\sigma^2}{2}.
\end{split}\label{eqn:completing_square}
\end{equation}&lt;/div&gt;
&lt;p&gt;
Using this result, we can rewrite the last expression of &lt;span class="math"&gt;\(\eqref{eqn:mean-log-normal}\)&lt;/span&gt; by shifting the parameter &lt;span class="math"&gt;\(\mu\)&lt;/span&gt; of the
normal distribution by &lt;span class="math"&gt;\(\sigma^2\)&lt;/span&gt;. Denoting with &lt;span class="math"&gt;\(f_s(y)\)&lt;/span&gt; the shifted pdf, we&amp;nbsp;have
&lt;/p&gt;
&lt;div class="math"&gt;$$
\int e^{\tilde y} f(\tilde y)\, \mathrm{d}\tilde y = e^{\mu + \frac{1}{2}\sigma^2}\int f_s(\tilde y)\, \mathrm{d}\tilde y = e^{\mu + \frac{\sigma^2}{2}},
$$&lt;/div&gt;
&lt;p&gt;
and subsequently we have proved that the expected value of the log-normal distribution indeed is &lt;span class="math"&gt;\(\exp(\mu + \frac{\sigma^2}{2})\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;A little recap of this section&amp;#8217;s most important points to remember. When minimizing l2, i.e. (R)&lt;span class="caps"&gt;MSE&lt;/span&gt;, only affine transformations
allow us the determine the expected value of the original target by applying the inverse transformation to the expected value
of the transformed target variable. When minimizing l1, i.e. &lt;span class="caps"&gt;MAE&lt;/span&gt;, all strictly monotonic increasing transformations can be
applied to determine the median from the transformed target variable followed by the inverse&amp;nbsp;transformation.&lt;/p&gt;
&lt;h2&gt;Transforming the target for fun and&amp;nbsp;profit&lt;/h2&gt;
&lt;p&gt;So we have seen that not everything is as it seems or as we might have expected by doing some rather academical analysis.
But can we somehow use this knowledge in our use-case of predicting the market value of a used car?
Yeah, this is the point where we close the circle to the beginning of the story. We already argued that the &lt;span class="caps"&gt;RMSE&lt;/span&gt; might not
be the right error measure to minimize. Log-transforming the target and still minimizing the &lt;span class="caps"&gt;RMSE&lt;/span&gt; 
gave us an approximation to the result we would have gotten if we had minimized the &lt;span class="caps"&gt;MAE&lt;/span&gt;, which quite likely is a more appropriate error 
measure in this use-case than the &lt;span class="caps"&gt;RMSE&lt;/span&gt;. This is a neat trick if our regression method
only allows minimizing the &lt;span class="caps"&gt;MSE&lt;/span&gt; or if it is too slow or unstable when minimizing the &lt;span class="caps"&gt;MAE&lt;/span&gt; directly. A word of caution again, this
only works if the residual distribution approximates a log-normal distribution. So far we have only seen that the target
distribution, not the residual distribution, is quite log-normal but since we are dealing with positive numbers, and also taking into account the fact 
that a car seller might be more inclined to start with a higher price, this justifies the assumption that the residual 
distribution (in case of a multivariate model) will also approximate a log-normal&amp;nbsp;distribution.&lt;/p&gt;
&lt;p&gt;Well, the &lt;span class="caps"&gt;MAE&lt;/span&gt; surely is quite nice, but how about minimizing some relative measure like the &lt;span class="caps"&gt;MAPE&lt;/span&gt;? Assuming that our regression
method does not support minimizing it directly, does the log-transformation do any good here? 
Intuitively, since we know that multiplicative, and thus relative, relationships become additive in log-space, 
we might expect it to be advantageous and indeed it does help. But before we do some experiments, let&amp;#8217;s first look at some
other relative error measure, namely the Root Mean Square Percentage Error (&lt;span class="caps"&gt;RMSPE&lt;/span&gt;),&amp;nbsp;i.e.
&lt;/p&gt;
&lt;div class="math"&gt;$$
\sqrt{\frac{1}{n}\sum_{i=1}^n\left(\frac{y_i-\hat y_i}{y_i}\right)^2}.
$$&lt;/div&gt;
&lt;p&gt;
This error measure was used for evaluation in the &lt;a href="https://www.kaggle.com/c/rossmann-store-sales/"&gt;Rossmann Store Sales&lt;/a&gt; Kaggle challenge. Since the &lt;span class="caps"&gt;RMSPE&lt;/span&gt; is a rather
unusual and uncommon error measure, most participants log-transformed the target and minimized the &lt;span class="caps"&gt;RMSE&lt;/span&gt; without giving
too much thought about it, just following their intuition. Some participants in the challenge dug deeper, like Tim Hochberg 
who proved in a &lt;a href="https://www.kaggle.com/c/rossmann-store-sales/discussion/17026"&gt;forum&amp;#8217;s post&lt;/a&gt; that minimizing the &lt;span class="caps"&gt;RMSE&lt;/span&gt; of the log-transformed target is a first-order approximation of the &lt;span class="caps"&gt;RMSPE&lt;/span&gt; 
using &lt;a href="https://en.wikipedia.org/wiki/Taylor_series"&gt;Taylor series&lt;/a&gt; expansion. Although his result is correct, it only tells us that we are asymptotically doing the 
right thing, i.e. only if we had some really glorious model that perfectly predicts the target, which of course is never true.
So in practice, the residual distribution might be quite narrow but if it was the &lt;a href="https://en.wikipedia.org/wiki/Dirac_delta_function"&gt;Dirac delta function&lt;/a&gt; we would have found some
deterministic relationship between our feature and the target variable, or more likely made a mistake by evaluating some
over-fitted model on the train set. A nice example of being asymptotically right but practically wrong, by the way.
In a &lt;a href="https://www.kaggle.com/c/rossmann-store-sales/discussion/17601"&gt;reply post&lt;/a&gt; to Tim&amp;#8217;s original post, a guy who just calls himself &lt;em&gt;&lt;span class="caps"&gt;ML&lt;/span&gt;&lt;/em&gt;, pointed
out the overly optimistic assumption and proved that a correction of &lt;span class="math"&gt;\(-\frac{3}{2}\sigma^2\)&lt;/span&gt; is necessary during
back-transformation assuming a log-normal residual distribution. Since his post is quite scrambled, and also just for the
fun of it, we will also prove this after some more practical applications using the notation we already established. 
And while we are at it, we will also show that the necessary correction in case of &lt;span class="caps"&gt;MAPE&lt;/span&gt; is &lt;span class="math"&gt;\(-\sigma^2\)&lt;/span&gt;. But for now, we will
just take for granted the&amp;nbsp;following&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;(R)&lt;span class="caps"&gt;MSE&lt;/span&gt;&lt;/th&gt;
&lt;th&gt;&lt;span class="caps"&gt;MAE&lt;/span&gt;&lt;/th&gt;
&lt;th&gt;&lt;span class="caps"&gt;MAPE&lt;/span&gt;&lt;/th&gt;
&lt;th&gt;&lt;span class="caps"&gt;RMSPE&lt;/span&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;correction terms, i.e.&lt;/td&gt;
&lt;td&gt;&lt;span class="math"&gt;\(+\sigma^2\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span class="math"&gt;\(0\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span class="math"&gt;\(-\sigma^2\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span class="math"&gt;\(-\frac{3}{2}\sigma^2\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;which need to be added to the minimum point obtained by the &lt;span class="caps"&gt;RMSE&lt;/span&gt; minimization of the log-transformed target before transforming it back. 
Needless to say, the correction for &lt;span class="caps"&gt;RMSPE&lt;/span&gt; was one of the decisive factors to win the Kaggle challenge and thus to make some profit. The 
winner Gert Jacobusse mentions this in the attached &lt;span class="caps"&gt;PDF&lt;/span&gt; of his &lt;a href="https://www.kaggle.com/c/rossmann-store-sales/discussion/18024"&gt;model documentation post&lt;/a&gt;. &lt;/p&gt;
&lt;p&gt;What if we don&amp;#8217;t have a log-normal residual distribution or only a really rough approximation, can we be better than applying
those theoretical corrections terms? Sure, we can! In the end, since we are transforming back using &lt;span class="math"&gt;\(\exp\)&lt;/span&gt;, it&amp;#8217;s only
a correction factor close to &lt;span class="math"&gt;\(1\)&lt;/span&gt; that we are applying. So in case of &lt;span class="caps"&gt;RMSPE&lt;/span&gt; and for our approximation &lt;span class="math"&gt;\(\hat\mu\)&lt;/span&gt; of the log-normal 
distribution, we have a factor of &lt;span class="math"&gt;\(c=\exp(-\frac{3}{2}\sigma^2)\)&lt;/span&gt; for the back-transformed target &lt;span class="math"&gt;\(\exp(\hat\mu)\)&lt;/span&gt;.
We can just treat this as another one-dimensional optimization problem and determine the best correction factor numerically. 
Speaking of numerical computation, we are not gonna determine a factor &lt;span class="math"&gt;\(c\)&lt;/span&gt; but equivalently a correction term &lt;span class="math"&gt;\(\tilde c\)&lt;/span&gt;,
so that &lt;span class="math"&gt;\(\exp(\hat \mu + \tilde c)=\hat y\)&lt;/span&gt;, which is numerically much more&amp;nbsp;stable. &lt;/p&gt;
&lt;p&gt;At my former employer &lt;a href="https://blueyonder.com/"&gt;Blue Yonder&lt;/a&gt;,
we used to call this the &lt;em&gt;Gronbach factor&lt;/em&gt; after our colleague Moritz Gronbach, who would successfully apply this fitted correction
to all kinds of regression problems with non-negative values. The implementation is actually quite easy
given the true value, our predicted value in log-space and some error&amp;nbsp;measure:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;get_corr&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_true&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_pred_log&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;error_func&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="n"&gt;kwargs&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;Determine correction delta for exp transformation&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;cost_func&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;delta&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;error_func&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;delta&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;y_pred_log&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;y_true&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;res&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sp&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;optimize&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;minimize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cost_func&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="n"&gt;kwargs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;res&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;success&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;res&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;
    &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;raise&lt;/span&gt; &lt;span class="ne"&gt;RuntimeError&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Finding correction term failed!&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;res&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Let&amp;#8217;s now get our hands dirty and evaluate how &lt;span class="caps"&gt;RMSE&lt;/span&gt;, &lt;span class="caps"&gt;MAE&lt;/span&gt;, &lt;span class="caps"&gt;MAPE&lt;/span&gt; and &lt;span class="caps"&gt;RMSPE&lt;/span&gt; behave in our use-case with the raw as well 
as the log-transformed target using no, the theoretical and the fitted correction. To do so we gonna do some feature engineering and apply some &lt;span class="caps"&gt;ML&lt;/span&gt; method. 
Regarding the former, we just do some extremely basic things like calculating the age of a car and average mileage per year,&amp;nbsp;i.e.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;monthOfRegistration&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;monthOfRegistration&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;replace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;dateOfRegistration&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;apply&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;ds&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;datetime&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ds&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;yearOfRegistration&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;ds&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;monthOfRegistration&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;ageInYears&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;apply&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;ds&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ds&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;dateCreated&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;ds&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;dateOfRegistration&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;days&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="mi"&gt;365&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;mileageOverAge&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;kilometer&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;ageInYears&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;In total, combined with the original features, we take as our feature set including the&amp;nbsp;target:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;FEATURES&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;vehicleType&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; 
            &lt;span class="s2"&gt;&amp;quot;ageInYears&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="s2"&gt;&amp;quot;mileageOverAge&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="s2"&gt;&amp;quot;gearbox&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; 
            &lt;span class="s2"&gt;&amp;quot;powerPS&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="s2"&gt;&amp;quot;model&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="s2"&gt;&amp;quot;kilometer&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; 
            &lt;span class="s2"&gt;&amp;quot;fuelType&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="s2"&gt;&amp;quot;brand&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="s2"&gt;&amp;quot;price&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;FEATURES&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;and transform all categorical features to integers,&amp;nbsp;i.e.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;col&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dtype&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;zip&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;columns&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dtypes&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;dtype&lt;/span&gt; &lt;span class="ow"&gt;is&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;O&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;col&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;col&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;astype&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;category&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cat&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;codes&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;to get our final feature matrix &lt;code&gt;X&lt;/code&gt; and target vector &lt;code&gt;y&lt;/code&gt; with&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;price&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;to_numpy&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;drop&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;columns&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;price&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;to_numpy&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;As &lt;span class="caps"&gt;ML&lt;/span&gt; method, let&amp;#8217;s just choose a &lt;a href="https://en.wikipedia.org/wiki/Random_forest"&gt;Random Forest&lt;/a&gt; as for me it&amp;#8217;s like the &lt;a href="https://en.wikipedia.org/wiki/Volkswagen_Passat"&gt;Volkswagen Passat Variant&lt;/a&gt; among all &lt;span class="caps"&gt;ML&lt;/span&gt; algorithms. 
Although you will not win any competition with it, in most use-cases it will do a pretty decent job without much hassle.
In a real world scenario, one would rather select and fine-tune some &lt;a href="https://en.wikipedia.org/wiki/Gradient_boosting"&gt;Gradient Boosted Decision Tree&lt;/a&gt; like &lt;a href="https://xgboost.readthedocs.io/"&gt;XGBoost&lt;/a&gt;,
&lt;a href="https://lightgbm.readthedocs.io/"&gt;LightGBM&lt;/a&gt; or maybe even better &lt;a href="https://catboost.ai/"&gt;CatBoost&lt;/a&gt; since categories (e.g. &lt;code&gt;vehicleType&lt;/code&gt; and &lt;code&gt;model&lt;/code&gt;) surely play an important 
part in this use-case. We will use the default &lt;span class="caps"&gt;MSE&lt;/span&gt; criterion of &lt;a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html"&gt;Scikit-Learn&amp;#8217;s RandomForestRegressor&lt;/a&gt; implementation for
all&amp;nbsp;experiments. &lt;/p&gt;
&lt;p&gt;To now evaluate this model, we gonna use a 10-fold cross-validation and split off a validation set from the training set 
in each split to calculate &lt;span class="math"&gt;\(\sigma^2\)&lt;/span&gt; and fit our correction term. The cross-validation will give us some indication about the variance in 
our results. In each of these 10 splits, we then fit the model and predict using&amp;nbsp;the&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;raw, i.e. untransformed,&amp;nbsp;target,&lt;/li&gt;
&lt;li&gt;log-transformed target with no&amp;nbsp;correction,&lt;/li&gt;
&lt;li&gt;log-transformed target with the corresponding sigma2&amp;nbsp;correction,&lt;/li&gt;
&lt;li&gt;log-transformed target with the fitted&amp;nbsp;correction,&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;and evaluate the results with &lt;span class="caps"&gt;RMSE&lt;/span&gt;, &lt;span class="caps"&gt;MAE&lt;/span&gt;, &lt;span class="caps"&gt;MAPE&lt;/span&gt; and &lt;span class="caps"&gt;RMSPE&lt;/span&gt;. To spare you the trivial implementation, which is to be found
in the &lt;a href="https://github.com/FlorianWilhelm/used-cars-log-trans/blob/master/notebooks/used-cars.ipynb"&gt;notebook&lt;/a&gt;, we jump directly to the results of the first of 10&amp;nbsp;splits:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="right"&gt;split&lt;/th&gt;
&lt;th align="left"&gt;target&lt;/th&gt;
&lt;th align="right"&gt;&lt;span class="caps"&gt;RMSE&lt;/span&gt;&lt;/th&gt;
&lt;th align="right"&gt;&lt;span class="caps"&gt;MAE&lt;/span&gt;&lt;/th&gt;
&lt;th align="right"&gt;&lt;span class="caps"&gt;MAPE&lt;/span&gt;&lt;/th&gt;
&lt;th align="right"&gt;&lt;span class="caps"&gt;RMSPE&lt;/span&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="right"&gt;0&lt;/td&gt;
&lt;td align="left"&gt;raw&lt;/td&gt;
&lt;td align="right"&gt;2368.36&lt;/td&gt;
&lt;td align="right"&gt;1249.34&lt;/td&gt;
&lt;td align="right"&gt;0.342704&lt;/td&gt;
&lt;td align="right"&gt;1.65172&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="right"&gt;0&lt;/td&gt;
&lt;td align="left"&gt;log &lt;span class="amp"&gt;&amp;amp;&lt;/span&gt; no corr&lt;/td&gt;
&lt;td align="right"&gt;2464.50&lt;/td&gt;
&lt;td align="right"&gt;1253.19&lt;/td&gt;
&lt;td align="right"&gt;0.307301&lt;/td&gt;
&lt;td align="right"&gt;1.56172&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="right"&gt;0&lt;/td&gt;
&lt;td align="left"&gt;log &lt;span class="amp"&gt;&amp;amp;&lt;/span&gt; sigma2 corr&lt;/td&gt;
&lt;td align="right"&gt;2475.48&lt;/td&gt;
&lt;td align="right"&gt;1253.19&lt;/td&gt;
&lt;td align="right"&gt;0.305424&lt;/td&gt;
&lt;td align="right"&gt;1.27903&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="right"&gt;0&lt;/td&gt;
&lt;td align="left"&gt;log &lt;span class="amp"&gt;&amp;amp;&lt;/span&gt; fitted corr&lt;/td&gt;
&lt;td align="right"&gt;2449.23&lt;/td&gt;
&lt;td align="right"&gt;1251.35&lt;/td&gt;
&lt;td align="right"&gt;0.299577&lt;/td&gt;
&lt;td align="right"&gt;0.85879&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;For each split, we take now the errors on the raw target, i.e. the first row, as baseline and calculate the percentage change along each column for the other rows. Then, we 
calculate for each cell the mean and standard deviation over all 10 splits, resulting&amp;nbsp;in:&lt;/p&gt;
&lt;table border="1" class="dataframe"&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th colspan="2" halign="left"&gt;&lt;span class="caps"&gt;RMSE&lt;/span&gt;&lt;/th&gt;
      &lt;th colspan="2" halign="left"&gt;&lt;span class="caps"&gt;MAE&lt;/span&gt;&lt;/th&gt;
      &lt;th colspan="2" halign="left"&gt;&lt;span class="caps"&gt;MAPE&lt;/span&gt;&lt;/th&gt;
      &lt;th colspan="2" halign="left"&gt;&lt;span class="caps"&gt;RMSPE&lt;/span&gt;&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;target&lt;/th&gt;
      &lt;th&gt;mean&lt;/th&gt;
      &lt;th&gt;std&lt;/th&gt;
      &lt;th&gt;mean&lt;/th&gt;
      &lt;th&gt;std&lt;/th&gt;
      &lt;th&gt;mean&lt;/th&gt;
      &lt;th&gt;std&lt;/th&gt;
      &lt;th&gt;mean&lt;/th&gt;
      &lt;th&gt;std&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;log &lt;span class="amp"&gt;&amp;amp;&lt;/span&gt; no corr&lt;/th&gt;
      &lt;td&gt;+3.42%&lt;/td&gt;
      &lt;td&gt;Â±1.07%p&lt;/td&gt;
      &lt;td&gt;-0.09%&lt;/td&gt;
      &lt;td&gt;Â±0.61%p&lt;/td&gt;
      &lt;td&gt;-10.99%&lt;/td&gt;
      &lt;td&gt;Â±0.65%p&lt;/td&gt;
      &lt;td&gt;-12.08%&lt;/td&gt;
      &lt;td&gt;Â±4.14%p&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;log &lt;span class="amp"&gt;&amp;amp;&lt;/span&gt; sigma2 corr&lt;/th&gt;
      &lt;td&gt;+4.14%&lt;/td&gt;
      &lt;td&gt;Â±0.84%p&lt;/td&gt;
      &lt;td&gt;-0.09%&lt;/td&gt;
      &lt;td&gt;Â±0.61%p&lt;/td&gt;
      &lt;td&gt;-11.03%&lt;/td&gt;
      &lt;td&gt;Â±0.74%p&lt;/td&gt;
      &lt;td&gt;-28.24%&lt;/td&gt;
      &lt;td&gt;Â±3.35%p&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;log &lt;span class="amp"&gt;&amp;amp;&lt;/span&gt; fitted corr&lt;/th&gt;
      &lt;td&gt;+2.75%&lt;/td&gt;
      &lt;td&gt;Â±1.00%p&lt;/td&gt;
      &lt;td&gt;-0.19%&lt;/td&gt;
      &lt;td&gt;Â±0.58%p&lt;/td&gt;
      &lt;td&gt;-13.23%&lt;/td&gt;
      &lt;td&gt;Â±0.68%p&lt;/td&gt;
      &lt;td&gt;-47.27%&lt;/td&gt;
      &lt;td&gt;Â±5.37%p&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Let&amp;#8217;s interpret these evaluation results and note that negative percentages mean an improvement over the error on the untransformed
target, the lower the better. The &lt;span class="caps"&gt;RMSE&lt;/span&gt; column shows us that if we really wanna get the best results for &lt;span class="caps"&gt;RMSE&lt;/span&gt;, transforming the target variable
leads to a worse result compared to a model trained on the original target. The theoretical sigma2 correction makes it even
worse which tells us that the residuals in log-space are not normally distributed. We can check that using for instance the
&lt;a href="https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test"&gt;KolmogorovâSmirnov test&lt;/a&gt;. At least the fitted correction improves somewhat over an uncorrected back-transformation. 
For the &lt;span class="caps"&gt;MAE&lt;/span&gt;, we see an improvement as expected and we know that theoretically there is no need for a correction, thus
the sigma2 correction cell shows exactly the same result. 
Again, noting that the log-normal assumption is quite idealistic, we can understand that the fitted correction is better than
the theoretical optimisation. Coming now to the more appropriate measures for this use-case, we see some nice
percentage improvements for &lt;span class="caps"&gt;MAPE&lt;/span&gt;. Applying the log-transformation here gets us a huge performance boost even without
correction. The sigma2 correction makes it a tad better but is outperformed by the fitted correction. Last but not least,
&lt;span class="caps"&gt;RMSPE&lt;/span&gt; brings us the most pleasing results. Transforming without correction is good, sigma2 makes it even better and the
fitted corrections is simply outstanding, at least percentage-wise compared to the baseline. In absolute numbers, judged
in the respective error measure, we would still need to improve the model a lot to use it in some production use-case
but that was not the actual point of this&amp;nbsp;exercise. &lt;/p&gt;
&lt;p&gt;Having proven mathematically and shown in our example use-case, we can conclude finally that transforming the target
variable is a dangerous business. It can be the key to success and wealth in a Kaggle challenge but it can also lead to 
disaster. It&amp;#8217;s a bit like wielding a double handed sword in a fight. Limbs will be cut off, we should just make sure they are not one of ours.
The rest of this post is only for the inquisitive reader who wants to know exactly where the correction terms for &lt;span class="caps"&gt;RMSPE&lt;/span&gt; 
and &lt;span class="caps"&gt;MAPE&lt;/span&gt; come from. So let&amp;#8217;s wash it all down with some more&amp;nbsp;math.&lt;/p&gt;
&lt;h2&gt;Aftermath&lt;/h2&gt;
&lt;p&gt;So you are still reading? I totally appreciate it and bet you&amp;#8217;re one of those people who wants to know for sure. 
If you ever had religious education, you were certainly a pain in the ass for your teacher and I can feel you.
But let&amp;#8217;s get started for what you are still here, that is proving that &lt;span class="math"&gt;\(-\frac{3}{2}\sigma^2\)&lt;/span&gt; is the right correction
for &lt;span class="caps"&gt;RMSPE&lt;/span&gt; and &lt;span class="math"&gt;\(-\sigma^2\)&lt;/span&gt; for &lt;span class="caps"&gt;MAPE&lt;/span&gt;. Let&amp;#8217;s start with the&amp;nbsp;former.&lt;/p&gt;
&lt;p&gt;We use again our notation &lt;span class="math"&gt;\(\tilde \ast = \log(\ast)\)&lt;/span&gt; for our variables and also to differentiate between the normal
and log-normal distribution. To minimize the error, we&amp;nbsp;have
&lt;/p&gt;
&lt;div class="math"&gt;$$
\mathrm{RMSPE}(\hat y) = \int\left(\frac{y-\hat y}{y}\right)^2\,\tilde f(y)\, \mathrm{d}y = 1 -2\hat y\int\frac{\tilde f(y)}{y}\, \mathrm{d}y + {\hat y}^2\int\frac{\tilde f(y)}{y^2}\, \mathrm{d}y.
$$&lt;/div&gt;
&lt;p&gt;
To find the minimum, we derive by &lt;span class="math"&gt;\(\hat y\)&lt;/span&gt; and set to &lt;span class="math"&gt;\(0\)&lt;/span&gt;, resulting&amp;nbsp;in
&lt;/p&gt;
&lt;div class="math"&gt;$$
\hat y=\frac{\int\frac{\tilde f(y)}{y}\, \mathrm{d}y}{\int\frac{\tilde f(y)}{y^2}\, \mathrm{d}y}
$$&lt;/div&gt;
&lt;p&gt;
Thus, we now need to&amp;nbsp;calculate
&lt;/p&gt;
&lt;div class="math"&gt;$$
q_\alpha = \int\frac{\tilde f(y)}{y^\alpha}\, \mathrm{d}y
$$&lt;/div&gt;
&lt;p&gt;
for &lt;span class="math"&gt;\(\alpha =1,2\)&lt;/span&gt;. To that end, we substitute &lt;span class="math"&gt;\(y=\exp(\tilde y)\)&lt;/span&gt; and using &lt;span class="math"&gt;\(\mathrm{d}y = e^{-\tilde y}\, \mathrm{d}\tilde y\)&lt;/span&gt;, we&amp;nbsp;have 
&lt;/p&gt;
&lt;div class="math"&gt;$$
q_\alpha = \int e^{-\alpha\tilde y}\,\tilde f(e^{\tilde y})e^{\tilde y}\, \mathrm{d}\tilde y = \int e^{-\alpha\tilde y}\,f(\tilde y)\, \mathrm{d}\tilde y.
$$&lt;/div&gt;
&lt;p&gt;
Writing out the exponent and completing the square similar to &lt;span class="math"&gt;\(\eqref{eqn:completing_square}\)&lt;/span&gt;, we&amp;nbsp;obtain
&lt;/p&gt;
&lt;div class="math"&gt;$$
\log(q_\alpha) = -\alpha \mu +\frac12 \alpha^2\sigma^2,
$$&lt;/div&gt;
&lt;p&gt;
leading in total&amp;nbsp;to
&lt;/p&gt;
&lt;div class="math"&gt;$$
\log(\hat y)=\log(q_1)-\log(q_2) = \mu -\frac{3}{2}\sigma^2.
$$&lt;/div&gt;
&lt;p&gt;
Subsequently, the correction term for &lt;span class="caps"&gt;RMSPE&lt;/span&gt; is &lt;span class="math"&gt;\(-\frac{3}{2}\sigma^2\)&lt;/span&gt;. For &lt;span class="caps"&gt;MAPE&lt;/span&gt; we&amp;nbsp;have
&lt;/p&gt;
&lt;div class="math"&gt;$$
\mathrm{MAPE}(\hat y) = \int_0^{\infty}\frac{\vert y-\hat y\vert}{y}\,\tilde f(y)\, \mathrm{d}y = \int_{\hat y}^{\infty}1 - \frac{\hat y}{y},\tilde f(y)\, \mathrm{d}y -\int_0^{\hat y}1-\frac{\hat y}{y}\,\tilde f(y)\, \mathrm{d}y,
$$&lt;/div&gt;
&lt;p&gt;
and after deriving by &lt;span class="math"&gt;\(\hat y\)&lt;/span&gt; as well as setting to 0, we need to find &lt;span class="math"&gt;\(\hat y\)&lt;/span&gt; such&amp;nbsp;that
&lt;/p&gt;
&lt;div class="math"&gt;$$
\int_{\hat y}^{\infty}\frac{1}{y}\,\tilde f(y)\, \mathrm{d}y - \int_0^{\hat y}\frac{1}{y}\,\tilde f(y)\, \mathrm{d}y = 0.
$$&lt;/div&gt;
&lt;p&gt;
Doing the same substitution as with &lt;span class="caps"&gt;RMSPE&lt;/span&gt;, results&amp;nbsp;in
&lt;/p&gt;
&lt;div class="math"&gt;$$
\int_{\log(\hat y)}^{\infty}e^{-\tilde y}\,f(\tilde y)\, \mathrm{d} \tilde y - \int_{-\infty}^{\log(\hat y)}e^{-\tilde y}\,f(\tilde y)\, \mathrm{d}\tilde y = 0.
$$&lt;/div&gt;
&lt;p&gt;
Again, we complete the square of the exponent similar to &lt;span class="math"&gt;\(\eqref{eqn:completing_square}\)&lt;/span&gt;, resulting&amp;nbsp;in
&lt;/p&gt;
&lt;div class="math"&gt;\begin{equation}
e^{-\mu+\frac{1}{2}\sigma^2}\left(\int_{\log(\hat y)}^{\infty}f_s(\tilde y)\, \mathrm{d} \tilde y - \int_{-\infty}^{\log(\hat y)}f_s(\tilde y)\, \mathrm{d}\tilde y\right) = 0,
\label{eqn:mape-proof}
\end{equation}&lt;/div&gt;
&lt;p&gt;
where 
&lt;/p&gt;
&lt;div class="math"&gt;$$
f_s(x) = {\frac {1}{ {\sqrt {2\pi\sigma^2 \,}}}}\exp \left(-{\frac {(x - (\mu - \sigma^2) )^{2}}{2\sigma ^{2}}}\right).
$$&lt;/div&gt;
&lt;p&gt;
We need the two integrals in &lt;span class="math"&gt;\(\eqref{eqn:mape-proof}\)&lt;/span&gt; to be equal to fulfill the equation, thus &lt;span class="math"&gt;\(\log(\hat y)\)&lt;/span&gt; needs to be
the median. With the shifted normal distribution &lt;span class="math"&gt;\(f_s(x)\)&lt;/span&gt;, we have that for &lt;span class="math"&gt;\(\log(\hat y) = \mu - \sigma^2\)&lt;/span&gt;. Consequently,
the correction term for &lt;span class="caps"&gt;MAPE&lt;/span&gt; is &lt;span class="math"&gt;\(-\sigma^2\)&lt;/span&gt;.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js','color.js'], equationNumbers: { autoNumber: 'auto' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="post"></category><category term="python"></category><category term="data science"></category><category term="mathematics"></category></entry><entry><title>More Efficient UD(A)Fs withÂ PySpark</title><link href="https://florianwilhelm.info/2019/04/more_efficient_udfs_with_pyspark/" rel="alternate"></link><published>2019-04-19T12:30:00+02:00</published><updated>2019-04-19T12:30:00+02:00</updated><author><name>Florian Wilhelm</name></author><id>tag:florianwilhelm.info,2019-04-19:/2019/04/more_efficient_udfs_with_pyspark/</id><summary type="html">&lt;p&gt;With the release of Spark 2.3 implementing user defined functions with PySpark became a lot easier and faster. Unfortunately, there are still some rough edges when it comes to complex data types that need to be worked&amp;nbsp;around.&lt;/p&gt;</summary><content type="html">&lt;p&gt;Some time has passed since my blog post on &lt;a href="https://florianwilhelm.info/2017/10/efficient_udfs_with_pyspark/"&gt;Efficient &lt;span class="caps"&gt;UD&lt;/span&gt;(A)Fs with PySpark&lt;/a&gt; which demonstrated how to define &lt;em&gt;User-Defined Aggregation Function&lt;/em&gt; (&lt;span class="caps"&gt;UDAF&lt;/span&gt;) with &lt;a href="https://spark.apache.org/docs/latest/api/python/index.html"&gt;PySpark&lt;/a&gt; 2.1 that allow you to use &lt;a href="http://pandas.pydata.org/"&gt;Pandas&lt;/a&gt;. Meanwhile, things got a lot easier with the release of Spark 2.3 which provides the &lt;a href="http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.pandas_udf"&gt;pandas_udf&lt;/a&gt; decorator. This decorator gives you the same functionality as our custom &lt;code&gt;pandas_udaf&lt;/code&gt; in the former post but performs much faster if &lt;a href="https://arrow.apache.org/"&gt;Apache Arrow&lt;/a&gt; is activated. &lt;em&gt;Nice, so life is good now? No more workarounds!? Well,&amp;nbsp;almost&amp;#8230;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;If you are just using simple data types in your Spark dataframes everything will work and even blazingly fast if you got Arrow activated but don&amp;#8217;t you dare dealing with complex data types like maps (dictionaries), arrays (lists) and structs. In that case, all you will get is a &lt;code&gt;TypeError: Unsupported type in conversion to Arrow&lt;/code&gt; which is already tracked under issue &lt;a href="https://jira.apache.org/jira/browse/SPARK-21187"&gt;&lt;span class="caps"&gt;SPARK&lt;/span&gt;-21187&lt;/a&gt;. Even a simple &lt;code&gt;toPandas()&lt;/code&gt; does not work which might get you to deactivate Arrow support altogether but this would also keep you from using &lt;code&gt;pandas_udf&lt;/code&gt; which is really&amp;nbsp;nice&amp;#8230; &lt;/p&gt;
&lt;p&gt;To save you from this dilemma, this blog post will demonstrate how to work around the current limitations of Arrow without too much hassle. I tested this on Spark 2.3 and it should also work on Spark 2.4. But before we start, let&amp;#8217;s first take a look into which features &lt;code&gt;pandas_udf&lt;/code&gt; provides and why we should make use of&amp;nbsp;it.&lt;/p&gt;
&lt;h2&gt;Features of Spark 2.3&amp;#8217;s&amp;nbsp;pandas_udf&lt;/h2&gt;
&lt;p&gt;Just to give you a little overview about the functionality, take a look at the table&amp;nbsp;below.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;function type&lt;/th&gt;
&lt;th&gt;Operation&lt;/th&gt;
&lt;th&gt;Input â Output&lt;/th&gt;
&lt;th&gt;Pandas equivalent&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;SCALAR&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Mapping&lt;/td&gt;
&lt;td&gt;Series â Series&lt;/td&gt;
&lt;td&gt;&lt;code&gt;df.transform(...)&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;GROUPED_MAP&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Group &lt;span class="amp"&gt;&amp;amp;&lt;/span&gt; Map&lt;/td&gt;
&lt;td&gt;DataFrame â DataFrame&lt;/td&gt;
&lt;td&gt;&lt;code&gt;df.apply(...)&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;GROUPED_AGG&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Reduce&lt;/td&gt;
&lt;td&gt;Series â Scalar&lt;/td&gt;
&lt;td&gt;&lt;code&gt;df.aggregate(...)&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Besides the return type of your &lt;span class="caps"&gt;UDF&lt;/span&gt;, the &lt;code&gt;pandas_udf&lt;/code&gt; needs you to specify a function type which describes the general behavior of your &lt;span class="caps"&gt;UDF&lt;/span&gt;. If you just want to map a scalar onto a scalar or equivalently a vector onto a vector with the same length, you would pass &lt;code&gt;PandasUDFType.SCALAR&lt;/code&gt;. This would also determine that your &lt;span class="caps"&gt;UDF&lt;/span&gt; retrieves a Pandas series as input and needs to return a series of the same length. It basically does the same as the &lt;code&gt;transform&lt;/code&gt; method of a Pandas dataframe. A &lt;code&gt;GROUPED_MAP&lt;/code&gt; &lt;span class="caps"&gt;UDF&lt;/span&gt; is the most flexible one since it gets a Pandas dataframe and is allowed to return a modified or new dataframe with an arbitrary shape. From Spark 2.4 on you also have the reduce operation &lt;code&gt;GROUPED_AGG&lt;/code&gt; which takes a Pandas Series as input and needs to return a scalar. Read more details about &lt;code&gt;pandas_udf&lt;/code&gt; in the &lt;a href="https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.pandas_udf"&gt;official Spark documentation&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Basic&amp;nbsp;idea&lt;/h2&gt;
&lt;p&gt;Our workaround will be quite simple. We make use of the &lt;a href="https://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=to_json#pyspark.sql.functions.to_json"&gt;to_json&lt;/a&gt; function and convert all columns with complex data types to &lt;span class="caps"&gt;JSON&lt;/span&gt; strings. Since Arrow can easily handle strings, we are able to use the &lt;a href="http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.pandas_udf"&gt;pandas_udf&lt;/a&gt; decorator. Within our &lt;span class="caps"&gt;UDF&lt;/span&gt;, we convert these columns back to their original types and do our actual work. If we want to return columns with complex types, we just do everything the other way around. That means we convert those columns to &lt;span class="caps"&gt;JSON&lt;/span&gt; within our &lt;span class="caps"&gt;UDF&lt;/span&gt;, return the Pandas dataframe and convert eventually the corresponding columns in the Spark dataframe from &lt;span class="caps"&gt;JSON&lt;/span&gt; to complex types with &lt;a href="https://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=to_json#pyspark.sql.functions.from_json"&gt;from_json&lt;/a&gt;. The following figure illustrates the&amp;nbsp;process.&lt;/p&gt;
&lt;figure&gt;
&lt;p align="center"&gt;
&lt;img class="noZoom" src="/images/pandas_udf_complex.png" alt="Converting complex data types to JSON before applying the UDF"&gt;
&lt;/p&gt;
&lt;/figure&gt;

&lt;p&gt;Our workaround involves a lot of bookkeeping and surely is not that user-friendly. Like we did in the last blog post, it is again possible to hide much of the details with the help of a &lt;a href="https://wiki.python.org/moin/PythonDecorators#What_is_a_Decorator"&gt;Python decorator&lt;/a&gt; from a user. So let&amp;#8217;s get&amp;nbsp;started!&lt;/p&gt;
&lt;h2&gt;Implementation&lt;/h2&gt;
&lt;p&gt;We split our implementation into three different kinds of functionalities: 1. functions that convert a Spark dataframe to and from &lt;span class="caps"&gt;JSON&lt;/span&gt;, 2. functions that do the same for Pandas dataframes and 3. we combine all of them in one decorator. The final and extended implementation can be found in the file &lt;a href="https://florianwilhelm.info/src/pyspark23_udaf.py"&gt;pyspark23_udaf.py&lt;/a&gt; where also some logging mechanism for easier debugging of UDFs was&amp;nbsp;added. &lt;/p&gt;
&lt;h3&gt;1. Conversion of Spark&amp;nbsp;Dataframe&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;pyspark.sql.types&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;MapType&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;StructType&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ArrayType&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;StructField&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;pyspark.sql.functions&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;to_json&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;from_json&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;is_complex_dtype&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;Check if dtype is a complex type&lt;/span&gt;

&lt;span class="sd"&gt;    Args:&lt;/span&gt;
&lt;span class="sd"&gt;        dtype: Spark Datatype&lt;/span&gt;

&lt;span class="sd"&gt;    Returns:&lt;/span&gt;
&lt;span class="sd"&gt;        Bool: if dtype is complex&lt;/span&gt;
&lt;span class="sd"&gt;    &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="nb"&gt;isinstance&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;MapType&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;StructType&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ArrayType&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;complex_dtypes_to_json&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;Converts all columns with complex dtypes to JSON&lt;/span&gt;

&lt;span class="sd"&gt;    Args:&lt;/span&gt;
&lt;span class="sd"&gt;        df: Spark dataframe&lt;/span&gt;

&lt;span class="sd"&gt;    Returns:&lt;/span&gt;
&lt;span class="sd"&gt;        tuple: Spark dataframe and dictionary of converted columns and their data types&lt;/span&gt;
&lt;span class="sd"&gt;    &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;conv_cols&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;dict&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;selects&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;field&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;schema&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;is_complex_dtype&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;field&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dataType&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="n"&gt;conv_cols&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;field&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;field&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dataType&lt;/span&gt;
            &lt;span class="n"&gt;selects&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;to_json&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;field&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;alias&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;field&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
        &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;selects&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;field&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;select&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;selects&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;conv_cols&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;complex_dtypes_from_json&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;col_dtypes&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;Converts JSON columns to complex types&lt;/span&gt;

&lt;span class="sd"&gt;    Args:&lt;/span&gt;
&lt;span class="sd"&gt;        df: Spark dataframe&lt;/span&gt;
&lt;span class="sd"&gt;        col_dtypes (dict): dictionary of columns names and their datatype&lt;/span&gt;

&lt;span class="sd"&gt;    Returns:&lt;/span&gt;
&lt;span class="sd"&gt;        Spark dataframe&lt;/span&gt;
&lt;span class="sd"&gt;    &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;selects&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;column&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;columns&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;column&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;col_dtypes&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;keys&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
            &lt;span class="n"&gt;schema&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;StructType&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;StructField&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;root&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;col_dtypes&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;column&lt;/span&gt;&lt;span class="p"&gt;])])&lt;/span&gt;
            &lt;span class="n"&gt;selects&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;from_json&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;column&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;schema&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;getItem&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;root&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;alias&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;column&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
        &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;selects&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;column&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;select&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;selects&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The function &lt;code&gt;complex_dtypes_to_json&lt;/code&gt; converts a given Spark dataframe to a new dataframe with all columns that have complex types replaced by &lt;span class="caps"&gt;JSON&lt;/span&gt; strings. Besides the converted dataframe, it also returns a dictionary with column names and their original data types which where converted. This information is used by &lt;code&gt;complex_dtypes_from_json&lt;/code&gt; to convert exactly those columns back to their original type. You might find it strange that we define some &lt;code&gt;root&lt;/code&gt; node in the schema. This is necessary due to some restrictions of Spark&amp;#8217;s &lt;a href="https://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=to_json#pyspark.sql.functions.from_json"&gt;from_json&lt;/a&gt; that we circumvent by this. After the conversion, we drop this &lt;code&gt;root&lt;/code&gt; struct again so that &lt;code&gt;complex_dtypes_to_json&lt;/code&gt; and &lt;code&gt;complex_dtypes_from_json&lt;/code&gt; are inverses of each other. We can now also easily define a &lt;code&gt;toPandas&lt;/code&gt; which also works with complex Spark&amp;nbsp;dataframes.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;toPandas&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;Same as df.toPandas() but converts complex types to JSON first&lt;/span&gt;

&lt;span class="sd"&gt;    Args:&lt;/span&gt;
&lt;span class="sd"&gt;        df: Spark dataframe&lt;/span&gt;

&lt;span class="sd"&gt;    Returns:&lt;/span&gt;
&lt;span class="sd"&gt;        Pandas dataframe&lt;/span&gt;
&lt;span class="sd"&gt;    &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;complex_dtypes_to_json&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;)[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;toPandas&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h3&gt;2. Conversion of Pandas&amp;nbsp;Dataframe&lt;/h3&gt;
&lt;p&gt;Analogously, we define the same functions as above but for Pandas dataframes. The difference is that we need to know which columns to convert to complex types for our actual &lt;span class="caps"&gt;UDF&lt;/span&gt; since we want to avoid probing every column containing strings. In the conversion to &lt;span class="caps"&gt;JSON&lt;/span&gt;, we add the &lt;code&gt;root&lt;/code&gt; node as explained&amp;nbsp;above. &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;json&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;cols_from_json&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;columns&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;Converts Pandas dataframe colums from json&lt;/span&gt;

&lt;span class="sd"&gt;    Args:&lt;/span&gt;
&lt;span class="sd"&gt;        df (dataframe): Pandas DataFrame&lt;/span&gt;
&lt;span class="sd"&gt;        columns (iter): list of or iterator over column names&lt;/span&gt;

&lt;span class="sd"&gt;    Returns:&lt;/span&gt;
&lt;span class="sd"&gt;        dataframe: new dataframe with converted columns&lt;/span&gt;
&lt;span class="sd"&gt;    &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;column&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;columns&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;column&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;column&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;apply&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;json&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;loads&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;ct_val_to_json&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;Convert a scalar complex type value to JSON&lt;/span&gt;

&lt;span class="sd"&gt;    Args:&lt;/span&gt;
&lt;span class="sd"&gt;        value: map or list complex value&lt;/span&gt;

&lt;span class="sd"&gt;    Returns:&lt;/span&gt;
&lt;span class="sd"&gt;        str: JSON string&lt;/span&gt;
&lt;span class="sd"&gt;    &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;json&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dumps&lt;/span&gt;&lt;span class="p"&gt;({&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;root&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;cols_to_json&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;columns&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;Converts Pandas dataframe columns to json and adds root handle&lt;/span&gt;

&lt;span class="sd"&gt;    Args:&lt;/span&gt;
&lt;span class="sd"&gt;        df (dataframe): Pandas DataFrame&lt;/span&gt;
&lt;span class="sd"&gt;        columns ([str]): list of column names&lt;/span&gt;

&lt;span class="sd"&gt;    Returns:&lt;/span&gt;
&lt;span class="sd"&gt;        dataframe: new dataframe with converted columns&lt;/span&gt;
&lt;span class="sd"&gt;    &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;column&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;columns&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;column&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;column&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;apply&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ct_val_to_json&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h3&gt;3.&amp;nbsp;Decorator&lt;/h3&gt;
&lt;p&gt;At this point we got everything we need for our final decorators named &lt;code&gt;pandas_udf_ct&lt;/code&gt; combining all our ingredients. Like Spark&amp;#8217;s official &lt;a href="http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.pandas_udf"&gt;pandas_udf&lt;/a&gt;, our decorator takes the arguments &lt;code&gt;returnType&lt;/code&gt; and &lt;code&gt;functionType&lt;/code&gt;. It&amp;#8217;s just a tad more complicated in the sense that you first have to pass &lt;code&gt;returnType&lt;/code&gt;, &lt;code&gt;functionType&lt;/code&gt; which leaves you with some special decorator. A function decorated with such a decorator takes the parameters &lt;code&gt;cols_in&lt;/code&gt; and &lt;code&gt;cols_out&lt;/code&gt; which specify which columns need to be converted to and from &lt;span class="caps"&gt;JSON&lt;/span&gt;. Only after passing those you end up with the actual &lt;span class="caps"&gt;UDF&lt;/span&gt; that you defined. No need to despair, an example below illustrates the usage but first we take a look at the&amp;nbsp;implementation.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;json&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;functools&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;wraps&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;pyspark.sql.functions&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;pandas_udf&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;PandasUDFType&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pandas&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pd&lt;/span&gt;

&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;pandas_udf_ct&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;object&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;Decorator for UDAFs with Spark &amp;gt;= 2.3 and complex types&lt;/span&gt;

&lt;span class="sd"&gt;    Args:&lt;/span&gt;
&lt;span class="sd"&gt;        returnType: the return type of the user-defined function. The value can be either a &lt;/span&gt;
&lt;span class="sd"&gt;                    pyspark.sql.types.DataType object or a DDL-formatted type string.&lt;/span&gt;
&lt;span class="sd"&gt;        functionType: an enum value in pyspark.sql.functions.PandasUDFType. Default: SCALAR.&lt;/span&gt;

&lt;span class="sd"&gt;    Returns:&lt;/span&gt;
&lt;span class="sd"&gt;        Function with arguments `cols_in` and `cols_out` defining column names having complex &lt;/span&gt;
&lt;span class="sd"&gt;        types that need to be transformed during input and output for GROUPED_MAP. In case of &lt;/span&gt;
&lt;span class="sd"&gt;        SCALAR, we are dealing with a series and thus transformation is done if `cols_in` or &lt;/span&gt;
&lt;span class="sd"&gt;        `cols_out` evaluates to `True`. &lt;/span&gt;
&lt;span class="sd"&gt;        Calling this functions with these arguments returns the actual UDF.&lt;/span&gt;
&lt;span class="sd"&gt;    &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;returnType&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;functionType&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;return_type&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;returnType&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;function_type&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;functionType&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__call__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;func&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="nd"&gt;@wraps&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;func&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;converter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cols_in&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cols_out&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;cols_in&lt;/span&gt; &lt;span class="ow"&gt;is&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                &lt;span class="n"&gt;cols_in&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
            &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;cols_out&lt;/span&gt; &lt;span class="ow"&gt;is&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                &lt;span class="n"&gt;cols_out&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

            &lt;span class="nd"&gt;@pandas_udf&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;return_type&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;function_type&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;udf_wrapper&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
                &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="nb"&gt;isinstance&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataFrame&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
                    &lt;span class="n"&gt;values&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;cols_from_json&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cols_in&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
                &lt;span class="k"&gt;elif&lt;/span&gt; &lt;span class="nb"&gt;isinstance&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Series&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="ow"&gt;and&lt;/span&gt; &lt;span class="n"&gt;cols_in&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                    &lt;span class="n"&gt;values&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;apply&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;json&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;loads&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
                &lt;span class="n"&gt;res&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;func&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
                &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;function_type&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;PandasUDFType&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;GROUPED_MAP&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="nb"&gt;isinstance&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;res&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Series&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
                        &lt;span class="n"&gt;res&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;res&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;to_frame&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt;
                    &lt;span class="n"&gt;res&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;cols_to_json&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;res&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cols_out&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
                &lt;span class="k"&gt;elif&lt;/span&gt; &lt;span class="n"&gt;cols_out&lt;/span&gt; &lt;span class="ow"&gt;and&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;function_type&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;PandasUDFType&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;SCALAR&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                    &lt;span class="n"&gt;res&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;res&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;apply&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ct_val_to_json&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
                &lt;span class="k"&gt;elif&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;isinstance&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;res&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;dict&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="ow"&gt;and&lt;/span&gt; 
                      &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;function_type&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;PandasUDFType&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;GROUPED_AGG&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
                    &lt;span class="n"&gt;res&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ct_val_to_json&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;res&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
                &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;res&lt;/span&gt;

            &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;udf_wrapper&lt;/span&gt;

        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;converter&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;It&amp;#8217;s just a typical decorator-with-parameters implementation but with one more layer of wrapping for &lt;code&gt;cols_in&lt;/code&gt; and &lt;code&gt;cols_out&lt;/code&gt;.  &lt;/p&gt;
&lt;h2&gt;Usage&lt;/h2&gt;
&lt;p&gt;An example says more than one thousand words of explanation. Let&amp;#8217;s first create some dummy Spark dataframe with complex data&amp;nbsp;types:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;pyspark.sql.types&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Row&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;pyspark.sql&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;SparkSession&lt;/span&gt;

&lt;span class="n"&gt;spark&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;SparkSession&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;builder&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;getOrCreate&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;spark&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;conf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;spark.sql.execution.arrow.enabled&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;true&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;spark&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;createDataFrame&lt;/span&gt;&lt;span class="p"&gt;([(&lt;/span&gt;&lt;span class="mf"&gt;1.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;a&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;},&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;a&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;a&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;Row&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt;
                            &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;2.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;b&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;},&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;a&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;b&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;Row&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;42&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt;
                            &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;3.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;a&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;b&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;},&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;d&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;e&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;Row&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))],&lt;/span&gt;
                           &lt;span class="n"&gt;schema&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;vals&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;maps&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;lists&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;structs&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;drop&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;lists&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# only Spark 2.4 supports ArrayTypes in to_json!&lt;/span&gt;
&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;For sake of simplicity, let&amp;#8217;s say we just want to add to the dictionaries in the &lt;code&gt;maps&lt;/code&gt; column a key &lt;code&gt;x&lt;/code&gt; with value &lt;code&gt;42&lt;/code&gt;. But first, we use &lt;code&gt;complex_dtypes_to_json&lt;/code&gt; to get a converted Spark dataframe &lt;code&gt;df_json&lt;/code&gt; and the converted columns &lt;code&gt;ct_cols&lt;/code&gt;. We define then the &lt;span class="caps"&gt;UDF&lt;/span&gt; &lt;code&gt;normalize&lt;/code&gt; and decorate it with our &lt;code&gt;pandas_udf_ct&lt;/code&gt; specifying the return type using &lt;code&gt;dfj_json.schema&lt;/code&gt; (since we only want simple data types) and the function type &lt;code&gt;GROUPED_MAP&lt;/code&gt;. &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;df_json&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ct_cols&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;complex_dtypes_to_json&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;change_vals&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dct&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;dct&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;x&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;42&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;dct&lt;/span&gt;

&lt;span class="nd"&gt;@pandas_udf_ct&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;df_json&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;schema&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;PandasUDFType&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;GROUPED_MAP&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;normalize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pdf&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;pdf&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;maps&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;apply&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;change_vals&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;pdf&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Just for demonstration, we now group by the &lt;code&gt;vals&lt;/code&gt; column of &lt;code&gt;df_json&lt;/code&gt; and apply our &lt;code&gt;normalize&lt;/code&gt; &lt;span class="caps"&gt;UDF&lt;/span&gt; on each group. Instead of just passing &lt;code&gt;normalize&lt;/code&gt; we have to call it first with parameters &lt;code&gt;cols_in&lt;/code&gt; and &lt;code&gt;cols_out&lt;/code&gt; as explained before. As input columns, we pass the output &lt;code&gt;ct_cols&lt;/code&gt; from our &lt;code&gt;complex_dtypes_to_json&lt;/code&gt; function and since we do not change the shape of our dataframe within the &lt;span class="caps"&gt;UDF&lt;/span&gt;, we use the same for the output &lt;code&gt;cols_out&lt;/code&gt;. In case your &lt;span class="caps"&gt;UDF&lt;/span&gt; removes columns or adds additional ones with complex data types, you would have to change &lt;code&gt;cols_out&lt;/code&gt; accordingly. As a final step we use &lt;code&gt;complex_dtypes_from_json&lt;/code&gt; to convert the &lt;span class="caps"&gt;JSON&lt;/span&gt; strings of our transformed Spark dataframe back to complex data&amp;nbsp;types.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;df_json&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;df_json&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;groupby&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;vals&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;apply&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;normalize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cols_in&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;ct_cols&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cols_out&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;ct_cols&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;df_final&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;complex_dtypes_from_json&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;df_json&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ct_cols&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;df_final&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;We have shown a practical workaround to deal with UDFs and complex data types for Spark 2.3/4. As with every workaround, it&amp;#8217;s far from perfect and hopefully the issue &lt;a href="https://jira.apache.org/jira/browse/SPARK-21187"&gt;&lt;span class="caps"&gt;SPARK&lt;/span&gt;-21187&lt;/a&gt; will be resolved soon rendering this workaround unnecessary. That being said, the presented workaround has been running smoothly in production for quite a while now and my data science colleagues adapted this framework to write their own UDFs based on&amp;nbsp;it.&lt;/p&gt;</content><category term="post"></category><category term="spark"></category><category term="python"></category><category term="big data"></category></entry><entry><title>Working efficiently with JupyterLabÂ Notebooks</title><link href="https://florianwilhelm.info/2018/11/working_efficiently_with_jupyter_lab/" rel="alternate"></link><published>2018-11-08T14:00:00+01:00</published><updated>2018-11-08T14:00:00+01:00</updated><author><name>Florian Wilhelm</name></author><id>tag:florianwilhelm.info,2018-11-08:/2018/11/working_efficiently_with_jupyter_lab/</id><summary type="html">&lt;p&gt;Being in the data science domain for quite some years, I have seen good Jupyter notebooks but also a lot of ugly. Notebooks can have the perfect balance between text, code and visualisations but how often do your notebooks rather get messy and incomprehensible after a while? Follow some simple best practices to work more efficiently with your&amp;nbsp;notebooks.&lt;/p&gt;</summary><content type="html">&lt;h2&gt;Motivation&lt;/h2&gt;
&lt;p&gt;If you have ever done something analytical or anything closely related to data science in Python, there is just no way you have not heard of Jupyter or IPython notebooks. In a nutshell, a notebook is an interactive document displayed in your browser which contains source code, e.g. Python and R, as well as rich text elements like paragraphs, equations, figures, links, etc. This combination makes it extremely useful for explorative tasks where the source code, documentation and even visualisations of your analysis are strongly intertwined. Due to this unique characteristic, Jupyter notebooks have achieved a strong adoption particularly in the data science community. But as Pythagoras already noted &amp;#8220;If there be light, then there is darkness.&amp;#8221; and with Jupyter notebooks it&amp;#8217;s no difference of&amp;nbsp;course.&lt;/p&gt;
&lt;p&gt;Being in the data science domain for quite some years, I have seen good but also a lot of ugly. Notebooks that are beautifully designed and perfectly convey ideas and concepts by having the perfect balance between text, code and visualisations like in my all time favourite &lt;a href="https://github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers"&gt;Probabilistic Programming and Bayesian Methods for Hackers&lt;/a&gt;. In strong contrast to this, and actually more often to find in practise, are notebooks with cells containing pages of incomprehensible source code, distracting you from the actual analysis. Also sharing these notebooks is quite often an unnecessary pain. Notebooks that need you to tamper with the &lt;code&gt;PYTHONPATH&lt;/code&gt; or to start Jupyter from a certain directory for modules to import correctly. In this blog post I will introduce several best practices and techniques that will help you to create notebooks which are focused, easy to comprehend and to work&amp;nbsp;with. &lt;/p&gt;
&lt;figure&gt;
&lt;p align="center"&gt;
&lt;img class="noZoom" src="/images/jupyter_worker.png" alt="Worker carrying JupyterLab"&gt;
&lt;/p&gt;
&lt;/figure&gt;

&lt;h2&gt;History&lt;/h2&gt;
&lt;p&gt;Before we get into the actual subject let&amp;#8217;s take some time to understand how &lt;a href="http://jupyter.org/"&gt;Project Jupyter&lt;/a&gt; evolved and where it came from. This will also clarify the confusion people sometimes have over IPython, Jupyter and JupyterLab notebooks. In 2001 Fernando PÃ©rez was quite dissatisfied with the capabilities of Python&amp;#8217;s interactive prompt compared to the commercial notebook environments of Maple and Mathematica which he really liked. In order to improve upon this situation he laid the foundation for a notebook environment by building &lt;a href="https://ipython.org/"&gt;IPython&lt;/a&gt; (Interactive Python), a command shell for interactive computing. IPython quickly became a success as the &lt;a href="https://en.wikipedia.org/wiki/Read%E2%80%93eval%E2%80%93print_loop"&gt;&lt;span class="caps"&gt;REPL&lt;/span&gt;&lt;/a&gt; of choice for many users but it was only a small step towards a graphical interactive notebook environment. Several years and many failed attempts later, it took until late 2010 for Grain Granger and several others to develop a first graphical console, named &lt;a href="https://qtconsole.readthedocs.io/"&gt;QTConsole&lt;/a&gt; which was based on &lt;a href="https://www.qt.io/"&gt;&lt;span class="caps"&gt;QT&lt;/span&gt;&lt;/a&gt;. As the speed of development picked up, IPython 0.12 was released only one year later in December 2011 and included for the first time a browser-based IPython notebook environment. People were psyched about the possibilities &lt;em&gt;IPython notebook&lt;/em&gt; provided them and the adoption rose&amp;nbsp;quickly. &lt;/p&gt;
&lt;p&gt;In 2014, &lt;a href="http://jupyter.org/"&gt;Project Jupyter&lt;/a&gt; started as a spin-off project from IPython for several reasons. At that time IPython encompassed an interactive shell, the notebook server, the &lt;span class="caps"&gt;QT&lt;/span&gt; console and other parts in a single repository with the obvious organisational downsides. After the spin-off, IPython concentrated on providing solely an interactive shell for Python while Project Jupyter itself started as an umbrella organisation for several components like &lt;a href="https://jupyter-notebook.readthedocs.io/"&gt;Jupyter notebook&lt;/a&gt; and &lt;a href="https://qtconsole.readthedocs.io/"&gt;QTConsole&lt;/a&gt;, which were moved over from IPython, as well as many others. Another reason for the split was the fact that Jupyter wanted to support other languages besides Python like &lt;a href="https://www.r-project.org/"&gt;R&lt;/a&gt;, &lt;a href="https://julialang.org/"&gt;Julia&lt;/a&gt; and more. The name Jupyter itself was chosen to reflect the fact that the three most popular languages in data science are supported among others, thus Jupyter is actually an acronym for &lt;strong&gt;Ju&lt;/strong&gt;lia, &lt;strong&gt;Pyt&lt;/strong&gt;hon, &lt;strong&gt;R&lt;/strong&gt;. &lt;/p&gt;
&lt;p&gt;But evolution never stops and the source code of Jupyter notebook built on the web technologies of 2011 started to show its age. As the code grew bigger, people also started to realise that it actually is more than just a notebook. Some parts of it rather dealt with managing files, running notebooks and parallel workers. This eventually led again to the idea of splitting these functionalities and laid the foundation for &lt;a href="https://jupyterlab.readthedocs.io/"&gt;JupyterLab&lt;/a&gt;. JupyterLab is an interactive development environment for working with notebooks, code and data. It has full support for Jupyter notebooks and enables you to use text editors, terminals, data file viewers, and other custom components side by side with notebooks in a tabbed work area. Since February 2018 it&amp;#8217;s officially considered to be &lt;a href="https://blog.jupyter.org/jupyterlab-is-ready-for-users-5a6f039b8906"&gt;ready for users&lt;/a&gt; and the 1.0 release is expected to happen end of&amp;nbsp;2018. &lt;/p&gt;
&lt;p&gt;According to my experience in the last months, JupyterLab is absolutely ready and I recommend everyone to migrate to it. In this post, I will thus focus on JupyterLab and the term notebook or sometimes even Jupyter notebook actually refers to a notebook that was opened with JupyterLab. Practically this means that you run &lt;code&gt;jupyter lab&lt;/code&gt; instead of &lt;code&gt;jupyter notebook&lt;/code&gt;. If you are interested in more historical details read the blog posts of &lt;a href="http://blog.fperez.org/2012/01/ipython-notebook-historical.html"&gt;Fernando PÃ©rez&lt;/a&gt; and &lt;a href="https://www.datacamp.com/community/blog/ipython-jupyter"&gt;Karlijn Willems&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Preparation &lt;span class="amp"&gt;&amp;amp;&lt;/span&gt;&amp;nbsp;Installation&lt;/h2&gt;
&lt;p&gt;The first good practice can actually be learnt before even starting JupyterLab. Since we want our analysis to be reproducible and shareable with colleagues it&amp;#8217;s a good practice to create a clean, isolated environment for every task. For Python you got basically two options &lt;a href="https://virtualenv.pypa.io/"&gt;virtualenv&lt;/a&gt; (also descendants like &lt;a href="https://pipenv.readthedocs.io/"&gt;pipenv&lt;/a&gt;) or &lt;a href="https://conda.io/"&gt;conda&lt;/a&gt; to achieve this. My favorite is conda for several reasons. First of all conda is a package manager of the &lt;a href="https://www.anaconda.com/distribution/"&gt;Anaconda&lt;/a&gt; distribution and allows you to install more than just Python packages. Anaconda is more like a whole operation system coming with packages for Python, R and C/C++ system libraries like libc. From this point of view it&amp;#8217;s much more than what virtualenv provides, since conda will also install system libraries like glibc if need be. Also the Python interpreter itself is installed separately into an isolated environment and thus independent of the one provided by your system. This makes it possible to easily pin down even the Python version of your environment. The tool &lt;a href="https://github.com/pyenv/pyenv"&gt;pyenv&lt;/a&gt; allows you to do the same within the virtualenv ecosystem but conda feels just more integrated and gives a unified approach. In total, conda allows for much more fined-grained control of what is going on in your virtual environment than virtualenv with less side effects induced by your&amp;nbsp;system. &lt;/p&gt;
&lt;p&gt;For these reasons conda is much more common than virtualenv in the field of data science, thus we will use it in this tutorial. Still, everything shown here can analogously be conducted with the help of &lt;a href="https://virtualenv.pypa.io/"&gt;virtualenv&lt;/a&gt;/&lt;a href="https://pipenv.readthedocs.io/"&gt;pipenv&lt;/a&gt; and all the concepts still apply as is also illustrated in a &lt;a href="https://cprohm.de/article/notebooks-and-modules.html"&gt;blog post of Christopher Prohm&lt;/a&gt;. For this tutorial, I assume you have &lt;a href="https://conda.io/miniconda.html"&gt;Miniconda&lt;/a&gt; installed on your system. Besides this, every programmer&amp;#8217;s machine should have &lt;a href="https://git-scm.com/"&gt;Git&lt;/a&gt; installed and set up. The result of the following demonstration can be found in the &lt;a href="https://github.com/FlorianWilhelm/boston_housing"&gt;boston_housing repository&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;0. Use an isolated&amp;nbsp;environment&lt;/h3&gt;
&lt;p&gt;In the spirit of Phil Karlton who supposedly said &amp;#8220;There are only two hard things in Computer Science: cache invalidation and naming things.&amp;#8221;, we gonna select a specific task, namely an analysis based on the all familiar &lt;a href="https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html"&gt;Boston housing dataset&lt;/a&gt;, to help us finding crisp names. Based on our task we create an environment &lt;code&gt;boston_housing&lt;/code&gt; including Python and some common data science libraries&amp;nbsp;with:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;conda create -n boston_housing python=3.6 jupyterlab pandas scikit-learn seaborn
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;After less than a minute the environment is ready to be used and we can activate it with &lt;code&gt;conda activate boston_housing&lt;/code&gt;.&lt;/p&gt;
&lt;h2&gt;Efficient&amp;nbsp;Workflow&lt;/h2&gt;
&lt;p&gt;The code in notebooks tends to grow and grow to the point of being incomprehensible. To overcome this problem, the only way is to extract parts of it into Python modules once in a while. Since it only makes sense to extract functions and classes into Python modules, I often start cleaning up a messy notebook by thinking about the actual task a group of cells is accomplishing. This helps me to refactor those cells into a proper function which I can then migrate into a Python&amp;nbsp;module. &lt;/p&gt;
&lt;p&gt;At the point where you create custom modules, things get trickier. By default Python will only allow you to import modules that are installed in your environment or in your current working directory. Due to this behaviour many people start creating their custom modules in the directory holding their notebook. Since JupyterLab is nice enough to set the current working directory to the directory containing your notebook, everything is fine at the beginning. But as the number of notebooks that share common functionality imported from modules grows, the single directory containing notebooks and modules will get messier as you go. The obvious split of notebooks and modules into different folders or even organizing your notebooks into different folders will not work with this approach since then your imports will&amp;nbsp;fail. &lt;/p&gt;
&lt;p&gt;This observation brings us to one of the most important best practices: &lt;strong&gt;develop your code as a Python package&lt;/strong&gt;. A Python package will allow you to structure your code nicely over several modules and even subpackages, you can easily create unit tests and the best part of it is that distributing and sharing it with your colleagues comes for free. &lt;em&gt;But creating a Python package is so much overhead; surely it&amp;#8217;s not worth this small little analysis I will complete in half a day anyway and then forget about it&lt;/em&gt;, I hear you say. Well, how often is this actually true? Things always start out small but then get bigger and messier if you don&amp;#8217;t adhere to a certain structure right from the start. About half a year later then, your boss will ask you about that specific analysis you did back then and if you could repeat it with the new data and some additional KPIs. But more importantly coming back to the first part of your comment, if you know how, it&amp;#8217;s no overhead at&amp;nbsp;all!&lt;/p&gt;
&lt;h3&gt;1. Develop your code in a Python&amp;nbsp;Package&lt;/h3&gt;
&lt;p&gt;With the help of &lt;a href="https://pyscaffold.org/"&gt;PyScaffold&lt;/a&gt; it is possible to create a proper and standard-compliant Python package within a second. Just install it while having the conda environment activated&amp;nbsp;with:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;conda install -c conda-forge pyscaffold
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This package adds the &lt;code&gt;putup&lt;/code&gt; command into our environment which we use to create a Python package&amp;nbsp;with:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;putup boston_housing
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Now we can change into the new &lt;code&gt;boston_housing&lt;/code&gt; directory and install the package inside our environment in development&amp;nbsp;mode:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;python setup.py develop
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The development mode installs the package in the conda environment by linking to the source code which resides in &lt;code&gt;boston_housing/src/boston_housing&lt;/code&gt;. By doing so all your changes to the code will be directly available without any need to reinstall the package&amp;nbsp;again.&lt;/p&gt;
&lt;p&gt;Let&amp;#8217;s start JupyterLab with &lt;code&gt;jupyter lab&lt;/code&gt; from the root of your new project where &lt;code&gt;setup.py&lt;/code&gt; resides. To keep everything tight and clean, we start by creating a new folder &lt;code&gt;notebooks&lt;/code&gt; using the file browser in the left sidebar. Within this empty folder we create a new notebook using the launcher and rename it to &lt;code&gt;housing_model&lt;/code&gt;. Within the notebook we can now directly test our package by&amp;nbsp;typing:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;boston_housing.skeleton&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;fib&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The &lt;code&gt;skeleton&lt;/code&gt; module is just a test module that &lt;a href="https://pyscaffold.org/"&gt;PyScaffold&lt;/a&gt; provides (omit it with &lt;code&gt;putup --no-skeleton ...&lt;/code&gt;) and we import the Fibonacci function &lt;code&gt;fib&lt;/code&gt; from it. You can now just test this function by calling &lt;code&gt;fib(42)&lt;/code&gt; for&amp;nbsp;instance. &lt;/p&gt;
&lt;p&gt;At that point after having only adhered to a single good practice, we already benefit from many advantages. Since we have nicely separated our notebook from the actual implementation, we can package and distribute our code by just calling &lt;code&gt;python setup.py bdist_wheel&lt;/code&gt; and use &lt;a href="https://twine.readthedocs.io/"&gt;twine&lt;/a&gt; to upload it to some artefact store like &lt;a href="https://pypi.org/"&gt;PyPI&lt;/a&gt; or &lt;a href="https://devpi.net/"&gt;devpi&lt;/a&gt; for internal-only use. Another big plus is that having a package allows us to collaboratively work on the source code in your package using Git. On the other hand using Git with notebooks is a big pain since it its format is not really designed to be human-readable and thus merge conflicts are a horror. 
Still we haven&amp;#8217;t yet added any functionality, so let&amp;#8217;s see how we do about&amp;nbsp;that.&lt;/p&gt;
&lt;h3&gt;2. Extract functionality from the&amp;nbsp;notebook&lt;/h3&gt;
&lt;p&gt;We start with loading the &lt;a href="https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html"&gt;Boston housing dataset&lt;/a&gt; into a dataframe with columns of the lower-cased feature names and the target variable &lt;em&gt;price&lt;/em&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pandas&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pd&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.datasets&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;load_boston&lt;/span&gt;

&lt;span class="n"&gt;boston&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;load_boston&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataFrame&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;boston&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;columns&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;lower&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;boston&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;feature_names&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;price&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;boston&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;target&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Now imagine we would go on like this, do some preprocessing etc., and after a while we would have a pretty extensive notebook of statements and expressions without any structure leading to name collisions and confusion. Since notebooks allow the executing of cells in different order this can be extremely harmful. For these reasons, we create a function&amp;nbsp;instead:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;get_boston_df&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="n"&gt;boston&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;load_boston&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataFrame&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;boston&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;columns&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;lower&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;boston&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;feature_names&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;price&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;boston&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;target&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;We test it inside the notebook but then directly extract and move it into a module &lt;code&gt;model.py&lt;/code&gt; that we create within our package under &lt;code&gt;src/boston_boston&lt;/code&gt;. Now, inside our notebook, we can just import and use&amp;nbsp;it:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;boston_housing.model&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;get_boston_df&lt;/span&gt;

&lt;span class="n"&gt;df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;get_boston_df&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Now that looks much cleaner and allows also for other notebooks to just use this bit of functionality without using copy &lt;span class="amp"&gt;&amp;amp;&lt;/span&gt; paste! This leads us to another best practice: Use JupyterLab only for integrating code from your package and keep complex functionality inside the package. Thus, extract larger bits of code from a notebook and move it into a package or directly develop code in a proper &lt;span class="caps"&gt;IDE&lt;/span&gt;.&lt;/p&gt;
&lt;h3&gt;3. Use a proper &lt;span class="caps"&gt;IDE&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;At that point the natural question comes up how to edit the code within your package. Of course JupyterLab will do the job but let&amp;#8217;s face it, it just sucks compared to a real Integrated Development Environment (&lt;span class="caps"&gt;IDE&lt;/span&gt;) for such tasks. On the other hand our package structure is just perfect for a proper &lt;span class="caps"&gt;IDE&lt;/span&gt; like &lt;a href="https://www.jetbrains.com/pycharm/"&gt;PyCharm&lt;/a&gt;, &lt;a href="https://code.visualstudio.com/"&gt;Visual Studio Code&lt;/a&gt; or &lt;a href="https://atom.io/"&gt;Atom&lt;/a&gt; among others. PyCharm which is my favourite &lt;span class="caps"&gt;IDE&lt;/span&gt; has for instance many code inspection and refactoring features that support you in writing high-quality, clean code. Figure 1 illustrates the current state of our little&amp;nbsp;project.   &lt;/p&gt;
&lt;figure&gt;
&lt;p align="center"&gt;
&lt;img class="noZoom" src="/images/pycharm_boston_housing.png" alt="Boston-Housing project view in PyCharm"&gt;
&lt;figcaption&gt;&lt;strong&gt;Figure 1:&lt;/strong&gt; Project structure of the &lt;em&gt;boston-housing&lt;/em&gt; package as created with PyScaffold. The &lt;code&gt;notebooks&lt;/code&gt; folder holds the notebooks for JupyterLab while the &lt;code&gt;src/boston_housing&lt;/code&gt; folder contains the actual code (&lt;code&gt;model.py&lt;/code&gt;) and defines an actual Python package.&lt;/figcaption&gt;
&lt;/p&gt;
&lt;/figure&gt;

&lt;p&gt;If we use an &lt;span class="caps"&gt;IDE&lt;/span&gt; for development we will run into an obvious problem. How can we modify a function in our package and have these modifications reflected in our notebook without restarting the kernel every time? At this point I want to introduce you to your new best friend, the &lt;a href="https://ipython.readthedocs.io/en/stable/config/extensions/autoreload.html"&gt;autoreload extension&lt;/a&gt;. Just add in the first cell of your&amp;nbsp;notebook &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="n"&gt;load_ext&lt;/span&gt; &lt;span class="n"&gt;autoreload&lt;/span&gt;
&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="n"&gt;autoreload&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;and execute. This extension reloads modules before executing user code and thus allows you to use your &lt;span class="caps"&gt;IDE&lt;/span&gt; for development while executing it inside of&amp;nbsp;JupyterLab.&lt;/p&gt;
&lt;h3&gt;4. Know your&amp;nbsp;tool&lt;/h3&gt;
&lt;p&gt;JupyterLab is a powerful tool and knowing how to handle it brings you many advantages. Covering everything would exceed the scope of this blog post and thus I will mention here only practices that I apply&amp;nbsp;commonly.&lt;/p&gt;
&lt;h4&gt;Use Shortcuts to speed up your&amp;nbsp;work.&lt;/h4&gt;
&lt;p&gt;&lt;kbd&gt;Accel&lt;/kbd&gt; means &lt;kbd&gt;Cmd&lt;/kbd&gt; on Mac and &lt;kbd&gt;Ctrl&lt;/kbd&gt; on&amp;nbsp;Windows/Linux.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Command&lt;/th&gt;
&lt;th&gt;Shortcut&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Enter Command Mode&lt;/td&gt;
&lt;td&gt;&lt;kbd&gt;Esc&lt;/kbd&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Run Cell&lt;/td&gt;
&lt;td&gt;&lt;kbd&gt;Ctrl&lt;/kbd&gt; &lt;kbd&gt;Enter&lt;/kbd&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Run Cell &lt;span class="amp"&gt;&amp;amp;&lt;/span&gt; Select Next&lt;/td&gt;
&lt;td&gt;&lt;kbd&gt;Shift&lt;/kbd&gt; &lt;kbd&gt;Enter&lt;/kbd&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Add Cell Above/Below&lt;/td&gt;
&lt;td&gt;&lt;kbd&gt;A&lt;/kbd&gt; / &lt;kbd&gt;B&lt;/kbd&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Copy/Cut/Paste Cell&lt;/td&gt;
&lt;td&gt;&lt;kbd&gt;C&lt;/kbd&gt; / &lt;kbd&gt;X&lt;/kbd&gt; / &lt;kbd&gt;V&lt;/kbd&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Look Around Up/Down&lt;/td&gt;
&lt;td&gt;&lt;kbd&gt;Alt&lt;/kbd&gt; &lt;kbd&gt;â§&lt;/kbd&gt; / &lt;kbd&gt;â©&lt;/kbd&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Markdown Cell&lt;/td&gt;
&lt;td&gt;&lt;kbd&gt;M&lt;/kbd&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Code Cell&lt;/td&gt;
&lt;td&gt;&lt;kbd&gt;Y&lt;/kbd&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Delete Cell Output&lt;/td&gt;
&lt;td&gt;&lt;kbd&gt;M&lt;/kbd&gt;, &lt;kbd&gt;Y&lt;/kbd&gt; (workaround)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Delete Cell&lt;/td&gt;
&lt;td&gt;&lt;kbd&gt;D&lt;/kbd&gt; &lt;kbd&gt;D&lt;/kbd&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Toggle Line Numbers&lt;/td&gt;
&lt;td&gt;&lt;kbd&gt;Shift&lt;/kbd&gt; &lt;kbd&gt;L&lt;/kbd&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Comment Line&lt;/td&gt;
&lt;td&gt;&lt;kbd&gt;Ctrl&lt;/kbd&gt; &lt;kbd&gt;/&lt;/kbd&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Command Palette&lt;/td&gt;
&lt;td&gt;&lt;kbd&gt;Accel&lt;/kbd&gt; &lt;kbd&gt;Shift&lt;/kbd&gt; &lt;kbd&gt;C&lt;/kbd&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;File Explorer&lt;/td&gt;
&lt;td&gt;&lt;kbd&gt;Accel&lt;/kbd&gt; &lt;kbd&gt;Shift&lt;/kbd&gt; &lt;kbd&gt;F&lt;/kbd&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Toggle Bar&lt;/td&gt;
&lt;td&gt;&lt;kbd&gt;Accel&lt;/kbd&gt; &lt;kbd&gt;B&lt;/kbd&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Fullscreen Mode&lt;/td&gt;
&lt;td&gt;&lt;kbd&gt;Accel&lt;/kbd&gt; &lt;kbd&gt;Shift&lt;/kbd&gt; &lt;kbd&gt;D&lt;/kbd&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Close Tab&lt;/td&gt;
&lt;td&gt;&lt;kbd&gt;Ctrl&lt;/kbd&gt; &lt;kbd&gt;Q&lt;/kbd&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Launcher&lt;/td&gt;
&lt;td&gt;&lt;kbd&gt;Accel&lt;/kbd&gt; &lt;kbd&gt;Shift&lt;/kbd&gt; &lt;kbd&gt;L&lt;/kbd&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h4&gt;Quickly access&amp;nbsp;documentation&lt;/h4&gt;
&lt;p&gt;If you have ever used a notebook or IPython you surely know that executing a command prefixed with &lt;code&gt;?&lt;/code&gt; gets you the docstring (and with &lt;code&gt;??&lt;/code&gt; the source code). Even easier than that is actually moving the cursor over the command and pressing &lt;kbd&gt;Shift&lt;/kbd&gt; &lt;kbd&gt;Tab&lt;/kbd&gt;. This will open a small drop-down menu displaying the help that closes automatically after the next key&amp;nbsp;stroke.  &lt;/p&gt;
&lt;h4&gt;Avoid unintended&amp;nbsp;outputs&lt;/h4&gt;
&lt;p&gt;Using &lt;code&gt;;&lt;/code&gt; in Python is actually frowned upon but in Jupyterlab you can put it to good use. You surely have noticed outputs like &lt;code&gt;&amp;lt;matplotlib.axes._subplots.AxesSubplot at 0x7fce2e03a208&amp;gt;&lt;/code&gt; when you use a library like Matplotlib for plotting. This is due to the fact that Jupyter renders in the output cell the return value of the function as well as the graphical output. You can easily suppress and only show the plot by appending &lt;code&gt;;&lt;/code&gt; to a command like &lt;code&gt;plt.plot(...);&lt;/code&gt;.&lt;/p&gt;
&lt;h4&gt;Arrange cells and windows according to your&amp;nbsp;needs&lt;/h4&gt;
&lt;p&gt;You can easily arrange two notebooks side by side or in many other ways by clicking and holding on a notebook&amp;#8217;s tab then moving it around. The same applies to cells. Just click on the cell&amp;#8217;s number, hold and move it up or&amp;nbsp;down.&lt;/p&gt;
&lt;h4&gt;Access a cell&amp;#8217;s&amp;nbsp;result&lt;/h4&gt;
&lt;p&gt;Surely you have experienced this facepalm moment when your cell with &lt;code&gt;long_running_transformation(df)&lt;/code&gt; is finally finished but you forgot to store the result in another variable. Don&amp;#8217;t despair! You can just use &lt;code&gt;result = _NUMBER&lt;/code&gt;, e.g. &lt;code&gt;result = _42&lt;/code&gt;, where &lt;code&gt;NUMBER&lt;/code&gt; is the execution number of your cell, e.g. &lt;code&gt;In [42]&lt;/code&gt;, to access and save your result. An alternative to &lt;code&gt;_NUMBER&lt;/code&gt; is &lt;code&gt;Out[NUMBER]&lt;/code&gt;.&lt;/p&gt;
&lt;h4&gt;Use the multicursor&amp;nbsp;support&lt;/h4&gt;
&lt;p&gt;Why should you be satisfied with only one cursor if you can have multiple? Just press &lt;kbd&gt;Alt&lt;/kbd&gt; while holding down your left mouse button to select several rows. Then type as you would normally do to insert or&amp;nbsp;delete. &lt;/p&gt;
&lt;h4&gt;Activate line&amp;nbsp;numbers&lt;/h4&gt;
&lt;p&gt;Let&amp;#8217;s assume you have to debug a cell with lots of code, I know you wouldn&amp;#8217;t have cells with tons of code so let&amp;#8217;s say your colleague caused that mess. To find the line corresponding to the error output more easily, you can just hit &lt;kbd&gt;Shift&lt;/kbd&gt; &lt;kbd&gt;L&lt;/kbd&gt; to show the line numbers for a&amp;nbsp;moment.&lt;/p&gt;
&lt;h4&gt;Search all available&amp;nbsp;actions&lt;/h4&gt;
&lt;p&gt;The Command Palette is surely one of the most powerful features of JupyterLab. Just hit the shortcut &lt;kbd&gt;Accel&lt;/kbd&gt; &lt;kbd&gt;Shift&lt;/kbd&gt; &lt;kbd&gt;C&lt;/kbd&gt; and use the incremental search to find whatever action you are looking for. No more browsing menu drop downs for&amp;nbsp;minutes!&lt;/p&gt;
&lt;h3&gt;5. Create your personal notebook&amp;nbsp;template&lt;/h3&gt;
&lt;p&gt;After I have been using notebooks for a while I realized that in many cases the content of the first cell looks quite similar over many of the notebooks I created. Still, whenever I started something new I typed down the same imports and searched StackOverflow for some Pandas, Seaborn etc. settings. Consequently, a good advise is to have a &lt;code&gt;template.ipynb&lt;/code&gt; notebook somewhere that includes imports of popular packages and often used settings. Instead of creating a new notebook with JupyterLab you then just right-click the &lt;code&gt;template.ipynb&lt;/code&gt; notebook and click &lt;em&gt;Duplicate&lt;/em&gt;. &lt;/p&gt;
&lt;p&gt;The content of my &lt;code&gt;template.ipynb&lt;/code&gt; is&amp;nbsp;basically:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;sys&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;logging&lt;/span&gt;

&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;scipy&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;sp&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;sklearn&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;statsmodels.api&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;sm&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;statsmodels.formula.api&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;ols&lt;/span&gt;

&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="n"&gt;load_ext&lt;/span&gt; &lt;span class="n"&gt;autoreload&lt;/span&gt;
&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="n"&gt;autoreload&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;

&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;matplotlib&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;mpl&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;plt&lt;/span&gt;
&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="n"&gt;matplotlib&lt;/span&gt; &lt;span class="n"&gt;inline&lt;/span&gt;
&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="n"&gt;config&lt;/span&gt; &lt;span class="n"&gt;InlineBackend&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;figure_format&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;retina&amp;#39;&lt;/span&gt;

&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;seaborn&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;sns&lt;/span&gt;
&lt;span class="n"&gt;sns&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_context&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;poster&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;sns&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;rc&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;figure.figsize&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;9.&lt;/span&gt;&lt;span class="p"&gt;)})&lt;/span&gt;
&lt;span class="n"&gt;sns&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_style&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;whitegrid&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pandas&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pd&lt;/span&gt;
&lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_option&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;display.max_rows&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;120&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_option&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;display.max_columns&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;120&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;logging&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;basicConfig&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;level&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;logging&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;INFO&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;stream&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;sys&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;stdout&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h3&gt;6. Document your&amp;nbsp;analysis&lt;/h3&gt;
&lt;p&gt;A really old programmer&amp;#8217;s joke goes like &amp;#8220;When I wrote this code, only God and I understood what it did. Now&amp;#8230; only God knows.&amp;#8221; The same goes for an analysis or creating a predictive model. Therefore your future self will be very thankful for documentation of your code and even some general information about goals and context. Notebooks allow you to use &lt;a href="https://daringfireball.net/projects/markdown/syntax"&gt;Markdown syntax&lt;/a&gt; to annotate your analysis and you should make plenty use of it. Even mathematical expressions can be embedded using the &lt;code&gt;$...$&lt;/code&gt; notation. More general information about the whole project can be put into &lt;code&gt;README.rst&lt;/code&gt; which was also created by PyScaffold. This file will also be used as long description when the package is built and thus be displayed by an artefact store like &lt;a href="https://pypi.org/"&gt;PyPI&lt;/a&gt; or &lt;a href="https://devpi.net/"&gt;devpi&lt;/a&gt;. Also GitHub and GitLab will display &lt;code&gt;README.rst&lt;/code&gt; and thus provide a good entry point into your project. If you are more into the &lt;a href="https://daringfireball.net/projects/markdown/syntax"&gt;Markdown syntax&lt;/a&gt; and thus rather want a &lt;code&gt;README.md&lt;/code&gt;, you can install the &lt;a href="https://github.com/pyscaffold/pyscaffoldext-markdown"&gt;pyscaffoldext-markdown&lt;/a&gt; extension for PyScaffold which adds a &lt;code&gt;--markdown&lt;/code&gt; flag to PyScaffold&amp;#8217;s &lt;code&gt;putup&lt;/code&gt; command.&lt;/p&gt;
&lt;p&gt;The actual source code in your package should be documented using docstrings which brings us to a famous joke of Andrew Tanenbaum &amp;#8220;The nice thing about standards is that you have so many to choose from&amp;#8221;. The three most common docstring standards for Python are the default &lt;a href="https://www.sphinx-doc.org/en/master/usage/restructuredtext/domains.html#the-python-domain"&gt;Sphinx RestructuredText&lt;/a&gt;, &lt;a href="http://www.sphinx-doc.org/en/master/usage/extensions/napoleon.html"&gt;Numpy and Google style&lt;/a&gt; which are all supported by PyCharm. Personally I like the Google style the most but tastes are different and more important is to be consistent after you have picked one. In case you have lots of documentation which would blow the scope of a single readme file, maybe you came up with a new &lt;span class="caps"&gt;ML&lt;/span&gt; algorithms and want to document the concept behind it, you should take a look at &lt;a href="https://www.sphinx-doc.org/"&gt;Sphinx&lt;/a&gt;. Our project setup already includes a &lt;code&gt;docs&lt;/code&gt; folder with an &lt;code&gt;index.rst&lt;/code&gt; as a starting point and new pages can be easily added. After you have installed Sphinx you can build your documentation as &lt;span class="caps"&gt;HTML&lt;/span&gt;&amp;nbsp;pages:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;conda install spinx
python setup.py docs
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;It&amp;#8217;s also possible to create a nice &lt;span class="caps"&gt;PDF&lt;/span&gt; and even serve your documentation as a web page using &lt;a href="https://readthedocs.org/"&gt;ReadTheDocs&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;7. State your dependencies for&amp;nbsp;reproducibility&lt;/h3&gt;
&lt;p&gt;Python and its ecosystem evolve steady and quick, thus things that worked today might break tomorrow after a version of one of your dependencies changed. If you consider yourself a data &lt;em&gt;scientist&lt;/em&gt;, you should always guarantee &lt;strong&gt;reproducibility&lt;/strong&gt; of whatever you do since it&amp;#8217;s the most fundamental pillar of any real science. Reproducibility means that given the same data and code your future you and of course others should be able to run your analysis or model receiving the same results. To achieve this technically we need to record all dependencies and their versions. Using &lt;code&gt;conda&lt;/code&gt; we can do this with our &lt;code&gt;boston_housing&lt;/code&gt; project&amp;nbsp;as:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;conda env export -n boston_housing -f environment.lock.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This creates a file &lt;code&gt;environment.lock.yaml&lt;/code&gt; that recursively states all dependencies and their version as well as the Python version that was used to allow anyone to deterministically reproduce this environment in the future. This is as easy&amp;nbsp;as &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;conda env create -f environment.lock.yaml --force
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Besides a &lt;em&gt;concrete&lt;/em&gt; environment file that exhaustively lists all dependencies, it&amp;#8217;s also common practice to define an &lt;code&gt;environment.yaml&lt;/code&gt; where you state your &lt;em&gt;abstract&lt;/em&gt; dependencies. These abstract dependencies comprise only libraries which are directly imported with no specific version. In our case this file looks&amp;nbsp;like:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nt"&gt;name&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;boston_housing&lt;/span&gt;
&lt;span class="nt"&gt;channels&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
  &lt;span class="p p-Indicator"&gt;-&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;defaults&lt;/span&gt;
&lt;span class="nt"&gt;dependencies&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
  &lt;span class="p p-Indicator"&gt;-&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;jupyterlab&lt;/span&gt;
  &lt;span class="p p-Indicator"&gt;-&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;pandas&lt;/span&gt;
  &lt;span class="p p-Indicator"&gt;-&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;scikit-learn&lt;/span&gt;
  &lt;span class="p p-Indicator"&gt;-&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;seaborn&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This file keeps track of all libraries you are directly using. If you added a new library you can use this file to update your current environment&amp;nbsp;with:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;conda env update --file environment.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Remember to regularly update and commit changes to these files in Git. Whenever you are satisfied with an iteration of your work also make use of Git tags in order to have reference points for later. These tags will also be used automatically as version numbers for your Python package which is another benefit of having used PyScaffold for your project&amp;nbsp;setup.&lt;/p&gt;
&lt;p&gt;Reproducible environments are only one aspect of reproducibility. Since many machine learning algorithms (most prominently Deep Learning) use random numbers it&amp;#8217;s important to keep them deterministic by fixing the random seed. This sounds easier at it is since depending on the used framework, there are different ways to accomplish this. A good overview for many common frameworks is provided in the talk &lt;a href="https://www.youtube.com/watch?v=MOBs6MNepDk&amp;amp;feature=youtu.be"&gt;Reproducibility, and Selection Bias in Machine Learning&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;8. Develop locally, execute&amp;nbsp;remotely&lt;/h3&gt;
&lt;p&gt;Quite often when you want to do some heavy lifting, your laptop won&amp;#8217;t be enough and thus you might use some powerful workstation by remote access. Running JupyterLab on the workstation and accessing it, maybe through some &lt;a href="https://www.ssh.com/ssh/tunneling/example"&gt;&lt;span class="caps"&gt;SSH&lt;/span&gt; tunnel&lt;/a&gt;, is no problem at all but how can we now work on the modules in our package? One way would be to run your &lt;span class="caps"&gt;IDE&lt;/span&gt; on the workstation but this comes potentially with many downsides depending on your connection. A flaky connection might lead to increased latencies when typing or reduced resolution. For this reason it&amp;#8217;s best to do the actual coding locally in your &lt;span class="caps"&gt;IDE&lt;/span&gt; and sync every change automatically to the workstation where JupyterLab runs. The general setup for the workstation is analogue to the local setup. We &lt;code&gt;git clone&lt;/code&gt; our repository and use the &lt;code&gt;environment.lock.yaml&lt;/code&gt; to create the exact same environment which we run locally, followed by a &lt;code&gt;python setup.py develop&lt;/code&gt;. If we now start JupyterLab within this environment we will be able to import our&amp;nbsp;package. &lt;/p&gt;
&lt;p&gt;Now comes the interesting part: every change in one of our local modules needs to be reflected also on the remote workstation. You can use a classical command line tool like &lt;a href="https://rsync.samba.org/"&gt;rsync&lt;/a&gt; for that or just rely on the features of your &lt;span class="caps"&gt;IDE&lt;/span&gt;. Over the last years I have grown quite fond of PyCharm&amp;#8217;s Deployment feature as illustrated in Figure 2, which is unfortunately only available in the Professional version. It allows you to configure remote servers and if &lt;em&gt;Automatic Upload&lt;/em&gt; is checked it syncs each file when saving. This convenient feature allows for blazing fast iterations. You make some changes to your model, maybe implement a new transformation function, hit &lt;kbd&gt;Accel&lt;/kbd&gt; &lt;kbd&gt;S&lt;/kbd&gt; to save, hit &lt;kbd&gt;Accel&lt;/kbd&gt; &lt;kbd&gt;Tab&lt;/kbd&gt; to switch to your browser with the JupyterLab tab and then rerun the modified model on the&amp;nbsp;workstation.  &lt;/p&gt;
&lt;p&gt;From time to time, we also need to commit our changes using Git. Since we developed mostly on our local machine we only need to download the content of the &lt;code&gt;notebooks&lt;/code&gt; folder from the remote workstation. For this we can again use rsync or the &lt;em&gt;Download from &amp;#8230;&lt;/em&gt; deployment feature of PyCharm Professional. Thus also all our git operations are executed locally avoiding merge conflicts between the local and remote repository. Git should not be used for syncing tasks&amp;nbsp;anyway.&lt;/p&gt;
&lt;figure&gt;
&lt;p align="center"&gt;
&lt;img class="noZoom" src="/images/pycharm_deployment.png" alt="Deployment tool of PyCharm"&gt;
&lt;figcaption&gt;&lt;strong&gt;Figure 2:&lt;/strong&gt; PyCharm Professional allows you to easily develop locally your Python modules and run them remotely in JupyterLab. It will keep track of local changes and upload them automatically what triggers JupterLab&amp;#8217;s autoreload extension.&lt;/figcaption&gt;
&lt;/p&gt;
&lt;/figure&gt;

&lt;p&gt;Another reason for running JupyterLab on a remote machine might be due to some firewall restrictions. Quite often in order to access sensitive data sources or a &lt;a href="https://spark.apache.org/"&gt;Spark&lt;/a&gt; cluster, you need to run JupyterLab on a gateway server. To invoke JupyterLab with Spark capabilities there are two ways. An ad hoc method is to just state on the command line that JupyterLab should use pyspark as kernel. For instance starting JupyterLab with Python 3.6 (needs to be consistent with your Spark distribution), 20 executors each having 5 cores might look like&amp;nbsp;this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;PYSPARK_PYTHON=python3.6 PYSPARK_DRIVER_PYTHON=&amp;quot;jupyter&amp;quot; PYSPARK_DRIVER_PYTHON_OPTS=&amp;quot;notebook --no-browser --port=8899&amp;quot; /usr/bin/pyspark2 --master yarn --deploy-mode client --num-executors 20  --executor-memory 10g --executor-cores 5 --conf spark.dynamicAllocation.enabled=false
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;In order to be able to create notebooks with a specific PySpark kernel directly from JupyterLab, just create a file &lt;code&gt;~/.local/share/jupyter/kernels/pyspark/kernel.json&lt;/code&gt; holding:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="p"&gt;{&lt;/span&gt;
 &lt;span class="nt"&gt;&amp;quot;display_name&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;PySpark&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
 &lt;span class="nt"&gt;&amp;quot;language&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;python&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
 &lt;span class="nt"&gt;&amp;quot;argv&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;
  &lt;span class="s2"&gt;&amp;quot;/usr/local/anaconda-py3/bin/python&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="s2"&gt;&amp;quot;-m&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="s2"&gt;&amp;quot;ipykernel&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="s2"&gt;&amp;quot;-f&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="s2"&gt;&amp;quot;{connection_file}&amp;quot;&lt;/span&gt;
 &lt;span class="p"&gt;],&lt;/span&gt;
 &lt;span class="nt"&gt;&amp;quot;env&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;quot;HADOOP_CONF_DIR&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;/etc/hadoop/conf&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;quot;HADOOP_USER_NAME&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;username&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;quot;HADOOP_CONF_LIB_NATIVE_DIR&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;/var/lib/cloudera/parcels/CDH/lib/hadoop/lib/native&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;quot;YARN_CONF_DIR&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;/etc/hadoop/conf&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;quot;SPARK_YARN_QUEUE&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;dev&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;quot;PYTHONPATH&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;/usr/local/anaconda-py3/bin/python:/usr/local/anaconda-py3/lib/python3.6/site-packages:/var/lib/cloudera/parcels/SPARK2/lib/spark2/python:/var/lib/cloudera/parcels/SPARK2/lib/spark2/python/lib/py4j-0.10.4-src.zip&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;quot;SPARK_HOME&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;/var/lib/cloudera/parcels/SPARK2/lib/spark2/&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;quot;PYTHONSTARTUP&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;/var/lib/cloudera/parcels/SPARK2/lib/spark2/python/pyspark/shell.py&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;quot;PYSPARK_SUBMIT_ARGS&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;--queue dev --conf spark.dynamicAllocation.enabled=false --conf spark.scheduler.minRegisteredResourcesRatio=1 --conf spark.sql.autoBroadcastJoinThreshold=-1 --master yarn --num-executors 5 --driver-memory 2g --executor-memory 20g --executor-cores 3 pyspark-shell&amp;quot;&lt;/span&gt;
 &lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;We have seen that using an own Python package in conjunction with JupyterLab gives us means to program much cleaner and the ability to use a proper &lt;span class="caps"&gt;IDE&lt;/span&gt;. JupyterLab is a mighty and flexible tool and thus all the more it&amp;#8217;s important to adhere to some best practices and processes to guarantee quality in your software and analysis. The &lt;a href="https://github.com/FlorianWilhelm/boston_housing"&gt;boston_housing repository&lt;/a&gt; demonstrates a simple analysis of the Boston Housing Dataset in accordance with the outlined points&amp;nbsp;above.&lt;/p&gt;
&lt;p&gt;JupyterLab also offers many powerful &lt;a href="https://jupyterlab.readthedocs.io/en/stable/user/extensions.html"&gt;extensions&lt;/a&gt;, e.g. &lt;a href="https://github.com/jupyterlab/jupyterlab-git"&gt;jupyterlab-git&lt;/a&gt;, &lt;a href="https://github.com/jupyterlab/jupyterlab-toc"&gt;jupyterlab-toc&lt;/a&gt;, etc., for improved productivity that are worth checking out. If you have any additions or neat tricks for JupyterLab that were not covered, please let me know by using the comments below. Since general concepts are transferable but the specific workflow may be different, also read the &lt;a href="https://cprohm.de/article/notebooks-and-modules.html"&gt;blog post of Christopher Prohm&lt;/a&gt; about the same topic but using partly a different&amp;nbsp;tooling.&lt;/p&gt;</content><category term="post"></category><category term="python"></category><category term="jupyter"></category></entry><entry><title>Multiplicative LSTM for sequence-basedÂ Recommenders</title><link href="https://florianwilhelm.info/2018/08/multiplicative_LSTM_for_sequence_based_recos/" rel="alternate"></link><published>2018-08-05T16:00:00+02:00</published><updated>2018-08-05T16:00:00+02:00</updated><author><name>Florian Wilhelm</name></author><id>tag:florianwilhelm.info,2018-08-05:/2018/08/multiplicative_LSTM_for_sequence_based_recos/</id><summary type="html">&lt;p&gt;Recommender Systems support the decision making processes of customers with personalized suggestions. They are widely used and influence the daily life of almost everyone in different domains like e-commerce, social media, or entertainment. Quite often the dimension of time plays a dominant role in the generation of a relevant&amp;nbsp;recommendation.&lt;/p&gt;</summary><content type="html">&lt;h2&gt;Motivation&lt;/h2&gt;
&lt;p&gt;Recommender Systems support the decision making processes of customers with personalized suggestions. 
They are widely used and influence the daily life of almost everyone in different domains like e-commerce, 
social media, or entertainment. Quite often the dimension of time plays a dominant role in the generation
of a relevant recommendation. Which user interaction occurred just before the point of time where we want to 
provide a recommendation?
How many interactions ago did the user interact with an item like this one?
Traditional user-item recommenders often neglect the dimension of time completely. 
This means that many traditional recommenders find for each user a latent representation based on the user&amp;#8217;s
historical item interactions without any notion of recency and sequence of interactions. To also incorporate 
this kind of contextual information about interactions, sequence-based recommenders were developed. 
With the advent of deep learning quite a few of them are nowadays based on &lt;a href="https://en.wikipedia.org/wiki/Recurrent_neural_network"&gt;Recurrent Neural Networks&lt;/a&gt;&amp;nbsp;(RNNs).&lt;/p&gt;
&lt;p&gt;Whenever I want to dig deeper into a topic like sequence-based recommenders I follow a few simple steps:
First of all, to learn something I directly need to apply it otherwise learning things doesn&amp;#8217;t work for me. In order to apply something I need a challenge and a small goal that keeps me motivated on the journey. Following the &lt;a href="https://en.wikipedia.org/wiki/SMART_criteria"&gt;&lt;span class="caps"&gt;SMART&lt;/span&gt; citeria&lt;/a&gt; a goal needs to be measurable and thus a typical outcome for me is a blog post like the one you are just reading. Another good thing about a blog post is the fact that no one wants to publish something completely crappy, so there is an intrinsic quality assurance attached to the whole process. This blog post is actually the outcome of several things I wanted to familiarize myself more and try&amp;nbsp;out:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="https://pytorch.org/"&gt;PyTorch&lt;/a&gt;, since this framework is used in a large fraction of recent publications about deep&amp;nbsp;learning,&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/maciejkula/spotlight"&gt;Spotlight&lt;/a&gt;, since this library gives you a sophisticated structure to play around with new ideas for recommender systems and already has a lot of functionality&amp;nbsp;implemented,&lt;/li&gt;
&lt;li&gt;applying a paper about &lt;a href="https://arxiv.org/abs/1609.07959"&gt;Multiplicative &lt;span class="caps"&gt;LSTM&lt;/span&gt; for sequence modelling&lt;/a&gt; to recommender systems and see how that performs compared to traditional&amp;nbsp;LSTMs.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Since Spotlight is based on PyTorch and multiplicative LSTMs (mLSTMs) are not yet implemented in PyTorch the task of evaluating mLSTMs vs. LSTMs inherently addresses all those points outlined above. The goal is set, so let&amp;#8217;s get&amp;nbsp;going!&lt;/p&gt;
&lt;h2&gt;Theory&lt;/h2&gt;
&lt;p&gt;Long short-term memory architectures (LSTMs) are maybe the most common incarnations of RNNs since they don&amp;#8217;t adhere 
to the &lt;a href="https://en.wikipedia.org/wiki/Vanishing_gradient_problem"&gt;vanishing gradient problem&lt;/a&gt; and thus are able to capture long-term relationships in a sequence. You can find a great
explanation of LSTMs in Colah&amp;#8217;s post &lt;a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/"&gt;Understanding &lt;span class="caps"&gt;LSTM&lt;/span&gt; Networks&lt;/a&gt; and more general about the power of RNNs in the 
article &lt;a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/"&gt;The Unreasonable Effectiveness of Recurrent Neural Networks&lt;/a&gt;. 
More recently, also Gated Recurrent Units (GRUs) which have a simplified structure compared to LSTMs are also used 
in sequential prediction tasks with occasionally superior results. &lt;a href="https://github.com/maciejkula/spotlight"&gt;Spotlight&lt;/a&gt; provides a sequential recommender based on LSTMs and 
the quite renowned &lt;a href="https://github.com/hidasib/GRU4Rec"&gt;GRU4Rec&lt;/a&gt; model uses GRUs but in general it&amp;#8217;s not possible to state that one always outperforms the&amp;nbsp;other.&lt;/p&gt;
&lt;p&gt;So given these ingredients, how do we now construct a sequential recommender? Let&amp;#8217;s assume on every timestep 
&lt;span class="math"&gt;\(t\in\{1,\ldots,T\}\)&lt;/span&gt; a user has interacted with an item &lt;span class="math"&gt;\(i_t\)&lt;/span&gt;. The basic idea is now to feed these interactions into
 an &lt;span class="caps"&gt;LSTM&lt;/span&gt; up to the time &lt;span class="math"&gt;\(t\)&lt;/span&gt; in order to get a representation of the user&amp;#8217;s preferences &lt;span class="math"&gt;\(h_t\)&lt;/span&gt; and use that to state
 if the user might like or dislike the next item &lt;span class="math"&gt;\(i_{t+1}\)&lt;/span&gt;. Just like in a non-sequential recommender we also do a
 &lt;a href="https://en.wikipedia.org/wiki/One-hot"&gt;one-hot encoding&lt;/a&gt; of the items followed by an embedding into a dense vector representation &lt;span class="math"&gt;\(e_{i_t}\)&lt;/span&gt;
 which is then feed into the &lt;span class="caps"&gt;LSTM&lt;/span&gt;. We can then just use the output &lt;span class="math"&gt;\(h_t\)&lt;/span&gt; of the &lt;span class="caps"&gt;LSTM&lt;/span&gt; and calculate the inner product (&lt;span class="math"&gt;\(\bigotimes\)&lt;/span&gt;) 
 with the embedding &lt;span class="math"&gt;\(e_{i_{t+1}}\)&lt;/span&gt; plus an item bias for varying item popularity to retrieve an output &lt;span class="math"&gt;\(p_{t+1}\)&lt;/span&gt;. 
 This output along with others is then used to calculate the actual loss depending on our sample strategy and loss function. 
 We train our model by sampling positive interactions and corresponding negative interactions. In an &lt;em&gt;explicit feedback&lt;/em&gt; context 
 a positive and negative interaction might be a positive and negative rating of a user for an item, respectively. In an &lt;em&gt;implicit feedback&lt;/em&gt; context, all item interactions of a user are considered positive whereas negative interactions arise from items the
 user did not interact with.
 During the training we adapt the weights of our model so that for a given user the scalar output of a positive interaction
 is greater than the output of a negative interaction. This can be seen as an approximation to a &lt;a href="https://en.wikipedia.org/wiki/Softmax_function"&gt;softmax&lt;/a&gt; in very high-dimensional output&amp;nbsp;space.&lt;/p&gt;
&lt;p&gt;Figure 1 illustrates our sequential recommender model and this is what&amp;#8217;s actually happening inside Spotlight&amp;#8217;s 
 sequential recommender with an &lt;span class="caps"&gt;LSTM&lt;/span&gt; representation. If you raise your eyebrow due to the usage of an inner product
 then be aware that &lt;a href="https://en.wikipedia.org/wiki/Low-rank_approximation"&gt;low-rank approximations&lt;/a&gt; have been and still are one of the most successful building blocks
 of recommender systems. An alternative would be to replace the inner product with a deep feed forward network but
 to quite some extent, this would also just learn to perform an approximation of an inner product. A recent paper
 &lt;a href="https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/46488.pdf"&gt;Latent Cross: Making Use of Context in Recurrent Recommender Systems&lt;/a&gt; by Google also emphasizes the power of learning
 low-rank relations with the help of inner&amp;nbsp;products.&lt;/p&gt;
&lt;figure&gt;
&lt;p align="center"&gt;
&lt;img class="noZoom" src="/images/mLSTM.png" alt="mLSTM"&gt;
&lt;figcaption&gt;&lt;strong&gt;Figure 1:&lt;/strong&gt; At timestep \(t\) the item \(i_t\) is embedded and fed into an &lt;span class="caps"&gt;LSTM&lt;/span&gt; together with
 cell state \(C_{t-1}\) and \(h_{t-1}\) of the last timestep which yields a new presentation \(h_t\). The inner product of 
 \(h_t\) with the embedding of the potential next item \(e_{i_{t+1}}\) yields a scalar value corresponding to how likely the user
 would interact with \(i_{t+1}\).&lt;/figcaption&gt;
&lt;/p&gt;
&lt;/figure&gt;

&lt;p&gt;What we want to do is basically replacing the &lt;span class="caps"&gt;LSTM&lt;/span&gt; part of Spotlight&amp;#8217;s sequential recommender with an mLSTM. 
But before we do that the obvious question is why? Let&amp;#8217;s recap the formulae of a typical &lt;a href="http://pytorch.org/docs/0.3.1/nn.html?highlight=lstm#torch.nn.LSTM"&gt;&lt;span class="caps"&gt;LSTM&lt;/span&gt; implementation&lt;/a&gt; 
like the one in&amp;nbsp;PyTorch:&lt;/p&gt;
&lt;div class="math"&gt;\begin{split}\begin{array}{ll}
i_t = \mathrm{sigmoid}(W_{ii} x_t + b_{ii} + W_{hi} h_{t-1} + b_{hi}) \\
f_t = \mathrm{sigmoid}(W_{if} x_t + b_{if} + W_{hf} h_{t-1} + b_{hf}) \\
g_t = \tanh(W_{ig} x_t + b_{ig} + W_{hc} h_{t-1} + b_{hg}) \\
o_t = \mathrm{sigmoid}(W_{io} x_t + b_{io} + W_{ho} h_{t-1} + b_{ho}) \\
c_t = f_t * c_{t-1} + i_t * g_t \\
h_t = o_t * \tanh(c_t)
\end{array}\end{split}&lt;/div&gt;
&lt;p&gt;
&lt;br&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class="math"&gt;\(i_t\)&lt;/span&gt; denotes the input gate, &lt;span class="math"&gt;\(f_t\)&lt;/span&gt; the forget gate and &lt;span class="math"&gt;\(o_t\)&lt;/span&gt; the output gate at timestep &lt;span class="math"&gt;\(t\)&lt;/span&gt;. If we look at
those lines again we can see a lot of terms in the form of &lt;span class="math"&gt;\(W_{**} x_t + W_{**} h_{t-1}\)&lt;/span&gt; neglecting the biases &lt;span class="math"&gt;\(b_*\)&lt;/span&gt; for a
moment. Thus a lot of an &lt;span class="caps"&gt;LSTM&lt;/span&gt;&amp;#8217;s inner workings depend on the addition of the transformed input with the transformed hidden
state. So what happens if a trained &lt;span class="caps"&gt;LSTM&lt;/span&gt; with thus fixed &lt;span class="math"&gt;\(W_{**}\)&lt;/span&gt; encounters some unexpected, completely surprising input
&lt;span class="math"&gt;\(x_t\)&lt;/span&gt;? This might disturb the cell state &lt;span class="math"&gt;\(c_t\)&lt;/span&gt; leading to pertubated future &lt;span class="math"&gt;\(h_t\)&lt;/span&gt; and it might take a long time for the
&lt;span class="caps"&gt;LSTM&lt;/span&gt; to recover from that singular surprising input. The authors of the paper &lt;a href="https://arxiv.org/abs/1609.07959"&gt;Multiplicative &lt;span class="caps"&gt;LSTM&lt;/span&gt; for sequence modelling&lt;/a&gt; 
now argue that &amp;#8220;&lt;span class="caps"&gt;RNN&lt;/span&gt; architectures with hidden-to-hidden transition functions that are input-dependent are better suited to recover 
from surprising inputs&amp;#8221;. By allowing the hidden state to react flexibly on the new input by changing its magnitude it might be
able to recover from mistakes faster. The quite vague formulation of &lt;em&gt;input-dependent transition functions&lt;/em&gt; is then 
actually achieved in a quite simple way. In an mLSTM the hidden state &lt;span class="math"&gt;\(h_{t-1}\)&lt;/span&gt; is transformed in a multiplicative way
using the input &lt;span class="math"&gt;\(x_t\)&lt;/span&gt; into an intermediate state &lt;span class="math"&gt;\(m_t\)&lt;/span&gt; before it is used in a plain &lt;span class="caps"&gt;LSTM&lt;/span&gt; as before. Eventually, there
is only a single equation to be prepended to the equations of an &lt;span class="caps"&gt;LSTM&lt;/span&gt;:&lt;/p&gt;
&lt;div class="math"&gt;\begin{split}\begin{array}{ll}
m_t = (W_{im} x_t + b_{im}) \odot{} ( W_{hm} h_{t-1} + b_{hm}) \\
i_t = \mathrm{sigmoid}(W_{ii} x_t + b_{ii} + W_{mi} m_t + b_{mi}) \\
f_t = \mathrm{sigmoid}(W_{if} x_t + b_{if} + W_{mf} m_t + b_{mf}) \\
g_t = \tanh(W_{ig} x_t + b_{ig} + W_{mc} m_t + b_{mg}) \\
o_t = \mathrm{sigmoid}(W_{io} x_t + b_{io} + W_{mo} m_t + b_{mo}) \\
c_t = f_t * c_{t-1} + i_t * g_t \\
h_t = o_t * \tanh(c_t)
\end{array}\end{split}&lt;/div&gt;
&lt;p&gt;
&lt;br&gt;&lt;/p&gt;
&lt;p&gt;The element-wise multiplication (&lt;span class="math"&gt;\(\odot\)&lt;/span&gt;) allows &lt;span class="math"&gt;\(m_t\)&lt;/span&gt; to flexibly change it&amp;#8217;s value with respect to &lt;span class="math"&gt;\(h_{t-1}\)&lt;/span&gt; and &lt;span class="math"&gt;\(x_t\)&lt;/span&gt;.
On a more theoretical note, if you picture the hidden states of an &lt;span class="caps"&gt;LSTM&lt;/span&gt; as a tree depending on the inputs at each timestep
then the number of all possible states at timestep &lt;span class="math"&gt;\(t\)&lt;/span&gt; will be much larger for an mLSTM compared to an &lt;span class="caps"&gt;LSTM&lt;/span&gt;. Therefore, 
the tree of an mLSTM will be much wider and consequently more flexible to represent different probability distributions
according to the paper. The paper focuses only on &lt;span class="caps"&gt;NLP&lt;/span&gt; tasks but since surprising inputs are also a concern in sequential recommender systems,
the self-evident idea is to evaluate if mLSTMs also excel in recommender&amp;nbsp;tasks. &lt;/p&gt;
&lt;h2&gt;Implementation&lt;/h2&gt;
&lt;p&gt;Everyone seems to love &lt;a href="https://pytorch.org/"&gt;PyTorch&lt;/a&gt; for it&amp;#8217;s beautiful &lt;span class="caps"&gt;API&lt;/span&gt; and I totally agree. For me its beauty lies in its simplicity. 
Every elementary building block of a neural network like a linear transformation is called a &lt;em&gt;Module&lt;/em&gt; in PyTorch. A
Module is just a class that inherits from &lt;code&gt;Module&lt;/code&gt; and implements a &lt;code&gt;forward&lt;/code&gt; method that does the transformation
with the help of tensor operations. A more complex neural network is again just a &lt;code&gt;Module&lt;/code&gt; and uses the 
&lt;a href="https://en.wikipedia.org/wiki/Composition_over_inheritance"&gt;composition principle&lt;/a&gt; to compose its functionality from simpler modules. Therefore, in my humble opinion, PyTorch
found a much nicer concept of combining low-level tensor operations with the high level composition of layers compared
to core &lt;a href="https://www.tensorflow.org/"&gt;TensorFlow&lt;/a&gt; and &lt;a href="https://keras.io/"&gt;Keras&lt;/a&gt; where you are either stuck on the level of tensor operations or the composition of&amp;nbsp;layers. &lt;/p&gt;
&lt;p&gt;For our task, we gonna need an &lt;code&gt;mLSTM&lt;/code&gt; module and luckily PyTorch provides &lt;code&gt;RNNBase&lt;/code&gt;, a base class for custom RNNs.
So all we have to do is to write a module that inherits from &lt;code&gt;RNNBase&lt;/code&gt;, defines additional parameters and implements
the mLSTM equations inside of &lt;code&gt;forward&lt;/code&gt;: &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;math&lt;/span&gt;

&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;torch&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;torch.nn&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Parameter&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;torch.nn.modules.rnn&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;RNNBase&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;LSTMCell&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;torch.nn&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;functional&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;F&lt;/span&gt;


&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;mLSTM&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;RNNBase&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;input_size&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;hidden_size&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;bias&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="nb"&gt;super&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mLSTM&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
            &lt;span class="n"&gt;mode&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;LSTM&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;input_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;input_size&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;hidden_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;hidden_size&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                 &lt;span class="n"&gt;num_layers&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;bias&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;bias&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;batch_first&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                 &lt;span class="n"&gt;dropout&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;bidirectional&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="n"&gt;w_im&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Tensor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;hidden_size&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;input_size&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;w_hm&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Tensor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;hidden_size&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;hidden_size&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;b_im&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Tensor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;hidden_size&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;b_hm&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Tensor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;hidden_size&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;w_im&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Parameter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;w_im&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;b_im&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Parameter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;b_im&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;w_hm&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Parameter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;w_hm&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;b_hm&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Parameter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;b_hm&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;lstm_cell&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;LSTMCell&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;input_size&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;hidden_size&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;bias&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reset_parameters&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;reset_parameters&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;stdv&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;math&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sqrt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;hidden_size&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;weight&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;parameters&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
            &lt;span class="n"&gt;weight&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;uniform_&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;stdv&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;stdv&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;forward&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;input&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;hx&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;n_batch&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_seq&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_feat&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;input&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

        &lt;span class="n"&gt;hx&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cx&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;hx&lt;/span&gt;
        &lt;span class="n"&gt;steps&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;cx&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;unsqueeze&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;seq&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_seq&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="n"&gt;mx&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;F&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linear&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;input&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="n"&gt;seq&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;:],&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;w_im&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;b_im&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;F&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linear&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;hx&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;w_hm&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;b_hm&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;hx&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mx&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cx&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;hx&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cx&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;lstm_cell&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;input&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="n"&gt;seq&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;:],&lt;/span&gt; &lt;span class="n"&gt;hx&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;steps&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cx&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;unsqueeze&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cat&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;steps&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dim&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The code is pretty much self-explanatory. We inherit from &lt;code&gt;RNNBase&lt;/code&gt; and initialize the additional parameters we need for the calculation of &lt;span class="math"&gt;\(m_t\)&lt;/span&gt;
in &lt;code&gt;__init__&lt;/code&gt;. In &lt;code&gt;forward&lt;/code&gt; we use those parameters to calculate &lt;span class="math"&gt;\(m_t = (W_{im} x_t + b_{im}) \odot{} ( W_{hm} h_{t-1} + b_{hm})\)&lt;/span&gt; with the help of &lt;code&gt;F.linear&lt;/code&gt; and pass it to an ordinary &lt;code&gt;LSTMCell&lt;/code&gt;. We collect the results for each timestep
in our sequence in &lt;code&gt;steps&lt;/code&gt; and return it as concatenated&amp;nbsp;tensor. &lt;/p&gt;
&lt;p&gt;The &lt;a href="https://github.com/maciejkula/spotlight"&gt;Spotlight&lt;/a&gt; library, in the spirit of PyTorch, also follows a modular concept of components that can be easily plugged together and replaced.
It has only five&amp;nbsp;components:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;embedding layers&lt;/strong&gt; which map item ids to dense&amp;nbsp;vectors,&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;user/item representations&lt;/strong&gt; which take embedding layers to calculate latent representations and the score for a 
    user/item&amp;nbsp;pair, &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;interactions&lt;/strong&gt; which give easy access to the user/item interactions and their explicit/implicit&amp;nbsp;feedback,&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;losses&lt;/strong&gt; which define the objective for the recommendation&amp;nbsp;task,&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;models&lt;/strong&gt; which take user/item representations, the user/item interactions and a given loss to train the&amp;nbsp;network.  &lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Due to this modular layout, we only need to write a new user/item representation module called &lt;code&gt;mLSTMNet&lt;/code&gt;. Since this
is straight-forward I leave it to you to take a look at the source code in my &lt;a href="https://github.com/FlorianWilhelm/mlstm4reco"&gt;mlstm4reco&lt;/a&gt; repository.
At this point I should mentioned that the whole layout of the repository was strongly inspired by Maciej Kula&amp;#8217;s 
&lt;a href="https://arxiv.org/abs/1711.08379"&gt;Mixture-of-tastes Models for Representing Users with Diverse Interests&lt;/a&gt; paper and the accompanying &lt;a href="https://github.com/maciejkula/mixture"&gt;source code&lt;/a&gt;.
My implementation also follows his advise of using an automatic hyperparameter optimisation for my own model and the
baseline model for comparison. This avoids quite a common bias in research when people put more effort in hand-tuning
their own model compared to the baseline to later show a better improvement in order to get the paper accepted.
Using a tool like &lt;a href="http://hyperopt.github.io/hyperopt/"&gt;HyperOpt&lt;/a&gt; for hyperparameter optimisation is quite easy and mitigates this bias to some extent at&amp;nbsp;least.&lt;/p&gt;
&lt;h2&gt;Evaluation&lt;/h2&gt;
&lt;p&gt;To compare Spotlight&amp;#8217;s &lt;a href="https://maciejkula.github.io/spotlight/sequence/implicit.html#module-spotlight.sequence.implicit"&gt;ImplicitSequenceModel&lt;/a&gt; with an &lt;span class="caps"&gt;LSTM&lt;/span&gt; to an mLSTM user representation, the
&lt;a href="https://github.com/FlorianWilhelm/mlstm4reco"&gt;mlstm4reco&lt;/a&gt; repository provides a &lt;code&gt;run.py&lt;/code&gt; script in the &lt;code&gt;experiments&lt;/code&gt; folder which takes several
command line options. Some might argue that this is a bit of over-engineering for a one time evaluation. 
But for me it&amp;#8217;s just one aspect of proper and reproducible research since it avoids errors and you can also easily
log which parameters were used to generate the results. I also used &lt;a href="https://pyscaffold.org/"&gt;PyScaffold&lt;/a&gt; to set up proper Python package
scaffold within seconds. This allows me to properly install the &lt;code&gt;mlstm4reco&lt;/code&gt; package and import its functionality from 
wherever I want without messing around with the &lt;span class="caps"&gt;PYTHONPATH&lt;/span&gt; environment variable which one should never do&amp;nbsp;anyway. &lt;/p&gt;
&lt;p&gt;For the evaluation matrix below I ran each experiment 200 times to give &lt;a href="http://hyperopt.github.io/hyperopt/"&gt;HyperOpt&lt;/a&gt; enough chances to find good 
hyperparameters for the number of epochs (&lt;code&gt;n_iter&lt;/code&gt;), number of embeddings (&lt;code&gt;embedding_dim&lt;/code&gt;), l2-regularisation (&lt;code&gt;l2&lt;/code&gt;),
batch size (&lt;code&gt;batch_size&lt;/code&gt;) and learning rate (&lt;code&gt;learn_rate&lt;/code&gt;). 
Each of our two models, i.e. &lt;code&gt;lstm&lt;/code&gt; and &lt;code&gt;mlstm&lt;/code&gt; user representation, were applied to three datasets, 
the &lt;a href="https://grouplens.org/datasets/movielens/"&gt;MovieLens&lt;/a&gt; 1m and 10m datasets as well as the &lt;a href="https://snap.stanford.edu/data/amazon-meta.html"&gt;Amazon&lt;/a&gt; dataset. For instance, to run 200 experiments with the mlstm 
model on the Movielens 10m dataset the command would be &lt;code&gt;./run.py -m mlstm -n 200 10m&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;In each experiment the data is split into a training, validation and test set where training is used to fit the model,
validation to find the right hyperparameters and test for the final evaluation after all parameters are determined. 
The performance of the models is measured with the help of the &lt;a href="https://en.wikipedia.org/wiki/Mean_reciprocal_rank"&gt;mean reciprocal rank&lt;/a&gt; (&lt;span class="caps"&gt;MRR&lt;/span&gt;) score. Here are the&amp;nbsp;results:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="right"&gt;dataset&lt;/th&gt;
&lt;th align="right"&gt;type&lt;/th&gt;
&lt;th align="right"&gt;validation&lt;/th&gt;
&lt;th align="right"&gt;test&lt;/th&gt;
&lt;th align="right"&gt;learn_rate&lt;/th&gt;
&lt;th align="right"&gt;batch_size&lt;/th&gt;
&lt;th align="right"&gt;embedding_dim&lt;/th&gt;
&lt;th align="right"&gt;l2&lt;/th&gt;
&lt;th align="right"&gt;n_iter&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="right"&gt;Movielens 1m&lt;/td&gt;
&lt;td align="right"&gt;&lt;span class="caps"&gt;LSTM&lt;/span&gt;&lt;/td&gt;
&lt;td align="right"&gt;0.1199&lt;/td&gt;
&lt;td align="right"&gt;0.1317&lt;/td&gt;
&lt;td align="right"&gt;1.93e-2&lt;/td&gt;
&lt;td align="right"&gt;208&lt;/td&gt;
&lt;td align="right"&gt;112&lt;/td&gt;
&lt;td align="right"&gt;6.01e-06&lt;/td&gt;
&lt;td align="right"&gt;50&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="right"&gt;Movielens 1m&lt;/td&gt;
&lt;td align="right"&gt;mLSTM&lt;/td&gt;
&lt;td align="right"&gt;0.1275&lt;/td&gt;
&lt;td align="right"&gt;0.1386&lt;/td&gt;
&lt;td align="right"&gt;1.25e-2&lt;/td&gt;
&lt;td align="right"&gt;240&lt;/td&gt;
&lt;td align="right"&gt;120&lt;/td&gt;
&lt;td align="right"&gt;5.90e-06&lt;/td&gt;
&lt;td align="right"&gt;40&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="right"&gt;Movielens 10m&lt;/td&gt;
&lt;td align="right"&gt;&lt;span class="caps"&gt;LSTM&lt;/span&gt;&lt;/td&gt;
&lt;td align="right"&gt;0.1090&lt;/td&gt;
&lt;td align="right"&gt;0.1033&lt;/td&gt;
&lt;td align="right"&gt;4.19e-3&lt;/td&gt;
&lt;td align="right"&gt;224&lt;/td&gt;
&lt;td align="right"&gt;120&lt;/td&gt;
&lt;td align="right"&gt;2.43e-07&lt;/td&gt;
&lt;td align="right"&gt;50&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="right"&gt;Movielens 10m&lt;/td&gt;
&lt;td align="right"&gt;mLSTM&lt;/td&gt;
&lt;td align="right"&gt;0.1142&lt;/td&gt;
&lt;td align="right"&gt;0.1115&lt;/td&gt;
&lt;td align="right"&gt;4.50e-3&lt;/td&gt;
&lt;td align="right"&gt;224&lt;/td&gt;
&lt;td align="right"&gt;128&lt;/td&gt;
&lt;td align="right"&gt;1.12e-06&lt;/td&gt;
&lt;td align="right"&gt;45&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="right"&gt;Amazon&lt;/td&gt;
&lt;td align="right"&gt;&lt;span class="caps"&gt;LSTM&lt;/span&gt;&lt;/td&gt;
&lt;td align="right"&gt;0.2629&lt;/td&gt;
&lt;td align="right"&gt;0.2642&lt;/td&gt;
&lt;td align="right"&gt;2.85e-3&lt;/td&gt;
&lt;td align="right"&gt;224&lt;/td&gt;
&lt;td align="right"&gt;128&lt;/td&gt;
&lt;td align="right"&gt;2.42e-11&lt;/td&gt;
&lt;td align="right"&gt;50&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="right"&gt;Amazon&lt;/td&gt;
&lt;td align="right"&gt;mLSTM&lt;/td&gt;
&lt;td align="right"&gt;0.3061&lt;/td&gt;
&lt;td align="right"&gt;0.3123&lt;/td&gt;
&lt;td align="right"&gt;2.48e-3&lt;/td&gt;
&lt;td align="right"&gt;144&lt;/td&gt;
&lt;td align="right"&gt;120&lt;/td&gt;
&lt;td align="right"&gt;4.53e-11&lt;/td&gt;
&lt;td align="right"&gt;50&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;If we compare the test results of the Movielens 1m dataset, it&amp;#8217;s an improvement of 5.30% when using mLSTM over &lt;span class="caps"&gt;LSTM&lt;/span&gt; 
representation, for Movielens 10m it&amp;#8217;s 7.96% more and for Amazon it&amp;#8217;s even 18.19%&amp;nbsp;more. &lt;/p&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;The performance improvements of using an mLSTM over an &lt;span class="caps"&gt;LSTM&lt;/span&gt; user representation are quite good but nothing spectacular.
They give us at least some indication that mLSTMs achieve superior results for sequential recommendation tasks. In order to 
further underpin this first assessment one could test with more datasets and also check other evaluation 
metrics besides &lt;span class="caps"&gt;MRR&lt;/span&gt;. I leave this to a dedicated reader, so if you are interested, please let me know and share your
results. With regard to my initial motivation and tasks, I have achieved much deeper insights into the domain of
sequential recommenders and with the help of PyTorch, Spotlight I am looking forward to my next side project! Let me
know if you liked this post and comment&amp;nbsp;below.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js','color.js'], equationNumbers: { autoNumber: 'auto' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="post"></category><category term="python"></category><category term="data science"></category><category term="deep learning"></category><category term="recommender systems"></category></entry></feed>