<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Florian Wilhelm</title><link href="http://www.florianwilhelm.info/" rel="alternate"></link><link href="http://www.florianwilhelm.info/feeds/post.atom.xml" rel="self"></link><id>http://www.florianwilhelm.info/</id><updated>2016-04-07T12:00:00+02:00</updated><entry><title>Leveraging the Value of Big Data with Automated Decision Making</title><link href="http://www.florianwilhelm.info/2016/04/leveraging_the_value_of-big_data_with_automated_decision_making/" rel="alternate"></link><updated>2016-04-07T12:00:00+02:00</updated><author><name>Florian Wilhelm</name></author><id>tag:www.florianwilhelm.info,2016-04-07:2016/04/leveraging_the_value_of-big_data_with_automated_decision_making/</id><summary type="html">&lt;p&gt;It is a widely accepted fact that we are living in the era of Big Data. Many
traditional companies are looking for ways to improve their business through
the virtues of Big Data and Data Science. While matured startups born in this
era like Facebook and Twitter seem to naturally exploit the value of their data,
many traditional companies struggle to find new ways of utilizing their data to
leverage its value for their classical&amp;nbsp;businesses.&lt;/p&gt;
&lt;p&gt;In this post I elaborate on the specific domain of decision making where Big Data
and Data Science can help to improve the efficiency of conventional businesses.
Proving the benefits of Big Data in a lighthouse project is of utmost importance
in long-established companies with regard to overcoming initial resistance in
the digital transformation of business processes. We will see that the
automatization of operational decisions, i.e. routine decisions related to the
day-to-day running of the business, are especially suitable candidates for
lighthouse projects to prove the value of Big Data in a&amp;nbsp;company.&lt;/p&gt;
&lt;p&gt;The notion of automating data-driven decisions with the help of Data Science is
often denoted with the term Prescriptive Analytics, which can be regarded as the
conclusive step after Predictive Analytics. In other words, the predictions
generated with the help of Predictive Analytics are used to optimize a predefined
metric under consideration of side conditions, strategic direction, business
processes etc. to derive excellent business decisions. The
&lt;a href="http://www.gartner.com/it-glossary/predictive-analytics/"&gt;predictive analytics diagram from Gartner&lt;/a&gt; illustrates the business value
compared to the difficulty of different analytical&amp;nbsp;approaches.&lt;/p&gt;
&lt;p&gt;In many businesses repetitive operational decisions consume lots of working time.
For instance pricing of articles and services, replenishment of stores or stocks,
demand forecasts and customer services involve operational decisions which are
often conducted in a manual process supported by traditional, rule based decision
support systems. Automating these decisions with the help of data-driven decision
systems has several&amp;nbsp;benefits:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Labor costs are reduced and scarce expertise can be leveraged for non-routine,
exceptional decisions. Less routine decisions means having more time for decisions
in extraordinary circumstances as well as decisions that are of a more tactical
or strategical nature. This encompasses also decisions in situations where data
is lacking as well as decisions about creative and visionary solutions. For
instance, no machine learning algorithm could have ever predicted the success of
the first iPhone since it was something completely new and its success was a
consequence of not only that but also many other soft&amp;nbsp;factors.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The quality of decisions is improved given that all information sources used in
the manual decision process are available as machine readable data. Modern machine
learning algorithm are able to quickly analyze huge amounts of data that a human
being could never even read in a lifetime. This plethora of data allows the
inference of patterns that lead to fast, consistent, high quality decisions which
are resistant to the &lt;a href="https://en.wikipedia.org/wiki/List_of_cognitive_biases"&gt;long list of cognitive biases&lt;/a&gt; or &lt;a href="https://en.wikipedia.org/wiki/Decision_fatigue"&gt;decision fatigue&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Prescriptive Analytics allows to scale the number of decisions. Too often in
traditional businesses, decisions are made by not actually taking a decision which
consequently leads to idleness and thus unexploited potential. Being able to scale
the number of decisions, enables this untapped potential to be fully realized and
can also generate new services. Imagine for instance that one marketing tool of a
company is to give special offerings and product recommendations based on different
market segments, not single customers. Being able to scale the number of decisions
due to automation would allow special offerings and recommendations for individual
customers, just like Amazon’s recommendation system.
To justify our statement that the automation of routine decisions with Prescriptive
Analytics is exceptionally well suited as a pioneer project in a traditional company,
it is necessary to elaborate on certain characteristics that many operational decisions&amp;nbsp;hold.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;While a single operational decision, e.g. a small change in the price of a single
article, may have an insignificantly small but direct impact on the revenue of
the whole business, the sum of all decisions quite often has great economic impact.
This is due to the fact that the frequency of operational decisions is often huge,
meaning that a small overall improvement in decision quality is highly profitable.
Obviously, candidates for a Prescriptive Analytics project should have exactly
these properties of high and direct economic impact. The ability to measure such
an impact requires that a performance metric or key performance indicator (&lt;span class="caps"&gt;KPI&lt;/span&gt;)
is already established. This is another important prerequisite for a successful
Prescriptive Analytics&amp;nbsp;project.&lt;/p&gt;
&lt;p&gt;Since operational decisions are often related to the core of the business even in
traditional companies huge amounts of data are already collected and available.
Often, routine decisions that are taken by analyzing spreadsheets and personal
experience are based on data with high predictive power. A quote, often attributed
to Mark Twain, says that “history doesn’t repeat itself, but it does rhyme” which
captures the essence of what automated decision making is about. Having lots of
data about past events allows us to find patterns and relationships which can
predict future events to some extent. The goal is to develop a model that describes
what happened in the past without being bound to the past and thus allowing to
apply the model to the future. In just the same way as our brain learns from
experiences and infers future outcomes in similar&amp;nbsp;situations.&lt;/p&gt;
&lt;p&gt;Consequently, the high frequency of routine decision with a direct economic impact
combined with an abundance of data and a metric to measure performance are
favorable characteristic of a business process that can be successfully automated.
In order to quantify the added value of Prescriptive Analytics an estimation of
the gain in decision quality and its impact on revenue is needed with the help
of the predefined metric or &lt;span class="caps"&gt;KPI&lt;/span&gt;. For this complex estimation it is recommended
for traditional companies to have an experienced partner alongside and optionally
a proof of concept to evaluate the predictive power of the data and the business
case as a&amp;nbsp;whole.&lt;/p&gt;
&lt;p&gt;We should not ignore the fact that automation also includes costs encompassing the
maintenance, licence &lt;span class="amp"&gt;&amp;amp;&lt;/span&gt; support etc. of an established automated decision system. The
automation costs depend mainly on the order of magnitude of performed decisions as well
as the time frame. For instance the &lt;span class="caps"&gt;IT&lt;/span&gt; setup of a decision system for one million decisions
per day will be much smaller than the setup for one billion that need to be determined in real-
time. The initial costs for the implementation of an automated decision system varies largely
depending on the domain of application, the necessary changes in the business processes
and other factors. An estimation of these costs is needed to determine the time-to-value.
Since the added value of an automated decision system quite often heavily surpasses
automation costs by at least one order of magnitude time-to-value is often&amp;nbsp;low.&lt;/p&gt;
&lt;p&gt;We conclude that the scaling in the number of decisions and the improved effectiveness of
the decisions are the main drivers of the added value in automated decision making.
Subsequently, operational decisions that are ubiquitous and directly influence the business
value are well suited for a Predictive Analytics light-house&amp;nbsp;project.&lt;/p&gt;
&lt;p&gt;Following mnemonic recaps the main qualities of a successful Prescriptive Analytics project.
It consists of the following questions that should be answered&amp;nbsp;positively:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Are decisions taken &lt;strong&gt;F&lt;/strong&gt;requently?&lt;/li&gt;
&lt;li&gt;Does the business process allow the &lt;strong&gt;A&lt;/strong&gt;utomation of&amp;nbsp;decisions?&lt;/li&gt;
&lt;li&gt;Are &lt;strong&gt;M&lt;/strong&gt;etrics defined to determine the quality of a&amp;nbsp;decision?&lt;/li&gt;
&lt;li&gt;Do decisions have a direct &lt;strong&gt;E&lt;/strong&gt;conomic&amp;nbsp;impact?&lt;/li&gt;
&lt;li&gt;Is enough and suitable &lt;strong&gt;D&lt;/strong&gt;ata available to base decisions&amp;nbsp;on?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Prescriptive Analytics projects with these properties are very likely to become &lt;strong&gt;famed&lt;/strong&gt;
in your company. The successful implementation of a lighthouse project in the business process
generates momentum for new projects. This drives the digital transformation of a classical
business in an iterative&amp;nbsp;way.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;This article was originally posted on the &lt;a href="http://www.21stcenturyit.de/leveraging-the-value-of-big-data-with-automated-decision-making/"&gt;&lt;span class="caps"&gt;CSC&lt;/span&gt; 21st Century &lt;span class="caps"&gt;IT&lt;/span&gt; blog&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;</summary><category term="big data"></category><category term="data science"></category><category term="predictive analytics"></category></entry><entry><title>Interactively visualizing distributions in a Jupyter notebook with Bokeh</title><link href="http://www.florianwilhelm.info/2016/03/jupyter_distribution_visualizer/" rel="alternate"></link><updated>2016-03-26T09:00:00+01:00</updated><author><name>Florian Wilhelm</name></author><id>tag:www.florianwilhelm.info,2016-03-26:2016/03/jupyter_distribution_visualizer/</id><summary type="html">&lt;p&gt;If you are doing probabilistic programming you are dealing with all kinds of
different distributions. That means choosing an ensemble of right distributions
which describe the underlying real-world process in a suitable way but also
choosing the right parameters for prior distributions. At that point I often
start visualizing the distributions with the help of &lt;a href="http://jupyter.org/"&gt;Jupyter&lt;/a&gt; notebooks,
&lt;a href="http://matplotlib.org/"&gt;matplotlib&lt;/a&gt; and &lt;a href="http://www.scipy.org/"&gt;SciPy&lt;/a&gt; to get a feeling how the distribution behaves when
changing its parameters. And please don&amp;#8217;t tell me you are able to visualize all the
distributions &lt;a href="http://docs.scipy.org/doc/scipy/reference/stats.html"&gt;scipy.stats&lt;/a&gt; has to offer just in your&amp;nbsp;head.&lt;/p&gt;
&lt;p&gt;For me, this surely is a repetitive task that every good and lazy programmer tries
to avoid. Additionally, I was never quite satisfied with the interactivity of
matplotlib in a notebook. Granted, the &lt;code&gt;%matplotlib notebook&lt;/code&gt; magic was a huge
step into the right direction but there is still much room for improvement.
The new and shiny kid on the block is &lt;a href="http://bokeh.pydata.org/"&gt;Bokeh&lt;/a&gt; and so far I have not really done
much with it, meaning it is a good candidate for a test ride. The same goes
actually for Jupyter&amp;#8217;s &lt;a href="http://ipywidgets.readthedocs.org/"&gt;ipywidgets&lt;/a&gt; and you see where this going. No evaluation
of a tool without a proper goal and that is now set to developing an interactive
visualization widget for Jupyter based on Bokeh and ipywidgets. So here we&amp;nbsp;go!&lt;/p&gt;
&lt;p&gt;It turned out that this task is easier than expected due the good documentation
and examples of ipywidgets and especially Bokeh. You can read all about the
implementation inside this &lt;a href="https://github.com/FlorianWilhelm/distvis/blob/master/index.ipynb"&gt;notebook&lt;/a&gt; which is hosted in a separate
&lt;a href="https://github.com/FlorianWilhelm/distvis"&gt;Github repository&lt;/a&gt;. This also always me to make use of a new service that I
just recently learned about, &lt;a href="http://mybinder.org/"&gt;binder&lt;/a&gt;. This totally rad service takes any
Github repository with a Jupyter notebook in it, fires up a container with Kubernetes,
installs necessary requirements and finally runs your notebook! By just clicking
on a link! Amazing to see how the ecosystem around Jupyter develops these&amp;nbsp;days.&lt;/p&gt;
&lt;p&gt;And of course to wet your appetite, here are the screenshots of the final tool
that you will experience interactively by &lt;a href="http://mybinder.org/repo/FlorianWilhelm/distvis"&gt;starting the notebook with binder&lt;/a&gt;.&lt;/p&gt;
&lt;figure&gt;
&lt;img class="noZoom" src="/images/alpha_dist.png" alt="Alpha distribution"&gt;
&lt;figcaption&gt;The probability density function of a continuous alpha distribution with shape parameter a=1.3&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;
&lt;figure&gt;
&lt;img class="noZoom" src="/images/binom_dist.png" alt="Binomial distribution"&gt;
&lt;figcaption&gt;The probability mass function of a discrete binomial distribution with shape parameters n=10 and p=0.7&lt;/figcaption&gt;
&lt;/figure&gt;</summary><category term="jupyter"></category><category term="python"></category><category term="scipy"></category><category term="bokeh"></category></entry></feed>