<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Florian Wilhelm - post</title><link href="https://florianwilhelm.info/" rel="alternate"></link><link href="https://florianwilhelm.info/feeds/post.atom.xml" rel="self"></link><id>https://florianwilhelm.info/</id><updated>2018-08-05T16:00:00+02:00</updated><entry><title>Multiplicative LSTM for sequence-based Recommenders</title><link href="https://florianwilhelm.info/2018/08/multiplicative_LSTM_for_sequence_based_recos/" rel="alternate"></link><published>2018-08-05T16:00:00+02:00</published><updated>2018-08-05T16:00:00+02:00</updated><author><name>Florian Wilhelm</name></author><id>tag:florianwilhelm.info,2018-08-05:/2018/08/multiplicative_LSTM_for_sequence_based_recos/</id><summary type="html">&lt;p&gt;Recommender Systems support the decision making processes of customers with personalized&amp;nbsp;suggestions.&lt;/p&gt;</summary><content type="html">&lt;p&gt;They are widely used and influence the daily life of almost everyone in different domains like e-commerce, 
social media, or entertainment. Quite often the dimension of time plays a dominant role in the generation
of a relevant&amp;nbsp;recommendation.&lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;Motivation&lt;/h2&gt;
&lt;p&gt;Recommender Systems support the decision making processes of customers with personalized suggestions. 
They are widely used and influence the daily life of almost everyone in different domains like e-commerce, 
social media, or entertainment. Quite often the dimension of time plays a dominant role in the generation
of a relevant recommendation. Which user interaction occurred just before the point of time where we want to 
provide a recommendation?
How many interactions ago did the user interact with an item like this one?
Traditional user-item recommenders often neglect the dimension of time completely. 
This means that many traditional recommenders find for each user a latent representation based on the user&amp;#8217;s
historical item interactions without any notion of recency and sequence of interactions. To also incorporate 
this kind of contextual information about interactions, sequence-based recommenders were developed. 
With the advent of deep learning quite a few of them are nowadays based on &lt;a href="https://en.wikipedia.org/wiki/Recurrent_neural_network"&gt;Recurrent Neural Networks&lt;/a&gt;&amp;nbsp;(RNNs).&lt;/p&gt;
&lt;p&gt;Whenever I want to dig deeper into a topic like sequence-based recommenders I follow a few simple steps:
First of all, to learn something I directly need to apply it otherwise learning things doesn&amp;#8217;t work for me. In order to apply something I need a challenge and a small goal that keeps me motivated on the journey. Following the &lt;a href="https://en.wikipedia.org/wiki/SMART_criteria"&gt;&lt;span class="caps"&gt;SMART&lt;/span&gt; citeria&lt;/a&gt; a goal needs to be measurable and thus a typical outcome for me is a blog post like the one you are just reading. Another good thing about a blog post is the fact that no one wants to publish something completely crappy, so there is an intrinsic quality assurance attached to the whole process. This blog post is actually the outcome of several things I wanted to familiarize myself more and try&amp;nbsp;out:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="https://pytorch.org/"&gt;PyTorch&lt;/a&gt;, since this framework is used in a large fraction of recent publications about deep&amp;nbsp;learning,&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/maciejkula/spotlight"&gt;Spotlight&lt;/a&gt;, since this library gives you a sophisticated structure to play around with new ideas for recommender systems and already has a lot of functionality&amp;nbsp;implemented,&lt;/li&gt;
&lt;li&gt;applying a paper about &lt;a href="https://arxiv.org/abs/1609.07959"&gt;Multiplicative &lt;span class="caps"&gt;LSTM&lt;/span&gt; for sequence modelling&lt;/a&gt; to recommender systems and see how that performs compared to traditional&amp;nbsp;LSTMs.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Since Spotlight is based on PyTorch and multiplicative LSTMs (mLSTMs) are not yet implemented in PyTorch the task of evaluating mLSTMs vs. LSTMs inherently addresses all those points outlined above. The goal is set, so let&amp;#8217;s get&amp;nbsp;going!&lt;/p&gt;
&lt;h2&gt;Theory&lt;/h2&gt;
&lt;p&gt;Long short-term memory architectures (LSTMs) are maybe the most common incarnations of RNNs since they don&amp;#8217;t adhere 
to the &lt;a href="https://en.wikipedia.org/wiki/Vanishing_gradient_problem"&gt;vanishing gradient problem&lt;/a&gt; and thus are able to capture long-term relationships in a sequence. You can find a great
explanation of LSTMs in Colah&amp;#8217;s post &lt;a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/"&gt;Understanding &lt;span class="caps"&gt;LSTM&lt;/span&gt; Networks&lt;/a&gt; and more general about the power of RNNs in the 
article &lt;a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/"&gt;The Unreasonable Effectiveness of Recurrent Neural Networks&lt;/a&gt;. 
More recently, also Gated Recurrent Units (GRUs) which have a simplified structure compared to LSTMs are also used 
in sequential prediction tasks with occasionally superior results. &lt;a href="https://github.com/maciejkula/spotlight"&gt;Spotlight&lt;/a&gt; provides a sequential recommender based on LSTMs and 
the quite renowned &lt;a href="https://github.com/hidasib/GRU4Rec"&gt;GRU4Rec&lt;/a&gt; model uses GRUs but in general it&amp;#8217;s not possible to state that one always outperforms the&amp;nbsp;other.&lt;/p&gt;
&lt;p&gt;So given these ingredients, how do we now construct a sequential recommender? Let&amp;#8217;s assume on every timestep 
&lt;span class="math"&gt;\(t\in\{1,\ldots,T\}\)&lt;/span&gt; a user has interacted with an item &lt;span class="math"&gt;\(i_t\)&lt;/span&gt;. The basic idea is now to feed these interactions into
 an &lt;span class="caps"&gt;LSTM&lt;/span&gt; up to the time &lt;span class="math"&gt;\(t\)&lt;/span&gt; in order to get a representation of the user&amp;#8217;s preferences &lt;span class="math"&gt;\(h_t\)&lt;/span&gt; and use these to state
 if the user might like or dislike the next item &lt;span class="math"&gt;\(i_{t+1}\)&lt;/span&gt;. Just like in a non-sequential recommender we also do a
 &lt;a href="https://en.wikipedia.org/wiki/One-hot"&gt;one-hot encoding&lt;/a&gt; of the items followed by an embedding into a dense vector representation &lt;span class="math"&gt;\(e_{i_t}\)&lt;/span&gt;
 which is then feed into the &lt;span class="caps"&gt;LSTM&lt;/span&gt;. We can then just use the output &lt;span class="math"&gt;\(h_t\)&lt;/span&gt; of the &lt;span class="caps"&gt;LSTM&lt;/span&gt; and calculate the inner product (&lt;span class="math"&gt;\(\bigotimes\)&lt;/span&gt;) 
 with the embedding &lt;span class="math"&gt;\(e_{i_{t+1}}\)&lt;/span&gt; plus an item bias for varying item popularity to retrieve an output &lt;span class="math"&gt;\(p_{t+1}\)&lt;/span&gt;. 
 This output along with others is then used to calculate the actual loss depending on our sample strategy and loss function. 
 We train our model by sampling positive interactions and corresponding negative interactions. In an &lt;em&gt;explicit feedback&lt;/em&gt; context 
 a positive and negative interaction might be a positive and negative rating of a user for an item, respectively. In an &lt;em&gt;implicit feedback&lt;/em&gt; context, all item interactions of a user are considered positive whereas negative interactions arise from items the
 user did not interact with.
 During the training we adapt the weights of our model so that for a given user the scalar output of positive interaction
 is greater than the output of a negative interaction. This can be seen as an approximation to a &lt;a href="https://en.wikipedia.org/wiki/Softmax_function"&gt;softmax&lt;/a&gt; in very high-dimensional output&amp;nbsp;space.&lt;/p&gt;
&lt;p&gt;Figure 1 illustrates our sequential recommender model and this is what&amp;#8217;s actually happening inside Spotlight&amp;#8217;s 
 sequential recommender with an &lt;span class="caps"&gt;LSTM&lt;/span&gt; representation. If you raise your eyebrow due to the usage of an inner product
 then be aware that &lt;a href="https://en.wikipedia.org/wiki/Low-rank_approximation"&gt;low-rank approximations&lt;/a&gt; have been and still are one of the most successful building blocks
 of recommender systems. An alternative would be to replace the inner product with a deep feed forward network but
 to quite some extent, this would also just learn to perform an approximation of an inner product. A recent paper
 &lt;a href="https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/46488.pdf"&gt;Latent Cross: Making Use of Context in Recurrent Recommender Systems&lt;/a&gt; by Google also emphasizes the power of learning
 low-rank relations with the help of inner&amp;nbsp;products.&lt;/p&gt;
&lt;figure&gt;
&lt;p align="center"&gt;
&lt;img class="noZoom" src="/images/mLSTM.png" alt="mLSTM"&gt;
&lt;figcaption&gt;&lt;strong&gt;Figure 1:&lt;/strong&gt; At timestep $t$ the item $i_t$ is embedded and fed into an &lt;span class="caps"&gt;LSTM&lt;/span&gt; together with
 cell state $C_{t-1}$ and $h_{t-1}$ of the last timestep which yields a new presentation $h_t$. The inner product of 
 $h_t$ with the embedding of the potential next item $e_{i_{t+1}}$ yields a scalar value corresponding to how likely the user
 would interact with $i_{t+1}$.&lt;/figcaption&gt;
&lt;/p&gt;
&lt;/figure&gt;

&lt;p&gt;What we want to do is basically replacing the &lt;span class="caps"&gt;LSTM&lt;/span&gt; part of Spotlight&amp;#8217;s sequential recommender with an mLSTM. 
But before we do that the obvious question is why? Let&amp;#8217;s recap the formulae of a typical &lt;a href="http://pytorch.org/docs/0.3.1/nn.html?highlight=lstm#torch.nn.LSTM"&gt;&lt;span class="caps"&gt;LSTM&lt;/span&gt; implementation&lt;/a&gt; 
like the one in&amp;nbsp;PyTorch:&lt;/p&gt;
&lt;div class="math"&gt;\begin{split}\begin{array}{ll}
i_t = \mathrm{sigmoid}(W_{ii} x_t + b_{ii} + W_{hi} h_{t-1} + b_{hi}) \\
f_t = \mathrm{sigmoid}(W_{if} x_t + b_{if} + W_{hf} h_{t-1} + b_{hf}) \\
g_t = \tanh(W_{ig} x_t + b_{ig} + W_{hc} h_{t-1} + b_{hg}) \\
o_t = \mathrm{sigmoid}(W_{io} x_t + b_{io} + W_{ho} h_{t-1} + b_{ho}) \\
c_t = f_t * c_{t-1} + i_t * g_t \\
h_t = o_t * \tanh(c_t)
\end{array}\end{split}&lt;/div&gt;
&lt;p&gt;
&lt;br&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class="math"&gt;\(i_t\)&lt;/span&gt; denotes the input gate, &lt;span class="math"&gt;\(f_t\)&lt;/span&gt; the forget gate and &lt;span class="math"&gt;\(o_t\)&lt;/span&gt; the output gate at timestep &lt;span class="math"&gt;\(t\)&lt;/span&gt;. If we look at
those lines again we can see a lot of terms in the form of &lt;span class="math"&gt;\(W_{**} x_t + W_{**} h_{t-1}\)&lt;/span&gt; neglecting the biases &lt;span class="math"&gt;\(b_*\)&lt;/span&gt; for a
moment. Thus a lot of an &lt;span class="caps"&gt;LSTM&lt;/span&gt;&amp;#8217;s inner workings depend on the addition of the transformed input with the transformed hidden
state. So what happens if a trained &lt;span class="caps"&gt;LSTM&lt;/span&gt; with thus fixed &lt;span class="math"&gt;\(W_{**}\)&lt;/span&gt; encounters some unexpected, completely surprising input
&lt;span class="math"&gt;\(x_t\)&lt;/span&gt;? This might disturb the cell state &lt;span class="math"&gt;\(c_t\)&lt;/span&gt; leading to pertubated future &lt;span class="math"&gt;\(h_t\)&lt;/span&gt; and it might take a long time for the
&lt;span class="caps"&gt;LSTM&lt;/span&gt; to recover from that singular surprising input. The paper &lt;a href="https://arxiv.org/abs/1609.07959"&gt;Multiplicative &lt;span class="caps"&gt;LSTM&lt;/span&gt; for sequence modelling&lt;/a&gt; now argues
that &amp;#8220;&lt;span class="caps"&gt;RNN&lt;/span&gt; architectures with hidden-to-hidden transition functions that are input-dependent are better suited to recover 
from surprising inputs&amp;#8221;. By allowing the hidden state to react flexibly on the new input by changing it&amp;#8217;s magnitude it might be
able to recover from mistakes faster. The quite vague formulation of &lt;em&gt;input-dependent transition functions&lt;/em&gt; is then 
actually achieved in a quite simple way. In an mLSTM the hidden state &lt;span class="math"&gt;\(h_{t-1}\)&lt;/span&gt; is transformed in a multiplicative way
using the input &lt;span class="math"&gt;\(x_t\)&lt;/span&gt; into an intermediate state &lt;span class="math"&gt;\(m_t\)&lt;/span&gt; before it is used in a plain &lt;span class="caps"&gt;LSTM&lt;/span&gt; as before. Eventually, there
is only a single equation to be prepended to the equations of an &lt;span class="caps"&gt;LSTM&lt;/span&gt;:&lt;/p&gt;
&lt;div class="math"&gt;\begin{split}\begin{array}{ll}
m_t = (W_{im} x_t + b_{im}) \odot{} ( W_{hm} h_{t-1} + b_{hm}) \\
i_t = \mathrm{sigmoid}(W_{ii} x_t + b_{ii} + W_{mi} m_t + b_{mi}) \\
f_t = \mathrm{sigmoid}(W_{if} x_t + b_{if} + W_{mf} m_t + b_{mf}) \\
g_t = \tanh(W_{ig} x_t + b_{ig} + W_{mc} m_t + b_{mg}) \\
o_t = \mathrm{sigmoid}(W_{io} x_t + b_{io} + W_{mo} m_t + b_{mo}) \\
c_t = f_t * c_{t-1} + i_t * g_t \\
h_t = o_t * \tanh(c_t)
\end{array}\end{split}&lt;/div&gt;
&lt;p&gt;
&lt;br&gt;&lt;/p&gt;
&lt;p&gt;The element-wise multiplication (&lt;span class="math"&gt;\(\odot\)&lt;/span&gt;) allows &lt;span class="math"&gt;\(m_t\)&lt;/span&gt; to flexibly change it&amp;#8217;s value with respect to &lt;span class="math"&gt;\(h_{t-1}\)&lt;/span&gt; and &lt;span class="math"&gt;\(x_t\)&lt;/span&gt;.
On a more theoretical note, if you picture the hidden states of an &lt;span class="caps"&gt;LSTM&lt;/span&gt; as a tree depending on the inputs at each timestep
then the number of all possible states at timestep &lt;span class="math"&gt;\(t\)&lt;/span&gt; will be much larger for an mLSTM compared to an &lt;span class="caps"&gt;LSTM&lt;/span&gt;. Therefore, 
the tree of an mLSTM will be much wider and consequently more flexible to represent different probability distributions
according to the paper. The paper focuses only on &lt;span class="caps"&gt;NLP&lt;/span&gt; tasks but since surprising inputs are also a concern in sequential recommender systems,
the self-evident idea is to evaluate if mLSTMs excel in recommender&amp;nbsp;tasks. &lt;/p&gt;
&lt;h2&gt;Implementation&lt;/h2&gt;
&lt;p&gt;Everyone seems to love &lt;a href="https://pytorch.org/"&gt;PyTorch&lt;/a&gt; for it&amp;#8217;s beautiful &lt;span class="caps"&gt;API&lt;/span&gt; and I totally agree. For me its beauty lies in its simplicity. 
Every elementary building block of a neural network like a linear transformation is called a &lt;em&gt;Module&lt;/em&gt; in PyTorch. A
Module is just a class that inherits from &lt;code&gt;Module&lt;/code&gt; and implements a &lt;code&gt;forward&lt;/code&gt; method that does the transformation
with the help of tensor operations. A more complex neural network is again just a &lt;code&gt;Module&lt;/code&gt; and uses the 
&lt;a href="https://en.wikipedia.org/wiki/Composition_over_inheritance"&gt;composition principle&lt;/a&gt; to compose its functionality from simpler modules. Therefore, in my humble opinion, PyTorch
found a much nicer concept of combining low-level tensor operations with the high level composition of layers compared
to core &lt;a href="https://www.tensorflow.org/"&gt;TensorFlow&lt;/a&gt; and &lt;a href="https://keras.io/"&gt;Keras&lt;/a&gt; where you are either stuck on the level of low-level tensor 
operations or the composition of&amp;nbsp;layers. &lt;/p&gt;
&lt;p&gt;For our task, we gonna need an &lt;code&gt;mLSTM&lt;/code&gt; module and luckily PyTorch provides &lt;code&gt;RNNBase&lt;/code&gt;, a base class for custom RNNs.
So all we have to do is to write a module that inherits from &lt;code&gt;RNNBase&lt;/code&gt;, defines additional parameters and implements
the mLSTM equations inside of &lt;code&gt;forward&lt;/code&gt;: &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;math&lt;/span&gt;

&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;torch&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;torch.nn&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Parameter&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;torch.nn.modules.rnn&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;RNNBase&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;LSTMCell&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;torch.nn&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;functional&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;F&lt;/span&gt;


&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;mLSTM&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;RNNBase&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;input_size&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;hidden_size&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;bias&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="nb"&gt;super&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mLSTM&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
            &lt;span class="n"&gt;mode&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;LSTM&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;input_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;input_size&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;hidden_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;input_size&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                 &lt;span class="n"&gt;num_layers&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;bias&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;bias&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;batch_first&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                 &lt;span class="n"&gt;dropout&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;bidirectional&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;False&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="n"&gt;w_im&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Tensor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;hidden_size&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;input_size&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;w_hm&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Tensor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;hidden_size&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;hidden_size&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;b_im&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Tensor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;hidden_size&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;b_hm&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Tensor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;hidden_size&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;w_im&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Parameter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;w_im&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;b_im&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Parameter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;b_im&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;w_hm&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Parameter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;w_hm&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;b_hm&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Parameter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;b_hm&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;lstm_cell&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;LSTMCell&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;input_size&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;hidden_size&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;bias&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reset_parameters&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;reset_parameters&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;stdv&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;math&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sqrt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;hidden_size&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;weight&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;parameters&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
            &lt;span class="n"&gt;weight&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;uniform_&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;stdv&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;stdv&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;forward&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;input&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;hx&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;n_batch&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_seq&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_feat&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;input&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

        &lt;span class="n"&gt;hx&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cx&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;hx&lt;/span&gt;
        &lt;span class="n"&gt;steps&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;cx&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;unsqueeze&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;seq&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_seq&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="n"&gt;mx&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;F&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linear&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;input&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="n"&gt;seq&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;:],&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;w_im&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;b_im&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;F&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linear&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;hx&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;w_hm&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;b_hm&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;hx&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mx&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cx&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;hx&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cx&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;lstm_cell&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;input&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="n"&gt;seq&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;:],&lt;/span&gt; &lt;span class="n"&gt;hx&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;steps&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cx&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;unsqueeze&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cat&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;steps&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dim&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The code is pretty much self-explanatory. We inherit from &lt;code&gt;RNNBase&lt;/code&gt; and initialize the additional parameters we need for the calculation of &lt;span class="math"&gt;\(m_t\)&lt;/span&gt;
in &lt;code&gt;__init__&lt;/code&gt;. In &lt;code&gt;forward&lt;/code&gt; we use those parameters to calculate &lt;span class="math"&gt;\(m_t = (W_{im} x_t + b_{im}) \odot{} ( W_{hm} h_{t-1} + b_{hm})\)&lt;/span&gt; with the help of &lt;code&gt;F.linear&lt;/code&gt; and pass it to an ordinary &lt;code&gt;LSTMCell&lt;/code&gt;. We collect the results for each timestep
in our sequence in &lt;code&gt;steps&lt;/code&gt; and return it as concatenated&amp;nbsp;tensor. &lt;/p&gt;
&lt;p&gt;The &lt;a href="https://github.com/maciejkula/spotlight"&gt;Spotlight&lt;/a&gt; library, in the spirit of PyTorch, also follows a modular concept of components that can be easily plugged together and replaced.
It has only five&amp;nbsp;components:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;embedding layers&lt;/strong&gt; which map item ids to dense&amp;nbsp;vectors,&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;user/item representations&lt;/strong&gt; which take embedding layers to calculate latent representations and the score for a 
    user/item&amp;nbsp;pair, &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;interactions&lt;/strong&gt; which give easy access to the usr/item interactions and their explicit/implicit&amp;nbsp;feedback,&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;losses&lt;/strong&gt; which define the objective for the recommendation&amp;nbsp;task,&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;models&lt;/strong&gt; which take user/item representations, the user/item interactions and a given loss to train the&amp;nbsp;network.  &lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Due to this modular layout, we only need to write a new user/item representation module called &lt;code&gt;mLSTMNet&lt;/code&gt;. Since this
is straight-forward I leave it to you to take a look at the source code in my &lt;a href="https://github.com/FlorianWilhelm/mlstm4reco"&gt;mlstm4reco&lt;/a&gt; repository.
At this point I should mentioned that the whole layout of the repository was strongly inspired by Maciej Kula&amp;#8217;s 
&lt;a href="https://arxiv.org/abs/1711.08379"&gt;Mixture-of-tastes Models for Representing Users with Diverse Interests&lt;/a&gt; paper and the accompanying &lt;a href="https://github.com/maciejkula/mixture"&gt;source code&lt;/a&gt;.
My implementation also follows his advise of using an automatic hyperparameter optimisation for my own model and the
baseline model for comparison. This avoids quite a common bias in research when people put more effort in hand-tuning
their own model compared to the baseline to later show a better improvement in order to get the paper accepted.
Using a tool like &lt;a href="http://hyperopt.github.io/hyperopt/"&gt;HyperOpt&lt;/a&gt; for hyperparameter optimisation is quite easy and mitigates this bias to some extent at&amp;nbsp;least.&lt;/p&gt;
&lt;h2&gt;Evaluation&lt;/h2&gt;
&lt;p&gt;To compare Spotlight&amp;#8217;s &lt;a href="https://maciejkula.github.io/spotlight/sequence/implicit.html#module-spotlight.sequence.implicit"&gt;ImplicitSequenceModel&lt;/a&gt; with an &lt;span class="caps"&gt;LSTM&lt;/span&gt; to an mLSTM user representation, the
&lt;a href="https://github.com/FlorianWilhelm/mlstm4reco"&gt;mlstm4reco&lt;/a&gt; repository provides a &lt;code&gt;run.py&lt;/code&gt; script in the &lt;code&gt;experiments&lt;/code&gt; folder which takes several
command line options. Some might argue that this is a bit of over-engineering for a one time evaluation. 
But for me it&amp;#8217;s just one aspect of proper and reproducible research since it avoids errors and you can also easily
log which parameters were used to generate the results. I also used &lt;a href="https://pyscaffold.org/"&gt;PyScaffold&lt;/a&gt; to set up proper Python package
scaffold within seconds. This allows me to properly install the &lt;code&gt;mlstm4reco&lt;/code&gt; package and import its functionality from 
wherever I want without messing around with the &lt;span class="caps"&gt;PYTHONPATH&lt;/span&gt; environment variable which one should never do&amp;nbsp;anyway. &lt;/p&gt;
&lt;p&gt;For the evaluation matrix below I ran each experiment 200 times to give &lt;a href="http://hyperopt.github.io/hyperopt/"&gt;HyperOpt&lt;/a&gt; enough chances to find good 
hyperparameters for the number of epochs (&lt;code&gt;n_iter&lt;/code&gt;), number of embeddings (&lt;code&gt;embedding_dim&lt;/code&gt;), l2-regularisation (&lt;code&gt;l2&lt;/code&gt;),
batch size (&lt;code&gt;batch_size&lt;/code&gt;) and learning rate (&lt;code&gt;learn_rate&lt;/code&gt;). 
Each of our two models, i.e. &lt;code&gt;lstm&lt;/code&gt; and &lt;code&gt;mlstm&lt;/code&gt; user representation, were applied to three datasets, 
the &lt;a href="https://grouplens.org/datasets/movielens/"&gt;MovieLens&lt;/a&gt; 1m and 10m datasets as well as the &lt;a href="https://snap.stanford.edu/data/amazon-meta.html"&gt;Amazon&lt;/a&gt; dataset. For instance, to run 200 experiments with the mlstm 
model on the Movielens 10m dataset the command would be &lt;code&gt;./run.py -m mlstm -n 200 10m&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;In each experiment the data is split into a training, validation and test set where training is used to fit the model,
validation to find the right hyperparameters and test for the final evaluation after all parameters are determined. 
The performance of the models is measured with the help of the &lt;a href="https://en.wikipedia.org/wiki/Mean_reciprocal_rank"&gt;mean reciprocal rank&lt;/a&gt; (&lt;span class="caps"&gt;MRR&lt;/span&gt;) score. Here are the&amp;nbsp;results:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="right"&gt;dataset&lt;/th&gt;
&lt;th align="right"&gt;type&lt;/th&gt;
&lt;th align="right"&gt;validation&lt;/th&gt;
&lt;th align="right"&gt;test&lt;/th&gt;
&lt;th align="right"&gt;learn_rate&lt;/th&gt;
&lt;th align="right"&gt;batch_size&lt;/th&gt;
&lt;th align="right"&gt;embedding_dim&lt;/th&gt;
&lt;th align="right"&gt;l2&lt;/th&gt;
&lt;th align="right"&gt;n_iter&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="right"&gt;Movielens 1m&lt;/td&gt;
&lt;td align="right"&gt;&lt;span class="caps"&gt;LSTM&lt;/span&gt;&lt;/td&gt;
&lt;td align="right"&gt;0.1199&lt;/td&gt;
&lt;td align="right"&gt;0.1317&lt;/td&gt;
&lt;td align="right"&gt;1.93e-2&lt;/td&gt;
&lt;td align="right"&gt;208&lt;/td&gt;
&lt;td align="right"&gt;112&lt;/td&gt;
&lt;td align="right"&gt;6.01e-06&lt;/td&gt;
&lt;td align="right"&gt;50&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="right"&gt;Movielens 1m&lt;/td&gt;
&lt;td align="right"&gt;mLSTM&lt;/td&gt;
&lt;td align="right"&gt;0.1275&lt;/td&gt;
&lt;td align="right"&gt;0.1386&lt;/td&gt;
&lt;td align="right"&gt;1.25e-2&lt;/td&gt;
&lt;td align="right"&gt;240&lt;/td&gt;
&lt;td align="right"&gt;120&lt;/td&gt;
&lt;td align="right"&gt;5.90e-06&lt;/td&gt;
&lt;td align="right"&gt;40&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="right"&gt;Movielens 10m&lt;/td&gt;
&lt;td align="right"&gt;&lt;span class="caps"&gt;LSTM&lt;/span&gt;&lt;/td&gt;
&lt;td align="right"&gt;0.1090&lt;/td&gt;
&lt;td align="right"&gt;0.1033&lt;/td&gt;
&lt;td align="right"&gt;4.19e-3&lt;/td&gt;
&lt;td align="right"&gt;224&lt;/td&gt;
&lt;td align="right"&gt;120&lt;/td&gt;
&lt;td align="right"&gt;2.43e-07&lt;/td&gt;
&lt;td align="right"&gt;50&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="right"&gt;Movielens 10m&lt;/td&gt;
&lt;td align="right"&gt;mLSTM&lt;/td&gt;
&lt;td align="right"&gt;0.1142&lt;/td&gt;
&lt;td align="right"&gt;0.1115&lt;/td&gt;
&lt;td align="right"&gt;4.50e-3&lt;/td&gt;
&lt;td align="right"&gt;224&lt;/td&gt;
&lt;td align="right"&gt;128&lt;/td&gt;
&lt;td align="right"&gt;1.12e-06&lt;/td&gt;
&lt;td align="right"&gt;45&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="right"&gt;Amazon&lt;/td&gt;
&lt;td align="right"&gt;&lt;span class="caps"&gt;LSTM&lt;/span&gt;&lt;/td&gt;
&lt;td align="right"&gt;0.2629&lt;/td&gt;
&lt;td align="right"&gt;0.2642&lt;/td&gt;
&lt;td align="right"&gt;2.85e-3&lt;/td&gt;
&lt;td align="right"&gt;224&lt;/td&gt;
&lt;td align="right"&gt;128&lt;/td&gt;
&lt;td align="right"&gt;2.42e-11&lt;/td&gt;
&lt;td align="right"&gt;50&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="right"&gt;Amazon&lt;/td&gt;
&lt;td align="right"&gt;mLSTM&lt;/td&gt;
&lt;td align="right"&gt;0.3061&lt;/td&gt;
&lt;td align="right"&gt;0.3123&lt;/td&gt;
&lt;td align="right"&gt;2.48e-3&lt;/td&gt;
&lt;td align="right"&gt;144&lt;/td&gt;
&lt;td align="right"&gt;120&lt;/td&gt;
&lt;td align="right"&gt;4.53e-11&lt;/td&gt;
&lt;td align="right"&gt;50&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;If we compare the test results of the Movielens 1m dataset, it&amp;#8217;s an improvement of 5.30% when using mLSTM over &lt;span class="caps"&gt;LSTM&lt;/span&gt; 
representation, for Movielens 10m it&amp;#8217;s 7.96% more and for Amazon it&amp;#8217;s even 18.19%&amp;nbsp;more. &lt;/p&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;The performance improvements of using an mLSTM over an &lt;span class="caps"&gt;LSTM&lt;/span&gt; user representation are quite good but nothing spectacular.
They give us at least some indication that mLSTMs achieve superior results for sequential recommendation tasks. In order to 
further underpin this first assessment one could test with more datasets and also check other evaluation 
metrics besides &lt;span class="caps"&gt;MRR&lt;/span&gt;. I leave this to a dedicated reader, so if you have are interested, please let me know and share your
results. With regard to my initial motivation and tasks, I have achieved much deeper insights into the domain of
sequential recommenders and with the help of PyTorch, Spotlight I am looking forward to my next side project! Let me
know if you liked this post and comment&amp;nbsp;below.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="python"></category><category term="data science"></category><category term="deep learning"></category><category term="recommender systems"></category></entry><entry><title>Managing isolated Environments with PySpark</title><link href="https://florianwilhelm.info/2018/03/isolated_environments_with_pyspark/" rel="alternate"></link><published>2018-03-08T15:10:00+01:00</published><updated>2018-03-08T15:10:00+01:00</updated><author><name>Florian Wilhelm</name></author><id>tag:florianwilhelm.info,2018-03-08:/2018/03/isolated_environments_with_pyspark/</id><summary type="html">&lt;p&gt;The Spark data processing platform becomes more and more important for data scientists using Python. PySpark - the official Python &lt;span class="caps"&gt;API&lt;/span&gt; for Spark - makes it easy to get started but managing applications and their dependencies in isolated environments is no easy&amp;nbsp;task.&lt;/p&gt;</summary><content type="html">&lt;h2&gt;Motivation&lt;/h2&gt;
&lt;p&gt;With the sustained success of the Spark data processing platform even data scientists with a strong focus on the Python ecosystem can no longer ignore it.
Fortunately, it is easy to get started with &lt;a href="http://spark.apache.org/docs/latest/api/python/pyspark.html"&gt;PySpark&lt;/a&gt; - the official Python &lt;span class="caps"&gt;API&lt;/span&gt; for Spark - due to millions of word count tutorials on the web. In contrast to that, resources on how to deploy and use Python packages like Numpy, Pandas, Scikit-Learn in an isolated environment with PySpark are scarce. A nice exception to that is a &lt;a href="https://developerzen.com/best-practices-writing-production-grade-pyspark-jobs-cb688ac4d20f"&gt;blog post by Eran Kampf&lt;/a&gt;. Being able to install your own Python libraries is especially important if you want to write User-Defined-Functions (UDFs) as explained in the blog post &lt;a href="https://florianwilhelm.info/2017/10/efficient_udfs_with_pyspark/"&gt;Efficient &lt;span class="caps"&gt;UD&lt;/span&gt;(A)Fs with PySpark&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;For most Spark/Hadoop distributions, which is Cloudera in my case, there are basically two options for managing isolated&amp;nbsp;environments:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;You give all your data scientists &lt;span class="caps"&gt;SSH&lt;/span&gt; access to all your cluster&amp;#8217;s nodes and let them do whatever they want like installing virtual environments with &lt;a href="https://virtualenv.pypa.io/en/stable/"&gt;virtualenv&lt;/a&gt; or &lt;a href="https://conda.io/docs/intro.html"&gt;conda&lt;/a&gt; as detailed in the &lt;a href="https://www.cloudera.com/documentation/enterprise/5-6-x/topics/spark_python.html#spark_python__section_kr2_4zs_b5"&gt;Cloudera documentation&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Your sysadmins install Anaconda Parcels using the Cloudera Manager Admin Console to provide the most popular Python packages in a one size fits all fashion for all your data scientists as described in a &lt;a href="http://blog.cloudera.com/blog/2016/02/making-python-on-apache-hadoop-easier-with-anaconda-and-cdh/"&gt;Cloudera blog post&lt;/a&gt;. &lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Both options have drawbacks which are as severe as obvious. Do you really want to let a bunch of data scientists run processes on your cluster and fill up the local hard-drives? The second option is not even a real isolated environment at all since all your applications would use the same libraries and maybe break after an update of a&amp;nbsp;library.   &lt;/p&gt;
&lt;p&gt;Therefore, we need to empower our data scientists developing a predictive application to manage isolated environments with their dependencies themselves. This was also recognized as a problem and several issues (&lt;a href="https://issues.apache.org/jira/browse/SPARK-13587"&gt;&lt;span class="caps"&gt;SPARK&lt;/span&gt;-13587&lt;/a&gt; &lt;span class="amp"&gt;&amp;amp;&lt;/span&gt; &lt;a href="https://issues.apache.org/jira/browse/SPARK-16367"&gt;&lt;span class="caps"&gt;SPARK&lt;/span&gt;-16367&lt;/a&gt;) suggest solutions, but none of them have been integrated yet. The most mature solution is actually &lt;a href="https://github.com/nteract/coffee_boat"&gt;coffee boat&lt;/a&gt;, which is still in beta and not meant for production. Therefore, we want to present here a simple but viable solution for this problem that we have been using in production for more than a&amp;nbsp;year.&lt;/p&gt;
&lt;p&gt;So how can we distribute Python modules and whole packages on our executors? Luckily, PySpark provides the functions &lt;a href="http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.SparkContext.addFile"&gt;sc.addFile&lt;/a&gt; and &lt;a href="http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.SparkContext.addPyFile"&gt;sc.addPyFile&lt;/a&gt; which allow us to upload files to every node in our cluster, even Python modules and egg files in case of the latter. Unfortunately, there is no way to upload wheel files which are needed for binary Python packages like Numpy, Pandas and so on. As a data scientist you cannot live without&amp;nbsp;those. &lt;/p&gt;
&lt;p&gt;At first sight this looks pretty bad but thanks to the simplicity of the wheel format it&amp;#8217;s not so bad at all. So here is what we do in a nutshell: For a given PySpark application, we will create an isolated environment on &lt;span class="caps"&gt;HDFS&lt;/span&gt; with the help of wheel files. When submitting our PySpark application, we copy the content of our environment to the driver and executors using &lt;a href="http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.SparkContext.addFile"&gt;sc.addFile&lt;/a&gt;. Simple but&amp;nbsp;effective.&lt;/p&gt;
&lt;h2&gt;Generating the&amp;nbsp;environment&lt;/h2&gt;
&lt;p&gt;In order to create our aforementioned environment we start by creating a directory that will contain our isolated environment, e.g. &lt;code&gt;venv&lt;/code&gt;, on our local Linux machine. Then we will populate this directory with the wheel files of all libraries that our PySpark application uses. Since wheel files contain compiled code they are dependent on the exact Python version and platform. 
For us this means we have to make sure that we use the same platform and Python version locally as we gonna use on the Spark cluster. In my case the cluster runs Ubuntu Trusty Linux with Python 3.4. To replicate this locally it&amp;#8217;s best to use a conda&amp;nbsp;environment:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;conda create -n py34 &lt;span class="nv"&gt;python&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;3&lt;/span&gt;.4
&lt;span class="nb"&gt;source&lt;/span&gt; activate py34
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Having activated the conda environment, we just use &lt;code&gt;pip download&lt;/code&gt; to download all the requirements of our PySpark application as wheel files. In case there is no wheel file available, &lt;code&gt;pip&lt;/code&gt; will download a source-based &lt;code&gt;tar.gz&lt;/code&gt; file instead but we can easily generate a wheel from it. To do so, we just unpack the archive, change into the directory and type &lt;code&gt;python setup.py bdist_wheel&lt;/code&gt;. A wheel file should now reside in the &lt;code&gt;dist&lt;/code&gt; subdirectory. At this point one should also be aware that some wheel files come with low-level Linux dependencies that just need to be installed by a sysadmin on every host, e.g. &lt;code&gt;python3-dev&lt;/code&gt; and &lt;code&gt;unixodbc-dev&lt;/code&gt;.   &lt;/p&gt;
&lt;p&gt;Now we copy the wheel files of all our PySpark application&amp;#8217;s dependencies into the &lt;code&gt;venv&lt;/code&gt; directory. After that, we unpack them with &lt;code&gt;unzip&lt;/code&gt; since they are just normal zip files with a strange suffix. Finally, we push everything to &lt;span class="caps"&gt;HDFS&lt;/span&gt;, e.g. &lt;code&gt;/my_venvs/venv&lt;/code&gt;, using &lt;code&gt;hdfs dfs -put ./venv /my_venvs/venv&lt;/code&gt; and make sure that the files are readable by&amp;nbsp;anyone.&lt;/p&gt;
&lt;h2&gt;Bootstrapping the&amp;nbsp;environment&lt;/h2&gt;
&lt;p&gt;When our PySpark application runs the first thing we do is calling &lt;code&gt;sc.addFile&lt;/code&gt; on every file in &lt;code&gt;/my_venvs/venv&lt;/code&gt;. Since this will also set the &lt;code&gt;PYTHONPATH&lt;/code&gt; correctly, importing any library which resides in &lt;code&gt;venv&lt;/code&gt; will just work. If our Python application itself is also nicely structured as a Python package (maybe using &lt;a href="http://pyscaffold.org/"&gt;PyScaffold&lt;/a&gt;) we can also push it to &lt;code&gt;/my_venvs/venv&lt;/code&gt;. This allows us to roll a full-blown PySpark application and nicely separate the boilerplate code that bootstraps our isolated environment from&amp;nbsp;it.&lt;/p&gt;
&lt;p&gt;Let&amp;#8217;s assume our PySpark application is a Python package called &lt;code&gt;my_pyspark_app&lt;/code&gt;. The boilerplate code to bootstrap &lt;code&gt;my_pyspark_app&lt;/code&gt;, i.e. to activate the isolated environment on Spark, will be in the module &lt;code&gt;activate_env.py&lt;/code&gt;. When we submit our Spark job we will specify this module and specify the environment as an argument,&amp;nbsp;e.g.:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nv"&gt;PYSPARK_PYTHON&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;python3.4 /opt/spark/bin/spark-submit --master yarn --deploy-mode cluster &lt;span class="se"&gt;\&lt;/span&gt;
--num-executors &lt;span class="m"&gt;4&lt;/span&gt; --driver-memory 12g --executor-memory 4g --executor-cores &lt;span class="m"&gt;1&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
--files /etc/spark/conf/hive-site.xml --queue default --conf spark.yarn.maxAppAttempts&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
activate_env.py /my_venvs/venv
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Easy and quite flexible! We are even able to change from one environment to another by just passing another &lt;span class="caps"&gt;HDFS&lt;/span&gt; directory. Here is how &lt;code&gt;activate_env.py&lt;/code&gt; which does the actual heavy lifting with &lt;code&gt;sc.addFile&lt;/code&gt; looks&amp;nbsp;like:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
&lt;span class="sd"&gt;Bootstrapping an isolated environment for `my_pyspark_app` on Spark&lt;/span&gt;
&lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;os&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;sys&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;logging&lt;/span&gt;

&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;pyspark.context&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;SparkContext&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;pyspark.sql&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;SparkSession&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;pyspark.sql.functions&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;

&lt;span class="n"&gt;_logger&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;logging&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;getLogger&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="vm"&gt;__name__&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;list_path_names&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;path&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;List files and directories in an HDFS path&lt;/span&gt;

&lt;span class="sd"&gt;    Args:&lt;/span&gt;
&lt;span class="sd"&gt;        path (str): HDFS path to directory&lt;/span&gt;

&lt;span class="sd"&gt;    Returns:&lt;/span&gt;
&lt;span class="sd"&gt;        [str]: list of file/directory names&lt;/span&gt;
&lt;span class="sd"&gt;    &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;sc&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;SparkContext&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;getOrCreate&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="c1"&gt;# low-level access to hdfs driver&lt;/span&gt;
    &lt;span class="n"&gt;hadoop&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_gateway&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;jvm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;org&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;apache&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;hadoop&lt;/span&gt;
    &lt;span class="n"&gt;path&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;hadoop&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fs&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Path&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;path&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;config&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;hadoop&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;conf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Configuration&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

    &lt;span class="n"&gt;status&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;hadoop&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fs&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;FileSystem&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;listStatus&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;path&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;path_status&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;getPath&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;getName&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;path_status&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;status&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;distribute_hdfs_files&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;hdfs_path&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;Distributes recursively a given directory in HDFS to Spark&lt;/span&gt;

&lt;span class="sd"&gt;    Args:&lt;/span&gt;
&lt;span class="sd"&gt;        hdfs_path (str): path to directory&lt;/span&gt;
&lt;span class="sd"&gt;    &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;sc&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;SparkContext&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;getOrCreate&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;path_name&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;list_path_names&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;hdfs_path&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;path&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;os&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;path&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;join&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;hdfs_path&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;path_name&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;_logger&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;info&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Distributing {}...&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;path&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
        &lt;span class="n"&gt;sc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;addFile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;path&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;recursive&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;main&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;args&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;Main entry point allowing external calls&lt;/span&gt;

&lt;span class="sd"&gt;    Args:&lt;/span&gt;
&lt;span class="sd"&gt;      args ([str]): command line parameter list&lt;/span&gt;
&lt;span class="sd"&gt;    &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="c1"&gt;# setup logging for driver&lt;/span&gt;
    &lt;span class="n"&gt;logging&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;basicConfig&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;level&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;logging&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DEBUG&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;stream&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;sys&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;stdout&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;_logger&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;logging&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;getLogger&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="vm"&gt;__name__&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;_logger&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;info&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Starting up...&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="c1"&gt;# Create the singleton instance&lt;/span&gt;
    &lt;span class="n"&gt;spark&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;SparkSession&lt;/span&gt;
             &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;builder&lt;/span&gt;
             &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;appName&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;My PySpark App in its own environment&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
             &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;enableHiveSupport&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
             &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;getOrCreate&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;

    &lt;span class="c1"&gt;# For simplicity we assume that the first argument is the environment on HDFS&lt;/span&gt;
    &lt;span class="n"&gt;VENV_DIR&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;args&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="c1"&gt;# make sure we have the latest version available on HDFS&lt;/span&gt;
    &lt;span class="n"&gt;distribute_hdfs_files&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;hdfs://&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;VENV_DIR&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;my_pyspark_app&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;main&lt;/span&gt;
    &lt;span class="n"&gt;main&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;args&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:])&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;run&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;Entry point for console_scripts&lt;/span&gt;
&lt;span class="sd"&gt;    &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;main&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sys&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argv&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:])&lt;/span&gt;


&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="vm"&gt;__name__&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;__main__&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;It is actually easier than it looks. In the &lt;code&gt;main&lt;/code&gt; function we initialize the &lt;code&gt;SparkSession&lt;/code&gt; the first time so that later calls to the session builder will use this instance. Thereafter, the passed path argument when doing the &lt;code&gt;spark-submit&lt;/code&gt; is extracted. Subsequently, this is passed to &lt;code&gt;distribute_hdfs_files&lt;/code&gt; which calls &lt;code&gt;sc.addFile&lt;/code&gt; recursively on every file to set up the isolated environment on the driver and executors. After this we are able to import our &lt;code&gt;my_pyspark_app&lt;/code&gt; package and call for instance its &lt;code&gt;main&lt;/code&gt; method. The following graphic illustrates the whole&amp;nbsp;concept: &lt;/p&gt;
&lt;figure&gt;
&lt;p align="center"&gt;
&lt;img class="noZoom" src="/images/pyspark_venv.png" alt="Isolated environment with PySpark"&gt;
&lt;figcaption&gt;&lt;strong&gt;Figure:&lt;/strong&gt; Executing &lt;em&gt;spark-submit&lt;/em&gt; uploads our &lt;em&gt;activate_env.py&lt;/em&gt; module and starts a Spark driver process. Thereafter, &lt;em&gt;activate_env.py&lt;/em&gt; is executed within the driver and bootstraps our &lt;em&gt;venv&lt;/em&gt; environment on the Spark driver as well as on the executors. Finally, &lt;em&gt;activate_env.py&lt;/em&gt; relinquishes control to &lt;em&gt;my_pyspark_app&lt;/em&gt;.&lt;/figcaption&gt;
&lt;/p&gt;
&lt;/figure&gt;

&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Setting up an isolated environment like this is a bit cumbersome and surely also somewhat hacky. Still, in our use-case it served us quite well and allowed the data scientists to set up their specific environments without access to the cluster&amp;#8217;s nodes. Since the explained method also works with &lt;a href="http://jupyter.org/"&gt;Jupyter&lt;/a&gt; this is not only useful for production but also for proof-of-concepts. That being said, we still hope that soon there will be an official solution by the Spark project&amp;nbsp;itself.&lt;/p&gt;</content><category term="spark"></category><category term="python"></category><category term="production"></category></entry><entry><title>Data Science in Production: Packaging, Versioning and Continuous Integration</title><link href="https://florianwilhelm.info/2018/01/ds_in_prod_packaging_ci/" rel="alternate"></link><published>2018-01-08T12:00:00+01:00</published><updated>2018-01-08T12:00:00+01:00</updated><author><name>Florian Wilhelm</name></author><id>tag:florianwilhelm.info,2018-01-08:/2018/01/ds_in_prod_packaging_ci/</id><summary type="html">&lt;p&gt;A common pattern in most data science projects I participated in is that it&amp;#8217;s all fun and games until someone wants to put it into production. All of a sudden the crucial question is how to deploy your model, which version, how can updates be rolled out, which requirements are needed and&amp;nbsp;&amp;#8230;&lt;/p&gt;</summary><content type="html">&lt;h2&gt;Motivation&lt;/h2&gt;
&lt;p&gt;A common pattern in most data science projects I participated in is that it&amp;#8217;s all 
fun and games until someone wants to put it into production. From that point in time on
no one will any longer give you a pat on the back for a high accuracy and smart
algorithm. All of a sudden the crucial question is how to deploy your model,
which version, how can updates be rolled out, which requirements are needed and so&amp;nbsp;on.&lt;/p&gt;
&lt;p&gt;The worst case in such a moment is to realize that up until now the glorious proof of concept
model is not an application but rather a stew of Python/R scripts which were deployed 
by cloning a git repo and run by some Jenkins jobs with a dash of&amp;nbsp;Bash.&lt;/p&gt;
&lt;p&gt;Bringing data science to production is a hot topic right now and there are many facets 
to it. This is the first in a series of posts about &lt;em&gt;data science in production&lt;/em&gt; and
focuses on aspects of modern software engineering like &lt;em&gt;packaging&lt;/em&gt;, &lt;em&gt;versioning&lt;/em&gt; as
well as &lt;em&gt;Continuous Integration&lt;/em&gt; in&amp;nbsp;general.&lt;/p&gt;
&lt;h2&gt;Packages vs.&amp;nbsp;Scripts&lt;/h2&gt;
&lt;p&gt;Being a data scientist does not free you from proper software engineering. Of course
most models start with a simple script or a Jupyter notebook maybe, just the essence
of your idea to test it quickly. But as your model evolves, the number of lines
of code grow, it&amp;#8217;s always a good idea to think about the structure of your code and to
move away from writing simple scripts to proper applications or&amp;nbsp;libraries. &lt;/p&gt;
&lt;p&gt;In case of a Python model, that means grouping functionality into different modules 
&lt;a href="https://en.wikipedia.org/wiki/Separation_of_concerns"&gt;separating different concerns&lt;/a&gt; which could be organised in Python packages on a higher
level. Maybe certain parts of the model are even so general that they could be packaged 
into an own library for greater reusability also for other projects. In the context
of Python, a bundle of software to be installed like a library or application is denoted 
with the term &lt;em&gt;package&lt;/em&gt;. Another synonym is &lt;em&gt;distribution&lt;/em&gt; which is easily to be confused with
a Linux distribution. Therefore the term package is more commonly used although there is an
ambiguity with the kind of package you import in your Python source code (i.e. a container of&amp;nbsp;modules).&lt;/p&gt;
&lt;p&gt;So what is now the key difference between a bunch of Python scripts with some modules 
and a proper package? A Python package adheres a certain structure and thus can be shipped and 
installed by others. Simple as it sounds this is a major advantage over having just some Python 
modules inside a repository. With a package it is possible
to make distinct code releases with different versions that can be stored for later reference. 
Dependencies like &lt;em&gt;numpy&lt;/em&gt; and &lt;em&gt;scikit-learn&lt;/em&gt; can be specified and dependency resolution is automated
by tools like &lt;a href="https://pip.pypa.io/"&gt;pip&lt;/a&gt; and &lt;a href="https://conda.io/"&gt;conda&lt;/a&gt;. Why is this so important? When bugs in production occur 
it&amp;#8217;s incredibly useful to know which state of your code actually is in production. Is it still
version 0.9 or already 1.0? Did the bug also occur in the last release? Most debugging starts
with reproducing the bug locally on your machine. But what if the release is already half a 
year old and there where major changes in its requirements? Maybe the bug is caused by one of
its dependencies? If your package also includes its dependencies with pinned versions, 
restoring the exact same state as in production but inside a local &lt;a href="https://virtualenv.pypa.io/"&gt;virtualenv&lt;/a&gt; or &lt;a href="https://conda.io/"&gt;conda&lt;/a&gt; 
environment will be a matter of&amp;nbsp;seconds.&lt;/p&gt;
&lt;h2&gt;Packaging and&amp;nbsp;Versioning&lt;/h2&gt;
&lt;p&gt;Python&amp;#8217;s history of packaging has had its dark times but nowadays things have pretty much settled 
and now there is only one obvious tool left to do it, namely &lt;a href="https://setuptools.readthedocs.io/"&gt;setuptools&lt;/a&gt;. 
An official Python &lt;a href="https://packaging.python.org/tutorials/distributing-packages/"&gt;packaging tutorial&lt;/a&gt; and many user articles like &lt;a href="http://veekaybee.github.io/2017/09/26/python-packaging/"&gt;Alice in Python projectland&lt;/a&gt; 
explain the various steps needed to set up a proper &lt;code&gt;setup.py&lt;/code&gt;
but it takes a long time to really master the subtleties of Python packaging and even then it
is quite cumbersome. This is the reason many developers refrain from building Python packages.
Another reason is that even if you have a correct Python package set up, proper versioning is
still a manual and thus error-prone process. Therefore the tool &lt;a href="https://github.com/pypa/setuptools_scm"&gt;setuptools_scm&lt;/a&gt; exists which
draws the current version automatically from git so a new release is as simple as creating a new tag.
Following the famous Unix principle &amp;#8220;Do one thing and do it well&amp;#8221; also a Python package is
composed of many specialised tools. Besides &lt;a href="https://setuptools.readthedocs.io/"&gt;setuptools&lt;/a&gt; and &lt;a href="https://github.com/pypa/setuptools_scm"&gt;setuptools_scm&lt;/a&gt; there 
is &lt;a href="http://www.sphinx-doc.org/"&gt;sphinx&lt;/a&gt; for documentation, testing tools like &lt;a href="https://docs.pytest.org/"&gt;pytest&lt;/a&gt; and &lt;a href="https://tox.readthedocs.io/"&gt;tox&lt;/a&gt; as well as many other
little helpers to consider when setting up a Python package. Already scared off of Python packaging?
Hold your breath, there is no reason to&amp;nbsp;be.&lt;/p&gt;
&lt;h3&gt;PyScaffold&lt;/h3&gt;
&lt;p&gt;Luckily there is one tool to rule them all, &lt;a href="http://pyscaffold.org/"&gt;PyScaffold&lt;/a&gt;, which provides a proper Python 
package within a second. It is installed easily&amp;nbsp;with&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;pip install pyscaffold
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;or &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;conda install -c conda-forge pyscaffold
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;if you prefer &lt;a href="https://conda.io/"&gt;conda&lt;/a&gt; over &lt;a href="https://pip.pypa.io/"&gt;pip&lt;/a&gt;. Generating now a project &lt;code&gt;Scikit-AI&lt;/code&gt; with a package &lt;code&gt;skai&lt;/code&gt; is just 
a matter of typing a single&amp;nbsp;command:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;putup Scikit-AI -p skai
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This will create a git repository &lt;code&gt;Scikit-AI&lt;/code&gt; including a fully configured &lt;code&gt;setup.py&lt;/code&gt; that can be configured easily
and in a descriptive way by modifying &lt;code&gt;setup.cfg&lt;/code&gt;. The typical Python package structure is provided including
subfolders such as &lt;code&gt;docs&lt;/code&gt; for &lt;a href="http://www.sphinx-doc.org/"&gt;sphinx&lt;/a&gt; documentation, &lt;code&gt;tests&lt;/code&gt; for unit testing as well as a &lt;code&gt;src&lt;/code&gt;
subfolder including the actual Python package &lt;code&gt;skai&lt;/code&gt;. Also &lt;a href="https://github.com/pypa/setuptools_scm"&gt;setuptools_scm&lt;/a&gt; is integrated
and other features can be activates optionally like support for &lt;a href="https://travis-ci.org/"&gt;Travis&lt;/a&gt;, &lt;a href="https://gitlab.com/"&gt;Gitlab&lt;/a&gt;, &lt;a href="https://tox.readthedocs.io/"&gt;tox&lt;/a&gt;, &lt;a href="http://pre-commit.com/"&gt;pre-commit&lt;/a&gt;
and many&amp;nbsp;more.&lt;/p&gt;
&lt;p&gt;An example of a more advanced usage of PyScaffold&amp;nbsp;is&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;putup Scikit-AI -p skai --travis --tox -d &lt;span class="s2"&gt;&amp;quot;Scientific AI library with a twist&amp;quot;&lt;/span&gt; -u &lt;span class="s2"&gt;&amp;quot;http://sky.net/&amp;quot;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;where also example configuration files for Travis and tox will be created. The additionally provided short description
with the flag &lt;code&gt;-d&lt;/code&gt; is used where appropriate as is the url passed by &lt;code&gt;-u&lt;/code&gt;. As usual with shell commands,
&lt;code&gt;putup --help&lt;/code&gt; provides information about the various&amp;nbsp;arguments.&lt;/p&gt;
&lt;h3&gt;Versioning&lt;/h3&gt;
&lt;p&gt;Having a proper Python package already gives us the possibility to ship something that can be installed by others
easily including its dependencies of course. But if you want to move fast also the deployment of your new model
package needs to be as much automated as possible. You want to make sure that bug fixes end up in production
automatically while new features need to be manually&amp;nbsp;approved. &lt;/p&gt;
&lt;p&gt;For this reason &lt;a href="https://semver.org/"&gt;Semantic Versioning&lt;/a&gt; was developed which basically says that a version number is composed of
&lt;span class="caps"&gt;MAJOR&lt;/span&gt;.&lt;span class="caps"&gt;MINOR&lt;/span&gt;.&lt;span class="caps"&gt;PATCH&lt;/span&gt; and you increment&amp;nbsp;the:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;span class="caps"&gt;MAJOR&lt;/span&gt; version when you make incompatible &lt;span class="caps"&gt;API&lt;/span&gt;&amp;nbsp;changes,&lt;/li&gt;
&lt;li&gt;&lt;span class="caps"&gt;MINOR&lt;/span&gt; version when you add functionality in a backwards-compatible manner,&amp;nbsp;and&lt;/li&gt;
&lt;li&gt;&lt;span class="caps"&gt;PATCH&lt;/span&gt; version when you make backwards-compatible bug&amp;nbsp;fixes.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This programming language independent concept also made its way into Python&amp;#8217;s official version identification &lt;a href="https://www.python.org/dev/peps/pep-0440/"&gt;&lt;span class="caps"&gt;PEP440&lt;/span&gt;&lt;/a&gt;.
Besides &lt;span class="caps"&gt;MAJOR&lt;/span&gt;, &lt;span class="caps"&gt;MINOR&lt;/span&gt; and &lt;span class="caps"&gt;PATCH&lt;/span&gt; the version number is also extended by semantics identifying development, post and pre 
releases. A package that was set up with PyScaffold uses the information from git to generate a &lt;a href="https://www.python.org/dev/peps/pep-0440/"&gt;&lt;span class="caps"&gt;PEP440&lt;/span&gt;&lt;/a&gt; compatible,
semantic  version identifier. A developer just needs to follow the conventions of &lt;a href="https://semver.org/"&gt;Semantic Versioning&lt;/a&gt; when tagging a
release with&amp;nbsp;git. &lt;/p&gt;
&lt;p&gt;Versioning becomes even more important when your company develops many interdependent packages. The effort of sticking
to the simple conventions of &lt;a href="https://semver.org/"&gt;Semantic Versioning&lt;/a&gt; right from the start is just a small price to pay compared to 
the myriad of pains in the &lt;a href="https://en.wikipedia.org/wiki/Dependency_hell"&gt;dependency hell&lt;/a&gt; you will otherwise end up in long-term. Believe me on that&amp;nbsp;one.&lt;/p&gt;
&lt;h2&gt;Continuous&amp;nbsp;Integration&lt;/h2&gt;
&lt;p&gt;Now that we know about packaging and versioning the next step is to establish an automated Continuous Integration (&lt;span class="caps"&gt;CI&lt;/span&gt;)
process. For this purpose a common choice is &lt;a href="https://jenkins-ci.org/"&gt;Jenkins&lt;/a&gt; especially for proprietary software since it can be installed
on&amp;nbsp;premise. &lt;/p&gt;
&lt;h3&gt;Artefact&amp;nbsp;Store&lt;/h3&gt;
&lt;p&gt;Besides the &lt;span class="caps"&gt;CI&lt;/span&gt; tool there is also a place needed to store the built packages. The term &lt;em&gt;artefact store&lt;/em&gt; is
used commonly for a service that offers a way to store and install packages from. In the Python world the 
Python Package Index (&lt;a href="https://pypi.python.org"&gt;PyPI&lt;/a&gt;) is the official artefact store to publish open source packages. For companies the
on-premise equivalent is &lt;a href="https://devpi.net/"&gt;devpi&lt;/a&gt;&amp;nbsp;that:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;acts as a PyPI&amp;nbsp;mirror, &lt;/li&gt;
&lt;li&gt;allows uploading, testing and staging with private&amp;nbsp;indexes,&lt;/li&gt;
&lt;li&gt;has a nice web interface for&amp;nbsp;searching,&lt;/li&gt;
&lt;li&gt;allows uploading and browsing the Sphinx documentation of&amp;nbsp;packages,&lt;/li&gt;
&lt;li&gt;has user management&amp;nbsp;and&lt;/li&gt;
&lt;li&gt;features Jenkins&amp;nbsp;integration.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If all you care about is Python then devpi is the right artefact store for you. In most companies also Java is used
and &lt;a href="http://www.sonatype.org/nexus/"&gt;Nexus&lt;/a&gt; often serves thereby already as artefact store. In this case it might be more advantageous to use Nexus also for
storing Python packages which is available since version 3.0 to avoid the complexity of maintaining another&amp;nbsp;service.&lt;/p&gt;
&lt;p&gt;In highly polylingual environments with many languages like Python, R, Java and C/C++ this will lead to many different
artefact stores and various different ways of installing artefacts. A unified approach is provided by &lt;a href="https://conda.io/"&gt;conda&lt;/a&gt; since
conda packages can be built for &lt;a href="https://conda.io/docs/user-guide/tutorials/build-postgis.html"&gt;general code projects&lt;/a&gt;. The on-premise artefact store provided by &lt;a href="https://anaconda.org/"&gt;Anaconda&lt;/a&gt; is
called &lt;a href="https://docs.anaconda.com/anaconda-repository/"&gt;anaconda-repository&lt;/a&gt; and is part of the proprietary enterprise server. Whenever a unified approach to storing and
installing artefacts of different languages is a major concern, &lt;a href="https://anaconda.org/"&gt;Anaconda&lt;/a&gt; might be a viable&amp;nbsp;solution.&lt;/p&gt;
&lt;h3&gt;Indices and&amp;nbsp;Channels&lt;/h3&gt;
&lt;p&gt;Common to all artifact stores is the availability of different &lt;em&gt;indices&lt;/em&gt; (or &lt;em&gt;channels&lt;/em&gt; in conda) to organize artefacts. 
It is a good practice to have different indices to describe the maturity of the contained packages like &lt;em&gt;unstable&lt;/em&gt;,
&lt;em&gt;testing&lt;/em&gt; and &lt;em&gt;stable&lt;/em&gt;. This complements the automatic &lt;a href="https://www.python.org/dev/peps/pep-0440/"&gt;&lt;span class="caps"&gt;PEP440&lt;/span&gt;&lt;/a&gt; versioning with &lt;a href="http://pyscaffold.org/"&gt;PyScaffold&lt;/a&gt; since it allows us to 
tell a development version which passed the unit tests (&lt;em&gt;testing&lt;/em&gt;) from a development version which did not (&lt;em&gt;unstable&lt;/em&gt;).&lt;br&gt;
Since &lt;a href="https://pip.pypa.io/"&gt;pip&lt;/a&gt; by default installs only stable releases, e.g. &lt;code&gt;1.0&lt;/code&gt; but not &lt;code&gt;1.0b3&lt;/code&gt;, while the &lt;code&gt;--pre&lt;/code&gt; flag 
is needed to install unstable releases the differentiation between &lt;em&gt;testing&lt;/em&gt; and &lt;em&gt;stable&lt;/em&gt; indices is not absolutely 
necessary. Still for organisational reasons, having an &lt;em&gt;testing&lt;/em&gt; index as input for &lt;span class="caps"&gt;QA&lt;/span&gt; and a &lt;em&gt;stable&lt;/em&gt; index that really
only holds releases that passed the whole &lt;span class="caps"&gt;QA&lt;/span&gt; process is a good idea. Also &lt;a href="https://conda.io/"&gt;conda&lt;/a&gt; does not seem to provide an equivalent
to the &lt;code&gt;--pre&lt;/code&gt; flag and thus different channels need to be&amp;nbsp;used.&lt;/p&gt;
&lt;p&gt;One should also note that git allows to tag a single commit several times which will lead to different versions of the
Python package having the same content. This gives means to the following convention: Let&amp;#8217;s say there was a bug in version
&lt;code&gt;1.2&lt;/code&gt; and after two commits the bug seems to be fixed. The automatically inferred version number by PyScaffold
will be &lt;code&gt;1.2.post0.pre2-gHASH&lt;/code&gt;. Being happy with her fix the developer tags the commit with &lt;code&gt;1.2.1rc1&lt;/code&gt; (first release
candidate of version 1.2.1). Since all unit tests pass this patch will end up in the &lt;em&gt;testing&lt;/em&gt; index where &lt;span class="caps"&gt;QA&lt;/span&gt; can put it to the
acid test. After that, the same commit will be tagged and signed by &lt;span class="caps"&gt;QA&lt;/span&gt; with name &lt;code&gt;1.2.1&lt;/code&gt; which results in a new package
that can be moved to the &lt;em&gt;stable&lt;/em&gt; index&amp;nbsp;automatically.&lt;/p&gt;
&lt;h3&gt;Automated &lt;span class="caps"&gt;CI&lt;/span&gt;&amp;nbsp;Process&lt;/h3&gt;
&lt;p&gt;With this components in mind we can establish an automated &lt;span class="caps"&gt;CI&lt;/span&gt; process. Upon a new commit on a central git repository 
the &lt;em&gt;packaging&lt;/em&gt; Jenkins job clones the repo and builds the package, e.g. with &lt;code&gt;python setup.py bdist_wheel&lt;/code&gt;. If this is
successful the package is uploaded to the &lt;em&gt;unstable&lt;/em&gt; index of the artefact store. Upon the successful completion of the
packaging job a second Jenkins job for &lt;em&gt;testing&lt;/em&gt; is triggered. The reason for packaging and publishing before running
any kind of unit tests is that already during the packaging can be major flaws that a typical unit test could never
find. For instance, missing data files that are in the repo but not specified in the package, missing or wrong
dependencies and so on. Therefore it is important to run unit tests always against the package installed in a clean
environment and that is exactly what the testing job does. After having set up a fresh environment with &lt;a href="https://virtualenv.pypa.io/"&gt;virtualenv&lt;/a&gt;
or &lt;a href="https://conda.io/"&gt;conda&lt;/a&gt; the just published package is installed from the artefact store. 
If this succeeds the git repo is cloned into a subfolder providing
the unit tests (in the &lt;code&gt;tests&lt;/code&gt; subfolder). These unit tests are then executed and check the installed package. In case
that all tests pass the package is moved from the &lt;em&gt;unstable&lt;/em&gt; index to the &lt;em&gt;testing&lt;/em&gt; index. In case the commit was
tagged as a stable release and thus the package&amp;#8217;s version is stable according to &lt;a href="https://www.python.org/dev/peps/pep-0440/"&gt;&lt;span class="caps"&gt;PEP440&lt;/span&gt;&lt;/a&gt; it is moved into the
 &lt;em&gt;stable&lt;/em&gt; index. Figure 1 illustrates the complete&amp;nbsp;process.&lt;/p&gt;
&lt;figure&gt;
&lt;p align="center"&gt;
&lt;img class="noZoom" src="/images/ci_build_publish.png" alt="Building and publishing a package"&gt;
&lt;figcaption&gt;&lt;strong&gt;Figure 1:&lt;/strong&gt; The &lt;em&gt;packaging&lt;/em&gt; job clones source code repository, builds the software package and pushes
it into the &lt;em&gt;unstable&lt;/em&gt; index of the artefact store. If these steps succeed the &lt;em&gt;testing&lt;/em&gt; job
is triggered which installs the package from the artefact store and its dependencies into a clean environment.
The source code reposistory is then cloned in order to run the unit tests against the installed package. If all 
unit tests pass the package is moved into the &lt;em&gt;testing&lt;/em&gt; index of the artefact store or optionally
to the &lt;em&gt;stable&lt;/em&gt; index if the version is a stable release.&lt;/figcaption&gt;
&lt;/p&gt;
&lt;/figure&gt;

&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;It is clear that packaging, versioning and &lt;span class="caps"&gt;CI&lt;/span&gt; are just one aspect of how to bring Data Science in production
and follow-up posts will shed some light on other aspects.
Whereas these aspects are quite important, their benefits are often underestimated. We have seen that proper packaging is
crucial to shipping, installing a package and dealing with its dependencies. Semantic Versioning supports us in automation
of rolling out patches and in the organisation of deployment. The advantages of Continuous Integration are quite obvious
and promoted a lot by the DevOps culture in recent years. Also Data Science can learn and benefit from this spirit and
we have seen that a minimal &lt;span class="caps"&gt;CI&lt;/span&gt; setup is easy to accomplish. All together they build a fundamental corner stone of
Data Science in production. Bringing data science to production plays a crucial part in many projects at &lt;a href="https://www.inovex.de/en/"&gt;inovex&lt;/a&gt;
since the added value of data science only shows in&amp;nbsp;production.&lt;/p&gt;
&lt;p&gt;Some good talks around this topic were held by &lt;a href="https://www.linkedin.com/in/sebastian-neubauer-16626a79/"&gt;Sebastian Neubauer&lt;/a&gt;, one of the acclaimed
DevOps rock stars of Python in production. His talks &lt;a href="https://www.youtube.com/watch?v=Ad9qSbrfnvk"&gt;A Pythonic Approach to &lt;span class="caps"&gt;CI&lt;/span&gt;&lt;/a&gt; and 
&lt;a href="https://www.youtube.com/watch?v=hnQKsxKjCUo"&gt;There should be one obvious way to bring Python into production&lt;/a&gt; perfectly complement this post and are even fun 
to&amp;nbsp;watch.&lt;/p&gt;</content><category term="python"></category><category term="data science"></category><category term="production"></category></entry><entry><title>Efficient UD(A)Fs with PySpark</title><link href="https://florianwilhelm.info/2017/10/efficient_udfs_with_pyspark/" rel="alternate"></link><published>2017-10-11T12:30:00+02:00</published><updated>2018-03-08T12:30:00+01:00</updated><author><name>Florian Wilhelm</name></author><id>tag:florianwilhelm.info,2017-10-11:/2017/10/efficient_udfs_with_pyspark/</id><summary type="html">&lt;p&gt;Nowadays, Spark surely is one of the most prevalent technologies in the fields of data science and big data. Luckily, even though it is developed in Scala and runs in the Java Virtual Machine (&lt;span class="caps"&gt;JVM&lt;/span&gt;), it comes with Python bindings also known as &lt;a href="https://spark.apache.org/docs/latest/api/python/index.html"&gt;PySpark&lt;/a&gt;, whose &lt;span class="caps"&gt;API&lt;/span&gt; was heavily influenced by …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Nowadays, Spark surely is one of the most prevalent technologies in the fields of data science and big data. Luckily, even though it is developed in Scala and runs in the Java Virtual Machine (&lt;span class="caps"&gt;JVM&lt;/span&gt;), it comes with Python bindings also known as &lt;a href="https://spark.apache.org/docs/latest/api/python/index.html"&gt;PySpark&lt;/a&gt;, whose &lt;span class="caps"&gt;API&lt;/span&gt; was heavily influenced by &lt;a href="http://pandas.pydata.org/"&gt;Pandas&lt;/a&gt;.
With respect to functionality, modern PySpark has about the same capabilities as Pandas when it comes to typical &lt;span class="caps"&gt;ETL&lt;/span&gt; and data wrangling, e.g. groupby, aggregations and so on.
As a general rule of thumb, one should consider an alternative to Pandas whenever the data set has more than 10,000,000 rows which, depending on the number of columns and data types, translates to about 5-10 &lt;span class="caps"&gt;GB&lt;/span&gt; of memory usage. At that point PySpark might be an option for you that does the job, but of course there are others like for instance &lt;a href="http://dask.pydata.org/en/latest/index.html"&gt;Dask&lt;/a&gt; which won&amp;#8217;t be addressed in this&amp;nbsp;post. &lt;/p&gt;
&lt;p&gt;If you are new to Spark, one important thing to note is that Spark has two remarkable features besides its programmatic data wrangling capabilities. One is that Spark comes with &lt;span class="caps"&gt;SQL&lt;/span&gt; as an alternative way of defining queries and the other is &lt;a href="https://spark.apache.org/docs/latest/ml-guide.html"&gt;Spark MLlib&lt;/a&gt; for machine learning. Both topics are beyond the scope of this post but should be taken into account if you are considering PySpark as an alternative to Pandas and scikit-learn for larger data&amp;nbsp;sets.&lt;/p&gt;
&lt;p&gt;But enough praise for PySpark, there are still some ugly sides as well as rough edges to it and we want to address some of them here, of course, in a constructive way.
First of all, due to its relatively young age, PySpark lacks some features that Pandas provides, for example in areas such as reshaping/pivoting or time series.
Also, it is not as straightforward to use advanced mathematical functions from SciPy within PySpark.
That&amp;#8217;s why sooner or later, you might walk into a scenario where you want to apply some Pandas or SciPy operations to your data frame in PySpark.
Unfortunately, there is no built-in mechanism for using Pandas transformations in PySpark.
In fact, this requires a lot of boilerplate code with many error-prone details to consider.
Therefore we make a wish to the coding fairy, cross two fingers that someone else already solved this and start googling&amp;#8230; and here we are&amp;nbsp;;-)&lt;/p&gt;
&lt;p&gt;The remainder of this blog post walks you through the process of writing efficient Pandas UDAFs in PySpark. In fact, we end up abstracting all the necessary boilerplate code into a single Python decorator, which allows us to conveniently specify our PySpark Pandas function.
To give more insights into performance considerations, this post also contains a little journey into the internals of&amp;nbsp;PySpark.&lt;/p&gt;
&lt;h2&gt;UDAFs with&amp;nbsp;RDDs&lt;/h2&gt;
&lt;p&gt;To start with a recap, an aggregation function is a function that operates on a set of rows and produces a result, for example a &lt;code&gt;sum()&lt;/code&gt; or &lt;code&gt;count()&lt;/code&gt; function.
A &lt;em&gt;User-Defined Aggregation Function&lt;/em&gt; (&lt;span class="caps"&gt;UDAF&lt;/span&gt;) is typically used for more complex aggregations that are not natively shipped with your analysis tool in question.
In our case, this means we provide some Python code that takes a set of rows and produces an aggregate result.
At the time of writing - with PySpark 2.2 as latest version - there is no &amp;#8220;official&amp;#8221; way of defining an arbitrary &lt;span class="caps"&gt;UDAF&lt;/span&gt; function.
Also, the tracking Jira issue &lt;a href="https://issues.apache.org/jira/browse/SPARK-10915"&gt;&lt;span class="caps"&gt;SPARK&lt;/span&gt;-10915&lt;/a&gt; does not indicate that this changes in near future.
Depending on your use-case, this might even be a reason to completely discard PySpark as a viable solution.
However, as you might have guessed from the title of this article, there are workarounds to the rescue.
&lt;!-- langsamer weg den wir probiert hatten: groupby() + collect_list() + udf die liste an events in pandas DF lädt ... --&gt;
This is where the &lt;a href="https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD"&gt;&lt;span class="caps"&gt;RDD&lt;/span&gt;&lt;/a&gt; &lt;span class="caps"&gt;API&lt;/span&gt; comes in.
As a reminder, a &lt;em&gt;Resilient Distributed Dataset&lt;/em&gt; (&lt;span class="caps"&gt;RDD&lt;/span&gt;) is the low-level data structure of Spark and a Spark &lt;a href="https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame"&gt;DataFrame&lt;/a&gt; is built on top of it. As we are mostly dealing with DataFrames in PySpark, we can get access to the underlying &lt;span class="caps"&gt;RDD&lt;/span&gt; with the help of the &lt;code&gt;rdd&lt;/code&gt; attribute and convert it back with &lt;code&gt;toDF()&lt;/code&gt;.
This &lt;span class="caps"&gt;RDD&lt;/span&gt; &lt;span class="caps"&gt;API&lt;/span&gt; allows us to specify arbitrary Python functions that get executed on the data.
To give an example, let&amp;#8217;s say we have a DataFrame &lt;code&gt;df&lt;/code&gt; of one billion rows with a boolean &lt;code&gt;is_sold&lt;/code&gt; column and we want to filter for rows with sold products. One could accomplish this with the&amp;nbsp;code&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rdd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;filter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;is_sold&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;toDF&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Although not explicitly declared as such, this lambda function is essentially a user-defined function (&lt;span class="caps"&gt;UDF&lt;/span&gt;).
For this exact use case, we could also use the more high-level DataFrame &lt;code&gt;filter()&lt;/code&gt; method, producing the same&amp;nbsp;result:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;filter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;is_sold&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Before we now go into the details on how to implement UDAFs using the &lt;span class="caps"&gt;RDD&lt;/span&gt; &lt;span class="caps"&gt;API&lt;/span&gt;, there is something important to keep in mind which might sound counterintuitive to the title of this post: in PySpark you should &lt;em&gt;avoid&lt;/em&gt; all kind of Python UDFs - like &lt;span class="caps"&gt;RDD&lt;/span&gt; functions or data frame UDFs - as much as possible!
Whenever there is a built-in DataFrame method available, this will be much faster than its &lt;span class="caps"&gt;RDD&lt;/span&gt; counterpart. 
To get a better understanding of the substantial performance difference, we will now take a little detour and investigate what happens behind the scenes in those two filter&amp;nbsp;examples.&lt;/p&gt;
&lt;!---
PySpark Internals
===========================

Communication between Python and Spark happens on different levels:
 1. *PySpark Remote Controlling Spark*: Local communication between the PySpark driver and the Java SparkContext
 2. *Python UDFs*: Data transfer between the data frames in JVM and the Python workers executing the UDF
 3. Data transfer between the distributed data frames in JVM memory and the Python driver PySpark actions and data frame creation from python (e.g.:("PySpark toDF(), c 

Local communication acts like a JVM remote control from Python. 
--&gt;

&lt;h2&gt;PySpark&amp;nbsp;internals&lt;/h2&gt;
&lt;p&gt;PySpark is actually a wrapper around the Spark core written in Scala. 
When you start your &lt;a href="http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.SparkSession"&gt;SparkSession&lt;/a&gt; in Python, in the background PySpark uses &lt;a href="https://www.py4j.org/"&gt;Py4J&lt;/a&gt; to launch a &lt;span class="caps"&gt;JVM&lt;/span&gt; and create a Java SparkContext. 
All PySpark operations, for example our &lt;code&gt;df.filter()&lt;/code&gt; method call, behind the scenes get translated into corresponding calls on the respective Spark DataFrame object within the &lt;span class="caps"&gt;JVM&lt;/span&gt; SparkContext. This is in general extremely fast and the overhead can be neglected as long as you don&amp;#8217;t call the function millions of times.
So in our &lt;code&gt;df.filter()&lt;/code&gt; example, the DataFrame operation and the filter condition will be send to the Java SparkContext, where it gets compiled into an overall optimized query plan.
Once the query is executed, the filter condition is evaluated on the distributed DataFrame within Java, without any callback to Python!
In case our workflow loads the DataFrame from Hive and saves the resulting DataFrame as Hive table, throughout the entire query execution all data operations are performed in a distributed fashion within Java Spark workers, which allows Spark to be very fast for queries on large data sets. 
Okay, so why is the &lt;span class="caps"&gt;RDD&lt;/span&gt; &lt;code&gt;filter()&lt;/code&gt; method then so much slower?
The reason is that the lambda function cannot be directly applied to the DataFrame residing in &lt;span class="caps"&gt;JVM&lt;/span&gt; memory. 
&lt;!--- 
To get a better understanding of the huge performance difference, we need to look more closely at the previously mentioned second point of data transfer between the JVM and Python.
--&gt;
What actually happens internally is that Spark spins up Python workers next to the Spark executors on the cluster nodes.
At execution time, the Spark workers send our lambda function to those Python workers.
Next, the Spark workers start serializing their &lt;span class="caps"&gt;RDD&lt;/span&gt; partitions and pipe them to the Python workers via sockets, where our lambda function gets evaluated on each row.
For the resulting rows, the whole serialization/deserialization procedure happens again in the opposite direction so that
the actual &lt;code&gt;filter()&lt;/code&gt; can be applied to the result&amp;nbsp;set.&lt;/p&gt;
&lt;p&gt;The entire data flow when using arbitrary Python functions in PySpark is also shown in the following image, which has been taken from the old &lt;a href="https://cwiki.apache.org/confluence/display/SPARK/PySpark+Internals"&gt;PySpark Internals&lt;/a&gt;&amp;nbsp;wiki:&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/pyspark_udf_dataflow.png"/&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Even if all of this sounded awkwardly technical to you, you get the point that executing Python functions in a distributed Java system is very expensive in terms of execution time due to excessive copying of data back and&amp;nbsp;forth.&lt;/p&gt;
&lt;p&gt;To give a short summary to this low-level excursion: as long as we avoid all kind of Python UDFs, a PySpark program will be approximately as fast as Spark program based on Scala.
If we cannot avoid UDFs, we should at least try to make them as efficient as possible, which is what we show in the remaining post. Before we move on though, one side note should be kept in mind. The general problem of accessing data frames from different programming languages in the realm of data analytics is currently addressed by the creator of Pandas &lt;a href="http://wesmckinney.com/"&gt;Wes McKinney&lt;/a&gt;. He is also the initiator of the &lt;a href="http://arrow.apache.org/"&gt;Apache Arrow&lt;/a&gt; project which tries to standardize the way columnar data is stored in memory so that everyone using Arrow won&amp;#8217;t need to do the cumbersome object translation by serialization and deserialization anymore. Hopefully with version 2.3, as shown in the issues &lt;a href="https://issues.apache.org/jira/browse/SPARK-13534"&gt;&lt;span class="caps"&gt;SPARK&lt;/span&gt;-13534&lt;/a&gt; and &lt;a href="https://issues.apache.org/jira/browse/SPARK-21190"&gt;&lt;span class="caps"&gt;SPARK&lt;/span&gt;-21190&lt;/a&gt;, Spark will make use of Arrow, which should drastically speed up our Python UDFs. Still, even in that case we should always prefer built-in Spark functions whenever&amp;nbsp;possible.&lt;/p&gt;
&lt;h1&gt;PySpark UDAFs with&amp;nbsp;Pandas&lt;/h1&gt;
&lt;p&gt;As mentioned before our detour into the internals of PySpark, for defining an arbitrary &lt;span class="caps"&gt;UDAF&lt;/span&gt; function we need an operation that allows us to operate on multiple rows and produce one or multiple resulting rows.
This functionality is provided by the &lt;a href="https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD"&gt;&lt;span class="caps"&gt;RDD&lt;/span&gt;&lt;/a&gt; method &lt;code&gt;mapPartitions&lt;/code&gt;, where we can apply an arbitrary Python function &lt;code&gt;my_func&lt;/code&gt; to a DataFrame &lt;code&gt;df&lt;/code&gt; partition&amp;nbsp;with:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rdd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mapPartitions&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;my_func&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;toDF&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;If you want to further read up on RDDs and partitions, you can checkout the chapter &lt;a href="https://jaceklaskowski.gitbooks.io/mastering-apache-spark/spark-rdd-partitions.html"&gt;Partitions and Partitioning&lt;/a&gt; of the excellent &lt;em&gt;Mastering Apache Spark 2&lt;/em&gt; book by Jacek Laskowski.
In most cases we would want to control the number of partitions, like 100, or even group by a column, let&amp;#8217;s say &lt;code&gt;country&lt;/code&gt;, in which case we would&amp;nbsp;write:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;repartition&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rdd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mapPartitions&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;my_func&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;toDF&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;or&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;repartition&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;country&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rdd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mapPartitions&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;my_func&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;toDF&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Having solved one problem, as it is quite often in life, we have introduced another problem. As we are working now with the low-level &lt;span class="caps"&gt;RDD&lt;/span&gt; interface, our function &lt;code&gt;my_func&lt;/code&gt; will be passed an iterator of PySpark &lt;a href="https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.Row"&gt;Row&lt;/a&gt; objects and needs to return them as well. A &lt;code&gt;Row&lt;/code&gt; object itself is only a container for the column values in one row, as you might have guessed. When we return such a &lt;code&gt;Row&lt;/code&gt;, the data types of these values therein must be interpretable by Spark in order to translate them back to Scala. This is a lot of low-level stuff to deal with since in most cases we would love to implement our &lt;span class="caps"&gt;UDF&lt;/span&gt;/&lt;span class="caps"&gt;UDAF&lt;/span&gt; with the help of Pandas, keeping in mind that one partition should hold less than 10 million&amp;nbsp;rows.&lt;/p&gt;
&lt;p&gt;So first we need to define a nice function that will convert a &lt;code&gt;Row&lt;/code&gt; iterator into a Pandas&amp;nbsp;DataFrame:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;logging&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pandas&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pd&lt;/span&gt;


&lt;span class="n"&gt;_logger&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;logging&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;getLogger&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="vm"&gt;__name__&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;rows_to_pandas&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;rows&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;Converts a Spark Row iterator of a partition to a Pandas DataFrame assuming YARN&lt;/span&gt;

&lt;span class="sd"&gt;    Args:&lt;/span&gt;
&lt;span class="sd"&gt;        rows: iterator over PySpark Row objects&lt;/span&gt;

&lt;span class="sd"&gt;    Returns:&lt;/span&gt;
&lt;span class="sd"&gt;        Pandas DataFrame&lt;/span&gt;
&lt;span class="sd"&gt;    &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;first_row&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;rows&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;peek&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;rows&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="n"&gt;first_row&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;_logger&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;warning&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Spark DataFrame is empty! Returning empty Pandas DataFrame!&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataFrame&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

    &lt;span class="n"&gt;first_row_info&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;{} ({}): {}&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;rtype&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;first_row&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt; &lt;span class="n"&gt;first_row&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
                      &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;first_row&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;__fields__&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;_logger&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;debug&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;First partition row: {}&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;first_row_info&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataFrame&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;from_records&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;rows&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;columns&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;first_row&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;__fields__&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;_logger&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;debug&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Converted partition to DataFrame of shape {} with types:&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s2"&gt;{}&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dtypes&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This function actually does only one thing which is calling &lt;code&gt;df = pd.DataFrame.from_records(rows, columns=first_row.__fields__)&lt;/code&gt; in order to generate a DataFrame. The rest of the code makes sure that the iterator is not empty and for debugging reasons we also peek into the first row and print the value as well as the datatype of each column. This has proven in practice to be extremely helpful in case something goes wrong and one needs to debug what&amp;#8217;s going on in the &lt;span class="caps"&gt;UDF&lt;/span&gt;/&lt;span class="caps"&gt;UDAF&lt;/span&gt;. The functions &lt;code&gt;peek&lt;/code&gt; and &lt;code&gt;rtype&lt;/code&gt; are defined as&amp;nbsp;follows:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;itertools&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;chain&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;peek&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;iterable&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;Peek into the first element and return the whole iterator again&lt;/span&gt;

&lt;span class="sd"&gt;    Args:&lt;/span&gt;
&lt;span class="sd"&gt;        iterable: iterable object like list or iterator&lt;/span&gt;

&lt;span class="sd"&gt;    Returns:&lt;/span&gt;
&lt;span class="sd"&gt;        tuple of first element and original iterable&lt;/span&gt;
&lt;span class="sd"&gt;    &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;iterable&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;iter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;iterable&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;try&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;first_elem&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;next&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;iterable&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;except&lt;/span&gt; &lt;span class="ne"&gt;StopIteration&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;iterable&lt;/span&gt;
    &lt;span class="n"&gt;iterable&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;chain&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;first_elem&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;iterable&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;first_elem&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;iterable&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;rtype&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;var&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;Heuristic representation for nested types/containers&lt;/span&gt;

&lt;span class="sd"&gt;    Args:&lt;/span&gt;
&lt;span class="sd"&gt;        var: some (nested) variable&lt;/span&gt;

&lt;span class="sd"&gt;    Returns:&lt;/span&gt;
&lt;span class="sd"&gt;        str: string representation of nested datatype (NA=Not Available)&lt;/span&gt;
&lt;span class="sd"&gt;    &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;etype&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="nb"&gt;type&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="vm"&gt;__name__&lt;/span&gt;

    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="nb"&gt;isinstance&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;var&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;elem_type&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;etype&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;var&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;var&lt;/span&gt; &lt;span class="k"&gt;else&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;NA&amp;quot;&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;List[{}]&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;elem_type&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;elif&lt;/span&gt; &lt;span class="nb"&gt;isinstance&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;var&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;dict&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;keys&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;var&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;keys&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;keys&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;key&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;keys&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
            &lt;span class="n"&gt;key_type&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;val_type&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;etype&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;key&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;etype&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;var&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;key&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
        &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;key_type&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;val_type&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;NA&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;NA&amp;quot;&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Dict[{}, {}]&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;key_type&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;val_type&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;elif&lt;/span&gt; &lt;span class="nb"&gt;isinstance&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;var&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;tuple&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;elem_types&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;, &amp;#39;&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;join&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;etype&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;elem&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;elem&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;var&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Tuple[{}]&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;elem_types&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;etype&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;var&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The next part is to actually convert the result of our &lt;span class="caps"&gt;UDF&lt;/span&gt;/&lt;span class="caps"&gt;UDAF&lt;/span&gt; back to an iterator of Row objects. Since our result will most likely be a Pandas DataFrame or Series, we define the&amp;nbsp;following:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;pyspark.sql.types&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Row&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;convert_dtypes&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;rows&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;Converts some Pandas data types to pure Python data types&lt;/span&gt;

&lt;span class="sd"&gt;    Args:&lt;/span&gt;
&lt;span class="sd"&gt;        rows (array): numpy recarray holding all rows&lt;/span&gt;

&lt;span class="sd"&gt;    Returns:&lt;/span&gt;
&lt;span class="sd"&gt;        Iterator over lists of row values&lt;/span&gt;
&lt;span class="sd"&gt;    &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;dtype_map&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Timestamp&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;to_pydatetime&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt;
                 &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;datetime64&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Timestamp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;to_pydatetime&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt;
                 &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;bool_&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;bool&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
                 &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;int8&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
                 &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;int16&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
                 &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;int32&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
                 &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;int64&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
                 &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float16&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
                 &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float32&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
                 &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float64&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
                 &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float128&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)}&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;row&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;rows&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;yield&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;dtype_map&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;type&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;elem&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)(&lt;/span&gt;&lt;span class="n"&gt;elem&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;elem&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;row&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;pandas_to_rows&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;Converts Pandas DataFrame to iterator of Row objects&lt;/span&gt;

&lt;span class="sd"&gt;    Args:&lt;/span&gt;
&lt;span class="sd"&gt;        df: Pandas DataFrame&lt;/span&gt;

&lt;span class="sd"&gt;    Returns:&lt;/span&gt;
&lt;span class="sd"&gt;        Iterator over PySpark Row objects&lt;/span&gt;
&lt;span class="sd"&gt;    &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt; &lt;span class="ow"&gt;is&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;_logger&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;debug&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Returning nothing&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="nb"&gt;iter&lt;/span&gt;&lt;span class="p"&gt;([])&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="nb"&gt;type&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="ow"&gt;is&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Series&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;to_frame&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;empty&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;_logger&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;warning&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Pandas DataFrame is empty! Returning nothing!&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="nb"&gt;iter&lt;/span&gt;&lt;span class="p"&gt;([])&lt;/span&gt;
    &lt;span class="n"&gt;_logger&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;debug&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Convert DataFrame of shape {} to partition with types:&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s2"&gt;{}&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dtypes&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;records&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;to_records&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;False&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;records&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;convert_dtypes&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;records&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;first_row&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;records&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;peek&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;records&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;first_row_info&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;{} ({}): {}&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;rtype&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;v&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;v&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;v&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;zip&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;columns&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;first_row&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;
    &lt;span class="n"&gt;_logger&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;debug&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;First record row: {}&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;first_row_info&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;row&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Row&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;columns&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;row&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;elems&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;elems&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;records&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This looks a bit more complicated but essentially we convert a Pandas Series to a DataFrame if necessary and handle the edge cases of an empty DataFrame or &lt;code&gt;None&lt;/code&gt; as return value. We then convert the DataFrame to records, convert some NumPy data types to the Python equivalent and create an iterator over Row objects from the converted&amp;nbsp;records. &lt;/p&gt;
&lt;p&gt;With these functions at hand we can define a &lt;a href="https://wiki.python.org/moin/PythonDecorators#What_is_a_Decorator"&gt;Python decorator&lt;/a&gt; that will allow us to automatically call the functions &lt;code&gt;rows_to_pandas&lt;/code&gt; and &lt;code&gt;pandas_to_rows&lt;/code&gt; at the right&amp;nbsp;time:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;functools&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;wraps&lt;/span&gt;


&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;pandas_udaf&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;object&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;Decorator for PySpark UDAFs using Pandas&lt;/span&gt;

&lt;span class="sd"&gt;    Args:&lt;/span&gt;
&lt;span class="sd"&gt;        loglevel (int): minimum loglevel for emitting messages&lt;/span&gt;
&lt;span class="sd"&gt;    &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;loglevel&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;logging&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;INFO&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;loglevel&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;loglevel&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__call__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;func&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="nd"&gt;@wraps&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;func&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;wrapper&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;args&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="c1"&gt;# use *args to allow decorating methods (incl. self arg)&lt;/span&gt;
            &lt;span class="n"&gt;args&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;args&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;setup_logger&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;loglevel&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;loglevel&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;args&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;rows_to_pandas&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;args&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
            &lt;span class="n"&gt;df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;func&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;args&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;pandas_to_rows&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;wrapper&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The code is pretty much self-explanatory if you have ever written a Python decorator; otherwise, you should read about it since it takes some time to wrap your head around it. Basically, we set up a default logger, create a Pandas DataFrame from the Row iterator, pass it to our &lt;span class="caps"&gt;UDF&lt;/span&gt;/&lt;span class="caps"&gt;UDAF&lt;/span&gt; and convert its return value back to a Row iterator. The only additional thing that might still raise questions is the usage of &lt;code&gt;args[-1]&lt;/code&gt;. This is due to the fact that &lt;code&gt;func&lt;/code&gt; might also be a method of an object. In this case, the first argument would be &lt;code&gt;self&lt;/code&gt; but the last argument is in either cases the actual argument that &lt;code&gt;mapPartitions&lt;/code&gt; will pass to us. The code of &lt;code&gt;setup_logger&lt;/code&gt; depends on your Spark installation. In case you are using Spark on Apache &lt;a href="https://hortonworks.com/apache/yarn/"&gt;&lt;span class="caps"&gt;YARN&lt;/span&gt;&lt;/a&gt;, it might look like&amp;nbsp;this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;os&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;sys&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;setup_logger&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;loglevel&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;logging&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;INFO&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;logfile&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;pyspark.log&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;Setup basic logging for logging on the executor&lt;/span&gt;

&lt;span class="sd"&gt;    Args:&lt;/span&gt;
&lt;span class="sd"&gt;        loglevel (int): minimum loglevel for emitting messages&lt;/span&gt;
&lt;span class="sd"&gt;        logfile (str): name of the logfile&lt;/span&gt;
&lt;span class="sd"&gt;    &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;logformat&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="si"&gt;%(asctime)s&lt;/span&gt;&lt;span class="s2"&gt; &lt;/span&gt;&lt;span class="si"&gt;%(levelname)s&lt;/span&gt;&lt;span class="s2"&gt; &lt;/span&gt;&lt;span class="si"&gt;%(module)s&lt;/span&gt;&lt;span class="s2"&gt;.&lt;/span&gt;&lt;span class="si"&gt;%(funcName)s&lt;/span&gt;&lt;span class="s2"&gt;: &lt;/span&gt;&lt;span class="si"&gt;%(message)s&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;datefmt&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;%y/%m/&lt;/span&gt;&lt;span class="si"&gt;%d&lt;/span&gt;&lt;span class="s2"&gt; %H:%M:%S&amp;quot;&lt;/span&gt;
    &lt;span class="k"&gt;try&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;logfile&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;os&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;path&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;join&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;os&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;environ&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;LOG_DIRS&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;split&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;,&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;logfile&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;except&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="ne"&gt;KeyError&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="ne"&gt;IndexError&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;logging&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;basicConfig&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;level&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;loglevel&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                            &lt;span class="n"&gt;stream&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;sys&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;stdout&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; 
                            &lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;logformat&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                            &lt;span class="n"&gt;datefmt&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;datefmt&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;logger&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;logging&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;getLogger&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="vm"&gt;__name__&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;logger&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;error&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;LOG_DIRS is not in environment variables or empty, using STDOUT instead.&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;logging&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;basicConfig&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;level&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;loglevel&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                        &lt;span class="n"&gt;filename&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;logfile&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                        &lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;logformat&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                        &lt;span class="n"&gt;datefmt&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;datefmt&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Now having all parts in place let&amp;#8217;s assume the code above resides in the python module &lt;a href="https://florianwilhelm.info/src/pyspark_udaf.py"&gt;pyspark_udaf.py&lt;/a&gt;. A future post will cover the topic of deploying dependencies in a systematic way for production requirements. For now we just presume that &lt;a href="https://florianwilhelm.info/src/pyspark_udaf.py"&gt;pyspark_udaf.py&lt;/a&gt; as well as all its dependencies like Pandas, NumPy, etc. are accessible by the Spark driver as well as the executors. This allows us to then easily define an example &lt;span class="caps"&gt;UDAF&lt;/span&gt; &lt;code&gt;my_func&lt;/code&gt; that collects some basic statistics for each country&amp;nbsp;as:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pyspark_udaf&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;logging&lt;/span&gt;


&lt;span class="nd"&gt;@pyspark_udaf.pandas_udaf&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;loglevel&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;logging&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DEBUG&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;my_func&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;empty&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt;
    &lt;span class="n"&gt;df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;groupby&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;country&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;apply&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;drop&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;country&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;describe&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reset_index&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;It is of course not really useful in practice to return some statistics with the help of a &lt;span class="caps"&gt;UDAF&lt;/span&gt; that could also be retrieved with basic PySpark functionality but this is just an example. We now generate a dummy data DataFrame and apply the function to each partition as above&amp;nbsp;with:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# make pyspark_udaf.py available to the executors&lt;/span&gt;
&lt;span class="n"&gt;spark&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sparkContext&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;addFile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;./pyspark_udaf.py&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;spark&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;createDataFrame&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;DEU&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;DEU&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;8.0&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;FRA&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;6.0&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
            &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;FRA&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;8.0&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;DEU&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;8.0&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;FRA&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;3.0&lt;/span&gt;&lt;span class="p"&gt;)],&lt;/span&gt;
    &lt;span class="n"&gt;schema&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;country&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;feature1&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;feature2&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

&lt;span class="n"&gt;stats_df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;repartition&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;country&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rdd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mapPartitions&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;my_func&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;toDF&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;stats_df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;toPandas&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The code above can be easily tested with the help of a Jupyter notebook with PySpark where the &lt;a href="http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.SparkSession"&gt;SparkSession&lt;/a&gt; &lt;code&gt;spark&lt;/code&gt; is&amp;nbsp;predefined.&lt;/p&gt;
&lt;h2&gt;Summary&lt;/h2&gt;
&lt;p&gt;Overall, this proposed method allows the definition of an &lt;span class="caps"&gt;UDF&lt;/span&gt; as well as an &lt;span class="caps"&gt;UDAF&lt;/span&gt; since it is up to the function &lt;code&gt;my_func&lt;/code&gt; if it returns (1) a DataFrame having as many rows as the input DataFrame (think &lt;a href="https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.transform.html"&gt;Pandas transform&lt;/a&gt;), (2) a DataFrame of only a single row or (3) optionally a Series (think &lt;a href="https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.aggregate.html"&gt;Pandas aggregate&lt;/a&gt;) or a DataFrame with an arbitrary number of rows (think &lt;a href="https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.apply.html"&gt;Pandas apply&lt;/a&gt;) with even varying columns.
Therefore, this approach should be applicable to a variety of use cases where the built-in PySpark functionality is not&amp;nbsp;sufficient.&lt;/p&gt;
&lt;p&gt;To wrap it up, this blog post gives you a template on how to write PySpark &lt;span class="caps"&gt;UD&lt;/span&gt;(A)Fs while abstracting all the boilerplate in a dedicated module.
We also went down the rabbit hole to explore the technical difficulties the Spark developers face in providing Python bindings to a distributed &lt;span class="caps"&gt;JVM&lt;/span&gt;-based system.
In this respect we are really looking forward to closer integration of &lt;a href="http://arrow.apache.org/"&gt;Apache Arrow&lt;/a&gt; and Spark in the upcoming Spark 2.3 and future&amp;nbsp;versions.&lt;/p&gt;
&lt;p&gt;This article was coauthored by my inovex colleague &lt;em&gt;Bernhard Schäfer&lt;/em&gt; and was also published on the &lt;a href="https://www.inovex.de/blog/"&gt;inovex blog&lt;/a&gt;.&lt;/p&gt;</content><category term="spark"></category><category term="python"></category><category term="big data"></category></entry><entry><title>Hive UDFs and UDAFs with Python</title><link href="https://florianwilhelm.info/2016/10/python_udf_in_hive/" rel="alternate"></link><published>2016-10-23T11:00:00+02:00</published><updated>2017-04-26T11:00:00+02:00</updated><author><name>Florian Wilhelm</name></author><id>tag:florianwilhelm.info,2016-10-23:/2016/10/python_udf_in_hive/</id><summary type="html">&lt;p&gt;Sometimes the analytical power of &lt;a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF"&gt;built-in Hive functions&lt;/a&gt; is just not enough.
In this case it is possible to write hand-tailored User-Defined Functions (UDFs)
for transformations and even aggregations which are therefore called User-Defined
Aggregation Functions (UDAFs). In this post we focus on how to write sophisticated
UDFs and UDAFs …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Sometimes the analytical power of &lt;a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF"&gt;built-in Hive functions&lt;/a&gt; is just not enough.
In this case it is possible to write hand-tailored User-Defined Functions (UDFs)
for transformations and even aggregations which are therefore called User-Defined
Aggregation Functions (UDAFs). In this post we focus on how to write sophisticated
UDFs and UDAFs in Python. By sophisticated we mean that our &lt;span class="caps"&gt;UD&lt;/span&gt;(A)Fs should
also be able to leverage external libraries like Numpy, Scipy, Pandas etc.
This makes things a lot more complicated since we have to provide not only some
Python script but also a full-blown virtual environment including the external
libraries since they may not be available on the cluster nodes.
Therefore, in this tutorial we require only that a basic installation of Python
is available on the data nodes of the Hive&amp;nbsp;cluster.&lt;/p&gt;
&lt;h2&gt;General&amp;nbsp;information&lt;/h2&gt;
&lt;p&gt;To keep the idea behind &lt;span class="caps"&gt;UD&lt;/span&gt;(A)Fs short, only some general notes are mentioned here.
With the help of the &lt;a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Transform"&gt;Transform/Map-Reduce syntax&lt;/a&gt;, i.e. &lt;code&gt;TRANSFORM&lt;/code&gt;, it is
possible to plug in your own custom mappers and reducers. This is where we gonna hook
in our Python script. A &lt;span class="caps"&gt;UDF&lt;/span&gt; is basically only a transformation done by a mapper
meaning that each row should be mapped to exactly one row. A &lt;span class="caps"&gt;UDAF&lt;/span&gt; on the
other hand allows us to transform a group of rows into one or more rows, meaning that we
can reduce the number of input rows to a single output row by some custom
aggregation. We can control if the script is run in a mapper or reducer step
by the way we formulate our HiveQL query. The statements &lt;code&gt;DISTRIBUTE BY&lt;/code&gt; and
&lt;code&gt;CLUSTER BY&lt;/code&gt; allow us to indicate that we want to actually perform an aggregation.
HiveQL feeds the data to the Python script or any other custom script by using
the standard input and reads the result from its standard out. All messages from
standard error are ignored and can therefore be used for debugging.
Since a &lt;span class="caps"&gt;UDAF&lt;/span&gt; is more complex than a &lt;span class="caps"&gt;UDF&lt;/span&gt; and actually can be seen as a generalization
of it, the development of a &lt;span class="caps"&gt;UDAF&lt;/span&gt; is demonstrated&amp;nbsp;here.   &lt;/p&gt;
&lt;h2&gt;Overview and a little&amp;nbsp;task&lt;/h2&gt;
&lt;p&gt;In order to not get lost in the details, here is what we want to achieve from
a high-level&amp;nbsp;perspective.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Set up small example Hive table within some&amp;nbsp;database.&lt;/li&gt;
&lt;li&gt;Create a virtual environment and upload it to Hive&amp;#8217;s distributed&amp;nbsp;cache.&lt;/li&gt;
&lt;li&gt;Write the actual &lt;span class="caps"&gt;UDAF&lt;/span&gt; as Python script and a little helper shell&amp;nbsp;script.&lt;/li&gt;
&lt;li&gt;Write a HiveQL query that feeds our example table into the Python&amp;nbsp;script.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Our dummy data consists of different types of vehicles (car or bike) and a price. For
each category we want to calculate the mean and the standard deviation with the help
of Pandas to keep things simple. It should not be necessary to mention that this
task can be handled in HiveQL directly, so this is really only for&amp;nbsp;demonstration.&lt;/p&gt;
&lt;h2&gt;1. Setting up our dummy&amp;nbsp;table&lt;/h2&gt;
&lt;p&gt;With the following query we generate our sample&amp;nbsp;data:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;CREATE&lt;/span&gt; &lt;span class="k"&gt;DATABASE&lt;/span&gt; &lt;span class="n"&gt;tmp&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="n"&gt;USE&lt;/span&gt; &lt;span class="n"&gt;tmp&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="k"&gt;CREATE&lt;/span&gt; &lt;span class="k"&gt;TABLE&lt;/span&gt; &lt;span class="n"&gt;foo&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;id&lt;/span&gt; &lt;span class="nb"&gt;INT&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;vtype&lt;/span&gt; &lt;span class="n"&gt;STRING&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;price&lt;/span&gt; &lt;span class="nb"&gt;FLOAT&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;span class="k"&gt;INSERT&lt;/span&gt; &lt;span class="k"&gt;INTO&lt;/span&gt; &lt;span class="k"&gt;TABLE&lt;/span&gt; &lt;span class="n"&gt;foo&lt;/span&gt; &lt;span class="k"&gt;VALUES&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="ss"&gt;&amp;quot;car&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1000&lt;/span&gt;&lt;span class="p"&gt;.);&lt;/span&gt;
&lt;span class="k"&gt;INSERT&lt;/span&gt; &lt;span class="k"&gt;INTO&lt;/span&gt; &lt;span class="k"&gt;TABLE&lt;/span&gt; &lt;span class="n"&gt;foo&lt;/span&gt; &lt;span class="k"&gt;VALUES&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="ss"&gt;&amp;quot;car&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;42&lt;/span&gt;&lt;span class="p"&gt;.);&lt;/span&gt;
&lt;span class="k"&gt;INSERT&lt;/span&gt; &lt;span class="k"&gt;INTO&lt;/span&gt; &lt;span class="k"&gt;TABLE&lt;/span&gt; &lt;span class="n"&gt;foo&lt;/span&gt; &lt;span class="k"&gt;VALUES&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="ss"&gt;&amp;quot;car&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10000&lt;/span&gt;&lt;span class="p"&gt;.);&lt;/span&gt;
&lt;span class="k"&gt;INSERT&lt;/span&gt; &lt;span class="k"&gt;INTO&lt;/span&gt; &lt;span class="k"&gt;TABLE&lt;/span&gt; &lt;span class="n"&gt;foo&lt;/span&gt; &lt;span class="k"&gt;VALUES&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="ss"&gt;&amp;quot;car&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;69&lt;/span&gt;&lt;span class="p"&gt;.);&lt;/span&gt;
&lt;span class="k"&gt;INSERT&lt;/span&gt; &lt;span class="k"&gt;INTO&lt;/span&gt; &lt;span class="k"&gt;TABLE&lt;/span&gt; &lt;span class="n"&gt;foo&lt;/span&gt; &lt;span class="k"&gt;VALUES&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="ss"&gt;&amp;quot;bike&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1426&lt;/span&gt;&lt;span class="p"&gt;.);&lt;/span&gt;
&lt;span class="k"&gt;INSERT&lt;/span&gt; &lt;span class="k"&gt;INTO&lt;/span&gt; &lt;span class="k"&gt;TABLE&lt;/span&gt; &lt;span class="n"&gt;foo&lt;/span&gt; &lt;span class="k"&gt;VALUES&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="ss"&gt;&amp;quot;bike&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;32&lt;/span&gt;&lt;span class="p"&gt;.);&lt;/span&gt;
&lt;span class="k"&gt;INSERT&lt;/span&gt; &lt;span class="k"&gt;INTO&lt;/span&gt; &lt;span class="k"&gt;TABLE&lt;/span&gt; &lt;span class="n"&gt;foo&lt;/span&gt; &lt;span class="k"&gt;VALUES&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="ss"&gt;&amp;quot;bike&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1234&lt;/span&gt;&lt;span class="p"&gt;.);&lt;/span&gt;
&lt;span class="k"&gt;INSERT&lt;/span&gt; &lt;span class="k"&gt;INTO&lt;/span&gt; &lt;span class="k"&gt;TABLE&lt;/span&gt; &lt;span class="n"&gt;foo&lt;/span&gt; &lt;span class="k"&gt;VALUES&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="ss"&gt;&amp;quot;bike&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="k"&gt;null&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Note that the last row even contains a null value that we need to handle&amp;nbsp;later.&lt;/p&gt;
&lt;h2&gt;2. Creating and uploading a virtual&amp;nbsp;environment&lt;/h2&gt;
&lt;p&gt;In order to prepare a proper virtual environment we need to execute the following
steps on an &lt;span class="caps"&gt;OS&lt;/span&gt; that is binary compatible to the &lt;span class="caps"&gt;OS&lt;/span&gt; on the Hive cluster. Typically
any recent 64bit Linux distribution will&amp;nbsp;do.&lt;/p&gt;
&lt;p&gt;We start by creating an empty virtual environment&amp;nbsp;with:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;virtualenv &amp;#8212;no-site-packages -p /usr/bin/python3&amp;nbsp;venv&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;assuming that &lt;code&gt;virtualenv&lt;/code&gt; was already installed with the help of pip. Note that
we explicitly ask for Python 3. Who uses Python 2 these days&amp;nbsp;anyhow?&lt;/p&gt;
&lt;p&gt;The problem with the &lt;code&gt;activate&lt;/code&gt; script of a virtual environment is that 
its path is hard-coded. We change that by replacing the&amp;nbsp;line &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nv"&gt;VIRTUAL_ENV&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;your/path/to/venv&amp;quot;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;with&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nv"&gt;HERE&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="k"&gt;$(&lt;/span&gt; &lt;span class="nb"&gt;cd&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="k"&gt;$(&lt;/span&gt; dirname &lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="si"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;BASH_SOURCE&lt;/span&gt;&lt;span class="p"&gt;[0]&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt; &lt;span class="k"&gt;)&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="nb"&gt;pwd&lt;/span&gt; &lt;span class="k"&gt;)&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;
&lt;span class="nv"&gt;VIRTUAL_ENV&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="k"&gt;$(&lt;/span&gt; readlink -f &lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="si"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;HERE&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;/../&amp;quot;&lt;/span&gt; &lt;span class="k"&gt;)&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;in &lt;code&gt;./venv/bin/activate&lt;/code&gt;. Additionally, we replace in &lt;code&gt;pip&lt;/code&gt; the shebang line, i.e.&amp;nbsp;replacing&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="ch"&gt;#!/your/path/to/venv/venv/bin/python3&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;with&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="ch"&gt;#!/usr/bin/env python&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This will help us later when we call &lt;code&gt;pip list&lt;/code&gt; for debugging&amp;nbsp;reasons.&lt;/p&gt;
&lt;p&gt;We activate the virtual environment and install Pandas in&amp;nbsp;it.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;source&amp;nbsp;venv/bin/activate&lt;/p&gt;
&lt;p&gt;pip install numpy&amp;nbsp;pandas&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This should install Pandas and all its dependencies into our virtual environment.
No we package the virtual environment for later deployment in the distributed&amp;nbsp;cache:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;cd&amp;nbsp;venv&lt;/p&gt;
&lt;p&gt;tar cvfhz ../venv.tgz&amp;nbsp;./&lt;/p&gt;
&lt;p&gt;cd&amp;nbsp;..&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Be aware that the archive was created with the actual content at its root so
when unpacking there will be no directory holding the actual content. We also
used the parameter &lt;code&gt;h&lt;/code&gt; to package linked&amp;nbsp;files.&lt;/p&gt;
&lt;p&gt;Now we push the archive to &lt;span class="caps"&gt;HDFS&lt;/span&gt; so that later Hive&amp;#8217;s data nodes will be able to
find&amp;nbsp;it:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;hdfs dfs -put venv.tgz&amp;nbsp;/tmp&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The directory &lt;code&gt;/tmp&lt;/code&gt; should be changed accordingly. One should also note that
in principle the same procedure should also be possible with conda environments. In
practice though, it might be a bit more involved since the activation of a conda
environment (what we need to do later) assumes an installation of at least
miniconda which might not be available on the data&amp;nbsp;nodes.&lt;/p&gt;
&lt;h2&gt;3. Writing and uploading the&amp;nbsp;scripts&lt;/h2&gt;
&lt;p&gt;We start by writing a simple Python script &lt;code&gt;udaf.py&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;sys&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;logging&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;itertools&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;groupby&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;operator&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;itemgetter&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pandas&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pd&lt;/span&gt;

&lt;span class="n"&gt;SEP&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="se"&gt;\t&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;
&lt;span class="n"&gt;NULL&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="se"&gt;\\&lt;/span&gt;&lt;span class="s1"&gt;N&amp;#39;&lt;/span&gt;

&lt;span class="n"&gt;_logger&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;logging&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;getLogger&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="vm"&gt;__name__&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;read_input&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;input_data&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;line&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;input_data&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;yield&lt;/span&gt; &lt;span class="n"&gt;line&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;strip&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;split&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;SEP&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;main&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="n"&gt;logging&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;basicConfig&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;level&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;logging&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;INFO&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;stream&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;sys&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;stderr&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;read_input&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sys&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;stdin&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;vtype&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;group&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;groupby&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;itemgetter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)):&lt;/span&gt;
        &lt;span class="n"&gt;_logger&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;info&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Reading group {}...&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;vtype&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
        &lt;span class="n"&gt;group&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[(&lt;/span&gt;&lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;rowid&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;vtype&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nan&lt;/span&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;price&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;NULL&lt;/span&gt; &lt;span class="k"&gt;else&lt;/span&gt; &lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;price&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
                 &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;rowid&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;vtype&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;price&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;group&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="n"&gt;df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataFrame&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;group&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;columns&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;id&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;vtype&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;price&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
        &lt;span class="n"&gt;output&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;vtype&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;price&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;price&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="p"&gt;()]&lt;/span&gt;
        &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;SEP&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;join&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;o&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;o&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;output&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;


&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="vm"&gt;__name__&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;__main__&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;main&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The script should be pretty much self-explanatory. We read from the standard
input with the help of a generator that strips and splits the lines by the
separator &lt;code&gt;\t&lt;/code&gt;. At any point we want to avoid to have more data in memory as
needed to perform the actual computation. We use the &lt;code&gt;groupby&lt;/code&gt; function that
is shipped with Python to iterate over our two types of vehicles. For each group
we convert the read values to their respective data types and at that point
also take care of &lt;code&gt;null&lt;/code&gt; values which are encoded as &lt;code&gt;\N&lt;/code&gt;. After this preprocessing
we finally feed everything into a Pandas dataframe, do our little mean and standard
deviation calculations and print everything as a tabular separated list.
It should also be noted that we set up a logger at the beginning which writes
everything to standard error. This really helps a lot with debugging and should
be used. For demonstration purposes the vehicle type of the group currently
processed is&amp;nbsp;printed.&lt;/p&gt;
&lt;p&gt;At this point we would actually be done if it wasn&amp;#8217;t for the fact that we are
importing external libraries like Pandas. So if we ran this Python script directly
as &lt;span class="caps"&gt;UDAF&lt;/span&gt; we would see import errors if Pandas is not installed on all cluster nodes.
But in the spirit of David Wheeler&amp;#8217;s &amp;#8220;All problems in computer science can be
solved by another level of indirection.&amp;#8221; we just write a little helper script
called &lt;code&gt;udaf.sh&lt;/code&gt; that does this job for us and calls the Python script&amp;nbsp;afterwards.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="ch"&gt;#!/bin/bash&lt;/span&gt;
&lt;span class="nb"&gt;set&lt;/span&gt; -e
&lt;span class="o"&gt;(&lt;/span&gt;&amp;gt;&lt;span class="p"&gt;&amp;amp;&lt;/span&gt;&lt;span class="m"&gt;2&lt;/span&gt; &lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Begin of script&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
&lt;span class="nb"&gt;source&lt;/span&gt; ./venv.tgz/bin/activate
&lt;span class="o"&gt;(&lt;/span&gt;&amp;gt;&lt;span class="p"&gt;&amp;amp;&lt;/span&gt;&lt;span class="m"&gt;2&lt;/span&gt; &lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Activated venv&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;(&lt;/span&gt;&amp;gt;&lt;span class="p"&gt;&amp;amp;&lt;/span&gt;&lt;span class="m"&gt;2&lt;/span&gt; pip list --format&lt;span class="o"&gt;=&lt;/span&gt;columns --no-cache-dir&lt;span class="o"&gt;)&lt;/span&gt;
python udaf.py
&lt;span class="o"&gt;(&lt;/span&gt;&amp;gt;&lt;span class="p"&gt;&amp;amp;&lt;/span&gt;&lt;span class="m"&gt;2&lt;/span&gt; &lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;End of script&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Again we use standard error to trace what the script is currently doing.
Furthermore, we use &lt;code&gt;pip list&lt;/code&gt; to output the content of the virtual environment
for debugging reasons.
With the help of &lt;code&gt;chmod u+x&lt;/code&gt; we make the script executable and now all that&amp;#8217;s
left is to push both files somewhere on &lt;span class="caps"&gt;HDFS&lt;/span&gt; for the cluster to&amp;nbsp;find:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;hdfs dfs -put udaf.py&amp;nbsp;/tmp&lt;/p&gt;
&lt;p&gt;hdfs dfs -put udaf.sh&amp;nbsp;/tmp&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;4. Writing the actual HiveQL&amp;nbsp;query&lt;/h2&gt;
&lt;p&gt;After we are all prepared and set we can write the actual HiveQL&amp;nbsp;query:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;DELETE&lt;/span&gt; &lt;span class="n"&gt;ARCHIVE&lt;/span&gt; &lt;span class="n"&gt;hdfs&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="o"&gt;///&lt;/span&gt;&lt;span class="n"&gt;tmp&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;venv&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;tgz&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="k"&gt;ADD&lt;/span&gt; &lt;span class="n"&gt;ARCHIVE&lt;/span&gt; &lt;span class="n"&gt;hdfs&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="o"&gt;///&lt;/span&gt;&lt;span class="n"&gt;tmp&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;venv&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;tgz&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="k"&gt;DELETE&lt;/span&gt; &lt;span class="n"&gt;FILE&lt;/span&gt; &lt;span class="n"&gt;hdfs&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="o"&gt;///&lt;/span&gt;&lt;span class="n"&gt;tmp&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;udaf&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;py&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="k"&gt;ADD&lt;/span&gt; &lt;span class="n"&gt;FILE&lt;/span&gt; &lt;span class="n"&gt;hdfs&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="o"&gt;///&lt;/span&gt;&lt;span class="n"&gt;tmp&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;udaf&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;py&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="k"&gt;DELETE&lt;/span&gt; &lt;span class="n"&gt;FILE&lt;/span&gt; &lt;span class="n"&gt;hdfs&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="o"&gt;///&lt;/span&gt;&lt;span class="n"&gt;tmp&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;udaf&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sh&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="k"&gt;ADD&lt;/span&gt; &lt;span class="n"&gt;FILE&lt;/span&gt; &lt;span class="n"&gt;hdfs&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="o"&gt;///&lt;/span&gt;&lt;span class="n"&gt;tmp&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;udaf&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sh&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;

&lt;span class="n"&gt;USE&lt;/span&gt; &lt;span class="n"&gt;tmp&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="k"&gt;SELECT&lt;/span&gt; &lt;span class="k"&gt;TRANSFORM&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;id&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;vtype&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;price&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;USING&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;udaf.sh&amp;#39;&lt;/span&gt; &lt;span class="k"&gt;AS&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;vtype&lt;/span&gt; &lt;span class="n"&gt;STRING&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;mean&lt;/span&gt; &lt;span class="nb"&gt;FLOAT&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;var&lt;/span&gt; &lt;span class="nb"&gt;FLOAT&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="k"&gt;FROM&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;SELECT&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="k"&gt;FROM&lt;/span&gt; &lt;span class="n"&gt;foo&lt;/span&gt; &lt;span class="k"&gt;CLUSTER&lt;/span&gt; &lt;span class="k"&gt;BY&lt;/span&gt; &lt;span class="n"&gt;vtype&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;AS&lt;/span&gt; &lt;span class="n"&gt;TEMP_TABLE&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;At first we add the zipped virtual environment to the distributed cache that
will be automatically unpacked for us due to the &lt;code&gt;ADD ARCHIVE&lt;/code&gt; command.
Then we upload the Python and helper script. To make sure the current version
in the cache is actually the latest, so in case changes are made, we
prepended &lt;code&gt;DELETE&lt;/code&gt; statements before each &lt;code&gt;ADD&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The actual query now calls &lt;code&gt;TRANSFORM&lt;/code&gt; with the three input column we expect
in our Python script. After the &lt;code&gt;USING&lt;/code&gt; statement our helper script is provided
as the actual &lt;span class="caps"&gt;UDAF&lt;/span&gt; seen by HiveQL. This is followed by &lt;code&gt;AS&lt;/code&gt; defining the names
and types of the output&amp;nbsp;columns.&lt;/p&gt;
&lt;p&gt;At this point we need to make sure that the script is executed in a reducer step.
We assure this by defining a subselect that reads from our &lt;code&gt;foo&lt;/code&gt; table and clusters
by the &lt;code&gt;vtype&lt;/code&gt;. &lt;code&gt;CLUSTER BY&lt;/code&gt; which is a shortcut for &lt;code&gt;DISTRIBUTE BY&lt;/code&gt; followed by
&lt;code&gt;SORT BY&lt;/code&gt; asserts that rows having the same &lt;code&gt;vtype&lt;/code&gt; column are also located on
the same reducer. Furthermore, the implicit &lt;code&gt;SORT BY&lt;/code&gt; orders within a reducer
the rows with respect to the &lt;code&gt;vtype&lt;/code&gt; column. The overall result are consecutive
partitions of a given vehicle type (car and bike in our case) whereas each partition resides
on a single reducer. Finally, our script is fed the whole data on a single reducer
and needs to figure out itself where one partition ends and another one starts
(what we did with &lt;code&gt;itertools.groupby&lt;/code&gt;).&lt;/p&gt;
&lt;h2&gt;Finally&lt;/h2&gt;
&lt;p&gt;Since our little task is now accomplished, it should also be noted that there
are some more Python libraries one should know when working with Hive.
To actually execute the HiveQL query we have written with the help of Python, there
is &lt;a href="https://github.com/cloudera/impyla"&gt;impyla&lt;/a&gt; by Cloudera with supports Python 3 in contrast to &lt;a href="https://github.com/dropbox/PyHive"&gt;PyHive&lt;/a&gt; by Dropbox.
In order to work with &lt;span class="caps"&gt;HDFS&lt;/span&gt; the best library around is &lt;a href="https://hdfs3.readthedocs.io/"&gt;hdfs3&lt;/a&gt;. That would
for instance allow us to push changes in &lt;code&gt;udaf.py&lt;/code&gt; automatically with a Python&amp;nbsp;script.&lt;/p&gt;
&lt;p&gt;Have fun hacking Hive with the power of&amp;nbsp;Python!&lt;/p&gt;</content><category term="python"></category><category term="hadoop"></category><category term="hive"></category><category term="big data"></category></entry></feed>