{"pages":[{"text":"","title":"404: Page not found","tags":"English","url":"http://www.florianwilhelm.info/404-not-found/"},{"text":"","title":"410: Page is gone","tags":"English","url":"http://www.florianwilhelm.info/410-gone/"},{"text":"","title":"500: Internal server error","tags":"English","url":"http://www.florianwilhelm.info/500-error/"},{"text":"My name is Florian Wilhelm and I am a Data Scientist living in Cologne, Germany. I studied mathematics at the Karlsruhe Institute of Technology ( KIT ) with a focus on numerical mathematics and computer science as a subsidiary subject. After my studies, I worked on Parallel Preconditioners for an Ocean Model in Climate Simulations and graduated from university ( KIT ) with a PhD degree. Main focus of my work was the mathematical modelling of the governing equations of fluid dynamics in order to optimally solve them with the help of high-performance computing ( HPC ) on a large-scale cluster. As Head of the Research Lab for Application and Innovation at the Institute of Applied and Numerical Mathematics, I worked in several industry-funded projects and supervised bachelor and master students. After my postdoctoral position I started as a Data Scientist at Blue Yonder , the leading platform provider for Predictive Applications and Big Data in the European market. Right now I enjoy working on innovative Data Science projects with experts every day at inovex . With more than three years of project experience in the field of Predictive & Prescriptive Analytics and Big Data, I have acquired profound knowledge in the domains of mathematical modelling, statistics, machine learning, high-performance computing and data mining. For the last years I programmed mostly with the Python Data Science stack ( NumPy , SciPy , Scikit-Learn , Pandas , Matplotlib , Jupyter , etc.) to which I also contributed several extensions. Due to some project work I have also gained experience in R as well as C/C++, Java, Matlab and Fortran. If you want to know more, please read all about my professional details in my résumé or on LinkedIn .","title":"About me","tags":"English","url":"http://www.florianwilhelm.info/about/"},{"text":"Sometimes the analytical power of built-in Hive functions is just not enough. In this case it is possible to write hand-tailored User-Defined Functions (UDFs) for transformations and even aggregations which are therefore called User-Defined Aggregation Functions (UDAFs). In this post we focus on how to write sophisticated UDFs and UDAFs in Python. By sophisticated we mean that our UD (A)Fs should also be able to leverage external libraries like Numpy, Scipy, Pandas etc. This makes things a lot more complicated since we have to provide not only some Python script but also a full-blown virtual environment including the external libraries. Therefore, we require only from the actual Hive setup that a basic installation of Python is available on the data nodes. General information To keep the idea behind UD (A)Fs short, only some general notes are mentioned here. With the help of the Transform/Map-Reduce syntax , i.e. TRANSFORM , it is possible to plug in own custom mappers and reducers. This is where we gonna hook in our Python script. An UDF is basically only a transformation done by a mapper meaning that each row should be mapped to exactly one row. A UDAF on the other hand allows us to transform a group of rows into one or more rows so we can reduce the number of input rows to a single output row by some custom aggregation. We can control if the script is run in a mapper or reducer step by the way we formulate our HiveQL query. The statements DISTRIBUTE BY and CLUSTER BY allow us to indicate that we want to actually perform an aggregation. HiveQL feeds its data to the Python script or any other custom script by using the standard input and reads the result from its standard out. All messages from standard error are ignored and can therefore be used for debugging. Since a UDAF is more complex than a UDF and actually can be seen as a generalization of it, the development of an UDAF is demonstrated here. Overview and our little task In order to not get lost in the details, here is what we want to achieve from a high-level perspective. Set up small example Hive table within some database. Create a virtual environment and upload it to Hive's distributed cache. Write the actual UDAF as Python script and a little helper shell script. Write a HiveQL query that feeds our example table into the Python script. Our dummy data consists of different types of vehicles (car or bike) and a price. For each category we want to calculate mean and the standard deviation with the help of Pandas to keep things simple. It should not be necessary to mention that this task can be handled in HiveQL directly, so this is really only for demonstration. 1. Setting up our dummy table With the following query we generate our sample data: CREATE DATABASE tmp ; USE tmp ; CREATE TABLE foo ( id INT , vtype STRING , price FLOAT ); INSERT INTO table foo VALUES ( 1 , \"car\" , 1000 .); INSERT INTO table foo VALUES ( 2 , \"car\" , 42 .); INSERT INTO table foo VALUES ( 3 , \"car\" , 10000 .); INSERT INTO table foo VALUES ( 4 , \"car\" , 69 .); INSERT INTO table foo VALUES ( 5 , \"bike\" , 1426 .); INSERT INTO table foo VALUES ( 6 , \"bike\" , 32 .); INSERT INTO table foo VALUES ( 7 , \"bike\" , 1234 .); INSERT INTO table foo VALUES ( 8 , \"bike\" , null ); Note that the last row even contains a null value we need to handle later. 2. Creating and uploading a virtual environment We start by creating an empty virtual environment with: virtualenv —no-site-packages -p /usr/bin/python3 venv assuming that virtualenv was already installed with the help of pip. Note that we explicitly ask for Python 3. Who uses Python 2 these days anyhow? We activate the virtual environment and install Pandas in it. source venv/bin/activate pip install numpy pandas This should install Pandas and all its dependencies into our virtual environment. No we package the virtual environment for later deployment in the distributed cache: cd venv tar cvfhz ../venv.tgz ./ cd .. Be aware that the archive was created with the actual content at its root so when unpacking there will be no directory holding the actual content. We also used the parameter h to package linked files. Now we push the archive to HDFS so that later Hive's data nodes will be able to find it: hdfs dfs -put venv.tgz /tmp The directory /tmp should be changed accordingly. One should also note that in principle the same procedure should also be possible with conda environments. In practice though, it might be a bit more involved since the activation of a conda environment (what we need to do later) assumes an installation of at least miniconda which might not be available on the data nodes. 3. Writing and uploading the scripts We start by writing a simple Python script udaf.py : import sys import logging from itertools import groupby from operator import itemgetter import numpy as np import pandas as pd SEP = ' \\t ' NULL = ' \\\\ N' _logger = logging . getLogger ( __name__ ) def read_input ( input_data ): for line in input_data : yield line . strip () . split ( SEP ) def main (): logging . basicConfig ( level = logging . INFO , stream = sys . stderr ) data = read_input ( sys . stdin ) for vtype , group in groupby ( data , itemgetter ( 1 )): _logger . info ( \"Reading group {}...\" . format ( vtype )) group = [( int ( rowid ), vtype , np . nan if price == NULL else float ( price )) for rowid , vtype , price in group ] df = pd . DataFrame ( group , columns = ( 'id' , 'vtype' , 'price' )) output = [ vtype , df [ 'price' ] . mean (), df [ 'price' ] . std ()] print ( SEP . join ( str ( o ) for o in output )) if __name__ == '__main__' : main () The script should be pretty much self-explanatory. We read from the standard input with the help of a generator that strips and splits the lines by the separator ‘\\t'. At any point we want to avoid to have more data in memory as needed to perform the actual computation. We use the groupby function that is shipped with Python to iterate over our two types of vehicles. For each group we convert the read values to their respective data types and at that point also take care of null values which are encoded as \\N . After this preprocessing we finally feed everything into a Pandas dataframe, do our little mean and standard deviation calculations and print everything as a tabular separated list. It should also be noted that we set up a logger at the beginning which writes everything to standard error. This really helps a lot with debugging and should be used. For demonstration purposes the vehicle type of the group currently processed is printed. At this point we would actually be done if it wasn't for the fact that we are importing external libraries like Pandas. So if we ran this Python script directly as UDAF we would see import errors if Pandas is not installed on all cluster nodes. But in the spirit of David Wheeler's \"All problems in computer science can be solved by another level of indirection.\" we just write a little helper script called udaf.sh that does this job for us and calls the Python script afterwards. #!/bin/bash set -e ( > & 2 echo \"Begin of script\" ) source ./venv.tgz/bin/activate ( > & 2 echo \"Activated venv\" ) ./venv.tgz/bin/python3 udaf.py ( > & 2 echo \"End of script\" ) With the help of chmod 755 we make sure that it is executable and now all that's left is to push both files somewhere on HDFS for the cluster to find: hdfs dfs -put udaf.py /tmp hdfs dfs -put udaf.sh /tmp 4. Writing the actual HiveQL query After we are all prepared and set we can write the actual HiveQL query: DELETE ARCHIVE hdfs : /// tmp / venv . tgz ; ADD ARCHIVE hdfs : /// tmp / venv . tgz ; DELETE FILE hdfs : /// tmp / udaf . py ; ADD FILE hdfs : /// tmp / udaf . py ; DELETE FILE hdfs : /// tmp / udaf . sh ; ADD FILE hdfs : /// tmp / udaf . sh ; USE tmp ; SELECT TRANSFORM ( id , vtype , price ) USING 'udaf.sh' AS ( vtype STRING , mean FLOAT , var FLOAT ) FROM ( SELECT * FROM foo CLUSTER BY vtype ) AS TEMP_TABLE ; At first we add the zipped virtual environment to the distributed cache that will be automatically unpacked for us due to the ADD ARCHIVE command. Then we upload the Python and helper script. To make sure the current version in the cache is actually the latest, so in case changes are made, we prepended DELETE statements before each ADD . The actual query now calls TRANSFORM with the three input column we expect in our Python script. After the USING statement our helper script is provided as the actual UDAF seen by HiveQL. This is followed by AS defining the names and types of the output columns. At this point we need to make sure that the script is executed in a reducer step. We assure this by defining a subselect that reads from our foo table and clusters by the vtype . CLUSTER BY which is a shortcut for DISTRIBUTE BY followed by SORT BY asserts that rows having the same vtype column are also located on the same reducer. Furthermore, the implicit SORT BY orders within a reducer the rows with respect to the vtype column. The overall result are consecutive partitions of a given vehicle type (car and bike in our case) whereas each partition resides on a single reducer. Finally, our script is fed the whole data on a single reducer and needs to figure out itself where one partition ends and another one starts (what we did with itertools.groupby ). Finally Since our little task is now accomplished, it should also be noted that there are some more Python libraries one should know when working with Hive. To actually execute the HiveQL query we have written with the help of Python, there is impyla by Cloudera with supports Python 3 in contrast to PyHive by Dropbox. In order to work with HDFS the best library around is hdfs3 . That would for instance allow us to push changes in udaf.py automatically with a Python script. Have fun hacking Hive with the power of Python!","title":"Python UDFs and UDAFs in Hive","tags":"post","url":"http://www.florianwilhelm.info/2016/10/python_udf_in_hive/"},{"text":"This talk presented at the EuroPython 2016 introduces several Python libraries related to the handling of GPS data. The slides of this talk are available on Github or on nbviewer . If you have ever happened to need to deal with GPS data in Python you may have felt a bit lost. There are many libraries at various states of maturity and scope. Finding a place to start and to actually work with the GPS data might not be as easy and obvious as you might expect from other Python domains. Inspired from my own experiences of dealing with GPS data in Python, I want to give an overview of some useful libraries. From basic reading and writing GPS tracks in the GPS Exchange Format with the help of gpxpy to adding missing elevation information with srtm.py. Additionally, I will cover mapping and visualising tracks on OpenStreetmap with mplleaflet that even supports interactive plots in a Jupyter notebook. Besides the tooling, I will also demonstrate and explain common algorithms like Douglas-Peucker to simplify a track and the famous Kalman filters for smoothing. For both algorithms I will give an intuition about how they work as well as their basic mathematical concepts. Especially the Kalman filter that is used for all kinds of sensor, not only GPS , has the reputation of being hard to understand. Still, its concept is really easy and quite comprehensible as I will also demonstrate by presenting an implementation in Python with the help of Numpy and Scipy. My presentation will make heavy use of the Jupyter notebook which is a wonderful tool perfectly suited for experimenting and learning.","title":"Handling GPS Data with Python","tags":"talk","url":"http://www.florianwilhelm.info/2016/07/handling_gps_data_with_python/"},{"text":"It is a widely accepted fact that we are living in the era of Big Data. Many traditional companies are looking for ways to improve their business through the virtues of Big Data and Data Science. While matured startups born in this era like Facebook and Twitter seem to naturally exploit the value of their data, many traditional companies struggle to find new ways of utilizing their data to leverage its value for their classical businesses. In this post I elaborate on the specific domain of decision making where Big Data and Data Science can help to improve the efficiency of conventional businesses. Proving the benefits of Big Data in a lighthouse project is of utmost importance in long-established companies with regard to overcoming initial resistance in the digital transformation of business processes. We will see that the automatization of operational decisions, i.e. routine decisions related to the day-to-day running of the business, are especially suitable candidates for lighthouse projects to prove the value of Big Data in a company. The notion of automating data-driven decisions with the help of Data Science is often denoted with the term Prescriptive Analytics, which can be regarded as the conclusive step after Predictive Analytics. In other words, the predictions generated with the help of Predictive Analytics are used to optimize a predefined metric under consideration of side conditions, strategic direction, business processes etc. to derive excellent business decisions. The predictive analytics diagram from Gartner illustrates the business value compared to the difficulty of different analytical approaches. In many businesses repetitive operational decisions consume lots of working time. For instance pricing of articles and services, replenishment of stores or stocks, demand forecasts and customer services involve operational decisions which are often conducted in a manual process supported by traditional, rule based decision support systems. Automating these decisions with the help of data-driven decision systems has several benefits: Labor costs are reduced and scarce expertise can be leveraged for non-routine, exceptional decisions. Less routine decisions means having more time for decisions in extraordinary circumstances as well as decisions that are of a more tactical or strategical nature. This encompasses also decisions in situations where data is lacking as well as decisions about creative and visionary solutions. For instance, no machine learning algorithm could have ever predicted the success of the first iPhone since it was something completely new and its success was a consequence of not only that but also many other soft factors. The quality of decisions is improved given that all information sources used in the manual decision process are available as machine readable data. Modern machine learning algorithm are able to quickly analyze huge amounts of data that a human being could never even read in a lifetime. This plethora of data allows the inference of patterns that lead to fast, consistent, high quality decisions which are resistant to the long list of cognitive biases or decision fatigue . Prescriptive Analytics allows to scale the number of decisions. Too often in traditional businesses, decisions are made by not actually taking a decision which consequently leads to idleness and thus unexploited potential. Being able to scale the number of decisions, enables this untapped potential to be fully realized and can also generate new services. Imagine for instance that one marketing tool of a company is to give special offerings and product recommendations based on different market segments, not single customers. Being able to scale the number of decisions due to automation would allow special offerings and recommendations for individual customers, just like Amazon's recommendation system. To justify our statement that the automation of routine decisions with Prescriptive Analytics is exceptionally well suited as a pioneer project in a traditional company, it is necessary to elaborate on certain characteristics that many operational decisions hold. While a single operational decision, e.g. a small change in the price of a single article, may have an insignificantly small but direct impact on the revenue of the whole business, the sum of all decisions quite often has great economic impact. This is due to the fact that the frequency of operational decisions is often huge, meaning that a small overall improvement in decision quality is highly profitable. Obviously, candidates for a Prescriptive Analytics project should have exactly these properties of high and direct economic impact. The ability to measure such an impact requires that a performance metric or key performance indicator ( KPI ) is already established. This is another important prerequisite for a successful Prescriptive Analytics project. Since operational decisions are often related to the core of the business even in traditional companies huge amounts of data are already collected and available. Often, routine decisions that are taken by analyzing spreadsheets and personal experience are based on data with high predictive power. A quote, often attributed to Mark Twain, says that \"history doesn't repeat itself, but it does rhyme\" which captures the essence of what automated decision making is about. Having lots of data about past events allows us to find patterns and relationships which can predict future events to some extent. The goal is to develop a model that describes what happened in the past without being bound to the past and thus allowing to apply the model to the future. In just the same way as our brain learns from experiences and infers future outcomes in similar situations. Consequently, the high frequency of routine decision with a direct economic impact combined with an abundance of data and a metric to measure performance are favorable characteristic of a business process that can be successfully automated. In order to quantify the added value of Prescriptive Analytics an estimation of the gain in decision quality and its impact on revenue is needed with the help of the predefined metric or KPI . For this complex estimation it is recommended for traditional companies to have an experienced partner alongside and optionally a proof of concept to evaluate the predictive power of the data and the business case as a whole. We should not ignore the fact that automation also includes costs encompassing the maintenance, licence & support etc. of an established automated decision system. The automation costs depend mainly on the order of magnitude of performed decisions as well as the time frame. For instance the IT setup of a decision system for one million decisions per day will be much smaller than the setup for one billion that need to be determined in real- time. The initial costs for the implementation of an automated decision system varies largely depending on the domain of application, the necessary changes in the business processes and other factors. An estimation of these costs is needed to determine the time-to-value. Since the added value of an automated decision system quite often heavily surpasses automation costs by at least one order of magnitude time-to-value is often low. We conclude that the scaling in the number of decisions and the improved effectiveness of the decisions are the main drivers of the added value in automated decision making. Subsequently, operational decisions that are ubiquitous and directly influence the business value are well suited for a Predictive Analytics light-house project. Following mnemonic recaps the main qualities of a successful Prescriptive Analytics project. It consists of the following questions that should be answered positively: Are decisions taken F requently? Does the business process allow the A utomation of decisions? Are M etrics defined to determine the quality of a decision? Do decisions have a direct E conomic impact? Is enough and suitable D ata available to base decisions on? Prescriptive Analytics projects with these properties are very likely to become famed in your company. The successful implementation of a lighthouse project in the business process generates momentum for new projects. This drives the digital transformation of a classical business in an iterative way. This article was originally posted on the CSC 21st Century IT blog .","title":"Leveraging the Value of Big Data with Automated Decision Making","tags":"post","url":"http://www.florianwilhelm.info/2016/04/leveraging_the_value_of-big_data_with_automated_decision_making/"},{"text":"If you are doing probabilistic programming you are dealing with all kinds of different distributions. That means choosing an ensemble of right distributions which describe the underlying real-world process in a suitable way but also choosing the right parameters for prior distributions. At that point I often start visualizing the distributions with the help of Jupyter notebooks, matplotlib and SciPy to get a feeling how the distribution behaves when changing its parameters. And please don't tell me you are able to visualize all the distributions scipy.stats has to offer just in your head. For me, this surely is a repetitive task that every good and lazy programmer tries to avoid. Additionally, I was never quite satisfied with the interactivity of matplotlib in a notebook. Granted, the %matplotlib notebook magic was a huge step into the right direction but there is still much room for improvement. The new and shiny kid on the block is Bokeh and so far I have not really done much with it, meaning it is a good candidate for a test ride. The same goes actually for Jupyter's ipywidgets and you see where this going. No evaluation of a tool without a proper goal and that is now set to developing an interactive visualization widget for Jupyter based on Bokeh and ipywidgets. So here we go! It turned out that this task is easier than expected due the good documentation and examples of ipywidgets and especially Bokeh. You can read all about the implementation inside this notebook which is hosted in a separate Github repository . This also always me to make use of a new service that I just recently learned about, binder . This totally rad service takes any Github repository with a Jupyter notebook in it, fires up a container with Kubernetes, installs necessary requirements and finally runs your notebook! By just clicking on a link! Amazing to see how the ecosystem around Jupyter develops these days. And of course to wet your appetite, here are the screenshots of the final tool that you will experience interactively by starting the notebook with binder . The probability density function of a continuous alpha distribution with shape parameter a=1.3 The probability mass function of a discrete binomial distribution with shape parameters n=10 and p=0.7","title":"Interactively visualizing distributions in a Jupyter notebook with Bokeh","tags":"post","url":"http://www.florianwilhelm.info/2016/03/jupyter_distribution_visualizer/"},{"text":"This talk presented at the PyData Amsterdam 2016 explains the idea of Bayesian model selection techniques, especially the Automatic Relevance Determination. The slides of this talk are available on SlideShare . Even in the era of Big Data there are many real-world problems where the number of input features has about the some order of magnitude than the number of samples. Often many of those input features are irrelevant and thus inferring the relevant ones is an important problem in order to prevent over-fitting. Automatic Relevance Determination solves this problem by applying Bayesian techniques. In order to motivate Automatic Relevance Determination ( ARD ) an intuition for the problem of choosing a complex model that fits the data well vs a simple model that generalizes well is established. Thereby the idea behind Occam's razor is presented as a way of balancing bias and variance. This leads us to the mathematical framework of Bayesian interpolation and model selection to choose between different models based on the data. To derive ARD as gently as possible the mathematical basics of a simple linear model are repeated as well as the idea of regularization to prevent over-fitting. Based on that, the Bayesian Ridge Regression (BayesianRidge in Scikit-Learn) is introduced. Generalizing the concept of Bayesian Ridge Regression even more gets us eventually to the the idea behind ARD (ARDRegression in Scikit-Learn). With the help of a practical example, we consolidate what has been learned so far and compare ARD to an ordinary least square model. Now we dive deep into the mathematics of ARD and present the algorithm that solves the minimization problem of ARD . Finally, some details of Scikit-Learn's ARD implementation are discussed.","title":"Explaining the Idea behind ARD and Bayesian Interpolation","tags":"talk","url":"http://www.florianwilhelm.info/2016/03/explaining_the_idea_behind_ard/"},{"text":"Often I get the question as a Data Scientist what the Python Data Science Stack actually is and where a beginner should start to learn. The Python ecosystem, especially around the topics such as data analytics, data mining, data science and machine learning is so vast and rich that it confuses many rookies. For such an audience I created a slide deck that starts with pointing out the benefits of the Python language for analytics. Even beginners in Python are addressed by some slides that explain the syntax of Python and how to get started. After that some slides present the most important packages of the data science stack, namely NumPy , SciPy , Pandas , Scikit-Learn , Jupyter and IPython . The merits of Jupyter are best shown in a live demonstration to convey its power. The interplay between Pandas and Scikit-Learn is shown based on Kaggle's Titanic: Machine Learning from Disaster dataset. Eventually, an outlook to further libraries in the data science domain are presented. View in fullscreen","title":"Introduction to the Python Data Science Stack","tags":"slides","url":"http://www.florianwilhelm.info/2016/02/introduction_to_the_python_data_science_stack/"},{"text":"In this presentation given at the EuroPython 2015 in Bilbao, I show how the hangups library can be used in order to write a small chatbot that connects to Google Hangouts and reminds you or someone else to take his/her medication. The secure and recommended OAuth2 protocol is used to authorize the bot application in the Google Developers Console in order to access the Google+ Hangouts API . Subsequently, I explain how to use an event-driven library to write a bot that sends scheduled messages, waits for a proper reply and repeats the question if need be. Thereby, a primer on event-driven, asynchronous architectures is given. The source code can be downloaded on GitHub and the slides are available as html preview .","title":"How to write a friendly reminder bot","tags":"talk","url":"http://www.florianwilhelm.info/2015/07/howto_write_a_friendly_reminder_bot/"},{"text":"Scikit-Learn is a well-known and popular framework for machine learning that is used by Data Scientists all over the world. In this tutorial presented at the EuroPython 2014 in Berlin, I show in a practical way how you can add your own estimator following the interfaces of Scikit-Learn. First a small introduction to the design of Scikit-Learn and its inner workings is given. Then I show how easily Scikit-Learn can be extended by creating an own estimator. In order to demonstrate this, I extend Scikit-Learn by the popular and robust Theil-Sen Estimator that was not in Scikit-Learn until version 0.16. I also motivate this estimator by outlining some of its superior properties compared to the ordinary least squares method (LinearRegression in Scikit-Learn).","title":"Extending Scikit-Learn with your own regressor","tags":"talk","url":"http://www.florianwilhelm.info/2014/07/extending_scikit-learn_with_your_own_regressor/"},{"text":"In this presentation held at the EuroPython 2014 in Berlin, my colleague Felix Wick demonstrates that setting up a new Python project from scratch can be quite complicated. Especially, structuring your files and directories the right way in order to build a Python package. Thereby, questions are addressed like \"Where should my modules, documentation and unit tests go?\" and \"How do I configure setup.py, Sphinx and so on?\" with proven answers! Finally, the template tool PyScaffold is demonstrated that I developed at Blue Yonder . PyScaffold extremely simplifies the process of setting up a new Python project and is freely available to anyone.","title":"How to setup a new Python project","tags":"talk","url":"http://www.florianwilhelm.info/2014/07/howto_setup_a_new_python_project/"},{"text":"The german Modellansatz podcast curated by Gudrun Thäter and Sebastian Ritterbusch , supported by the Karlsruhe Institute for Technology ( KIT ), conveys the importance of mathematics and mathematical modeling in our everyday life. Starting from the water tap, automatic velocity control systems on highways to your mobile phone, mathematics is almost everywhere. In issue 25 of the podcast, I talk about predictive analytics, especially the challenges of making sales forecasts for retailers. With the help of these forecasts optimal order decisions can be made with respect to the availability of goods and write-offs.","title":"Podcast about Predictive Analytics","tags":"podcast","url":"http://www.florianwilhelm.info/2014/06/podcast_predictive_analytics/"},{"text":"The talk presented at the PyCon 2013 in Cologne gives a small introduction of how Blue Yonder applies machine learning and Predictive Analytics in various fields as well as the challenges of Big Data. Using the example of Blue Yonder's machine learning software NeuroBayes, I show the made efforts and hit dead ends in order to provide a flexible and yet easy to use interface for NeuroBayes to Data Scientists. Since NeuroBayes is written in FORTRAN for performance reasons different interface approaches were tried which lead us eventually to a Python interface. In this talk, I elaborate on the up- and downsides of the different approaches and the various reasons why Python won the race with an emphasize on the benefits of the Python ecosystem itself. Also, I discuss performance as well as scalability issues with Python and how we address them at Blue Yonder. In detail, I show the application of Cython to speed up calculations in the Python interface layer as well as distributed computing in a private cloud called Stratosphere. Scalability and efficiency is of utmost importance when data processing is time critical. The overall goal is to give the audience an overview how Python fits in the software ecosystem of a company handling Big Data.","title":"Handling Big Data with Python","tags":"talk","url":"http://www.florianwilhelm.info/2013/10/handling_big_data_with_python/"}]}