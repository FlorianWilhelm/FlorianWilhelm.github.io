{"pages":[{"url":"http://www.florianwilhelm.info/404-not-found/","text":"","tags":"English","title":"404: Page not found"},{"url":"http://www.florianwilhelm.info/410-gone/","text":"","tags":"English","title":"410: Page is gone"},{"url":"http://www.florianwilhelm.info/500-error/","text":"","tags":"English","title":"500: Internal server error"},{"url":"http://www.florianwilhelm.info/about/","text":"My name is Florian Wilhelm and I am a Data Scientist living in Cologne, Germany. I studied mathematics at the Karlsruhe Institute of Technology ( KIT ) with a focus on numerical mathematics and computer science as a subsidiary subject. After my studies, I worked on Parallel Preconditioners for an Ocean Model in Climate Simulations and graduated from university ( KIT ) with a PhD degree. Main focus of my work was the mathematical modelling of the governing equations of fluid dynamics in order to optimally solve them with the help of high-performance computing ( HPC ) on a large-scale cluster. As Head of the Research Lab for Application and Innovation at the Institute of Applied and Numerical Mathematics, I worked in several industry-funded projects and supervised bachelor and master students. After my postdoctoral position I started as a Data Scientist at Blue Yonder , the leading platform provider for Predictive Applications and Big Data in the European market. With more than three years of project experience in the field of Predictive & Prescriptive Analytics and Big Data, I have acquired profound knowledge in the domains of mathematical modelling, statistics, machine learning, high-performance computing and data mining. For the last years I programmed mostly with the Python Data Science stack ( NumPy , SciPy , Scikit-Learn , Pandas , Matplotlib , Jupyter , etc.) to which I also contributed several extensions. Due to some project work I have also gained experience in R as well as C/C++, Java, Matlab and Fortran. If you want to know more, please read all about my professional details in my résumé or on LinkedIn .","tags":"English","title":"About me"},{"url":"http://www.florianwilhelm.info/2016/03/explaining_the_idea_behind_ard/","text":"This talk presented at the PyData Amsterdam 2016 explains the idea of Bayesian model selection techniques, especially the Automatic Relevance Determination. The slides of this talk are available on SlideShare . Even in the era of Big Data there are many real-world problems where the number of input features has about the some order of magnitude than the number of samples. Often many of those input features are irrelevant and thus inferring the relevant ones is an important problem in order to prevent over-fitting. Automatic Relevance Determination solves this problem by applying Bayesian techniques. In order to motivate Automatic Relevance Determination ( ARD ) an intuition for the problem of choosing a complex model that fits the data well vs a simple model that generalizes well is established. Thereby the idea behind Occam's razor is presented as a way of balancing bias and variance. This leads us to the mathematical framework of Bayesian interpolation and model selection to choose between different models based on the data. To derive ARD as gently as possible the mathematical basics of a simple linear model are repeated as well as the idea of regularization to prevent over-fitting. Based on that, the Bayesian Ridge Regression (BayesianRidge in Scikit-Learn) is introduced. Generalizing the concept of Bayesian Ridge Regression even more gets us eventually to the the idea behind ARD (ARDRegression in Scikit-Learn). With the help of a practical example, we consolidate what has been learned so far and compare ARD to an ordinary least square model. Now we dive deep into the mathematics of ARD and present the algorithm that solves the minimization problem of ARD . Finally, some details of Scikit-Learn's ARD implementation are discussed.","tags":"talk","title":"Explaining the Idea behind ARD and Bayesian Interpolation"},{"url":"http://www.florianwilhelm.info/2016/02/introduction_to_the_python_data_science_stack/","text":"Often I get the question as a Data Scientist what the Python Data Science Stack actually is and where a beginner should start to learn. The Python ecosystem, especially around the topics such as data analytics, data mining, data science and machine learning is so vast and rich that it confuses many rookies. For such an audience I created a slide deck that starts with pointing out the benefits of the Python language for analytics. Even beginners in Python are addressed by some slides that explain the syntax of Python and how to get started. After that some slides present the most important packages of the data science stack, namely NumPy , SciPy , Pandas , Scikit-Learn , Jupyter and IPython . The merits of Jupyter are best shown in a live demonstration to convey its power. The interplay between Pandas and Scikit-Learn is shown based on Kaggle's Titanic: Machine Learning from Disaster dataset. Eventually, an outlook to further libraries in the data science domain are presented. View in fullscreen","tags":"slides","title":"Introduction to the Python Data Science Stack"},{"url":"http://www.florianwilhelm.info/2015/07/howto_write_a_friendly_reminder_bot/","text":"In this presentation given at the EuroPython 2015 in Bilbao, I show how the hangups library can be used in order to write a small chatbot that connects to Google Hangouts and reminds you or someone else to take his/her medication. The secure and recommended OAuth2 protocol is used to authorize the bot application in the Google Developers Console in order to access the Google+ Hangouts API . Subsequently, I explain how to use an event-driven library to write a bot that sends scheduled messages, waits for a proper reply and repeats the question if need be. Thereby, a primer on event-driven, asynchronous architectures is given. The source code can be downloaded on GitHub and the slides are available as html preview .","tags":"talk","title":"How to write a friendly reminder bot"},{"url":"http://www.florianwilhelm.info/2014/07/extending_scikit-learn_with_your_own_regressor/","text":"Scikit-Learn is a well-known and popular framework for machine learning that is used by Data Scientists all over the world. In this tutorial presented at the EuroPython 2014 in Berlin, I show in a practical way how you can add your own estimator following the interfaces of Scikit-Learn. First a small introduction to the design of Scikit-Learn and its inner workings is given. Then I show how easily Scikit-Learn can be extended by creating an own estimator. In order to demonstrate this, I extend Scikit-Learn by the popular and robust Theil-Sen Estimator that was not in Scikit-Learn until version 0.16. I also motivate this estimator by outlining some of its superior properties compared to the ordinary least squares method (LinearRegression in Scikit-Learn).","tags":"talk","title":"Extending Scikit-Learn with your own regressor"},{"url":"http://www.florianwilhelm.info/2014/07/howto_setup_a_new_python_project/","text":"In this presentation held at the EuroPython 2014 in Berlin, my colleague Felix Wick demonstrates that setting up a new Python project from scratch can be quite complicated. Especially, structuring your files and directories the right way in order to build a Python package. Thereby, questions are addressed like \"Where should my modules, documentation and unit tests go?\" and \"How do I configure setup.py, Sphinx and so on?\" with proven answers! Finally, the template tool PyScaffold is demonstrated that I developed at Blue Yonder . PyScaffold extremely simplifies the process of setting up a new Python project and is freely available to anyone.","tags":"talk","title":"How to setup a new Python project"},{"url":"http://www.florianwilhelm.info/2014/06/podcast_predictive_analytics/","text":"The german Modellansatz podcast curated by Gudrun Thäter and Sebastian Ritterbusch , supported by the Karlsruhe Institute for Technology ( KIT ), conveys the importance of mathematics and mathematical modeling in our everyday life. Starting from the water tap, automatic velocity control systems on highways to your mobile phone, mathematics is almost everywhere. In issue 25 of the podcast, I talk about predictive analytics, especially the challenges of making sales forecasts for retailers. With the help of these forecasts optimal order decisions can be made with respect to the availability of goods and write-offs.","tags":"podcast","title":"Podcast about Predictive Analytics"},{"url":"http://www.florianwilhelm.info/2013/10/handling_big_data_with_python/","text":"The talk presented at the PyCon 2013 in Cologne gives a small introduction of how Blue Yonder applies machine learning and Predictive Analytics in various fields as well as the challenges of Big Data. Using the example of Blue Yonder's machine learning software NeuroBayes, I show the made efforts and hit dead ends in order to provide a flexible and yet easy to use interface for NeuroBayes to Data Scientists. Since NeuroBayes is written in FORTRAN for performance reasons different interface approaches were tried which lead us eventually to a Python interface. In this talk, I elaborate on the up- and downsides of the different approaches and the various reasons why Python won the race with an emphasize on the benefits of the Python ecosystem itself. Also, I discuss performance as well as scalability issues with Python and how we address them at Blue Yonder. In detail, I show the application of Cython to speed up calculations in the Python interface layer as well as distributed computing in a private cloud called Stratosphere. Scalability and efficiency is of utmost importance when data processing is time critical. The overall goal is to give the audience an overview how Python fits in the software ecosystem of a company handling Big Data.","tags":"talk","title":"Handling Big Data with Python"}]}